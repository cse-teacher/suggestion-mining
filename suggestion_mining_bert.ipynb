{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cse-teacher/suggestion-mining/blob/main/suggestion_mining_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMyi6aqLVl_v"
      },
      "source": [
        "# Suggestion Mining using BERT\n",
        "Suggestion mining is the task of extracting suggestions from user reviews\n",
        "\n",
        "Developed: 11 Feb 2024 \\\\\n",
        "Last Update: 11 Feb 2024 \\\\\n",
        "Author: Muharram Mansoorizadeh plus Various AI tools (Google search, chatGPT, Gemini , ...)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0rqNS_w7Wvo"
      },
      "source": [
        "## Install Required Packagaes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMhPx71AqsaM",
        "outputId": "a3d50155-95ee-4195-9b17-5f5680ab7462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in c:\\python\\python38\\lib\\site-packages (2.10.1)\n",
            "Requirement already satisfied: cleantext in c:\\python\\python38\\lib\\site-packages (1.1.4)\n",
            "Requirement already satisfied: nltk in c:\\python\\python38\\lib\\site-packages (from cleantext) (3.8.1)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from click->nltk->cleantext) (0.4.6)\n",
            "Requirement already satisfied: nltk in c:\\python\\python38\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python38\\lib\\site-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\python\\python38\\lib\\site-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: pyenchant in c:\\python\\python38\\lib\\site-packages (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\python\\python38\\lib\\site-packages (1.3.2)\n",
            "Requirement already satisfied: lightgbm in c:\\python\\python38\\lib\\site-packages (4.3.0)\n",
            "Requirement already satisfied: catboost in c:\\python\\python38\\lib\\site-packages (1.2.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.24.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: graphviz in c:\\python\\python38\\lib\\site-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in c:\\python\\python38\\lib\\site-packages (from catboost) (3.7.5)\n",
            "Requirement already satisfied: pandas>=0.24 in c:\\python\\python38\\lib\\site-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: plotly in c:\\python\\python38\\lib\\site-packages (from catboost) (5.19.0)\n",
            "Requirement already satisfied: six in c:\\python\\python38\\lib\\site-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (6.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\python\\python38\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\python\\python38\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.17.0)\n",
            "Requirement already satisfied: gensim in c:\\python\\python38\\lib\\site-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\python\\python38\\lib\\site-packages (from gensim) (1.24.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\python\\python38\\lib\\site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\python\\python38\\lib\\site-packages (from gensim) (7.0.1)\n",
            "Requirement already satisfied: wrapt in c:\\python\\python38\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: transformers in c:\\python\\python38\\lib\\site-packages (4.38.2)\n",
            "Requirement already satisfied: sentencepiece in c:\\python\\python38\\lib\\site-packages (0.2.0)\n",
            "Requirement already satisfied: sacremoses in c:\\python\\python38\\lib\\site-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in c:\\python\\python38\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\python\\python38\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\python\\python38\\lib\\site-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "#Install required packages and libraries\n",
        "\n",
        "!apt-get install libenchant-2-2\n",
        "!pip install emoji\n",
        "!pip install cleantext\n",
        "!pip install nltk\n",
        "!pip install pyenchant\n",
        "!pip install scikit-learn lightgbm catboost\n",
        "!pip install gensim\n",
        "!pip install transformers sentencepiece sacremoses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUeHgN3BM3x0"
      },
      "source": [
        "## Import data\n",
        "\n",
        "Get the required data files from github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PGYc5OXNBFh",
        "outputId": "f1e09cdd-9278-4fd3-eb8b-e9adda88df89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fatal: destination path 'suggestion-mining' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cse-teacher/suggestion-mining.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2FjFI8E7gDn"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xWr3Gqpm9264"
      },
      "outputs": [],
      "source": [
        "# Read data from input files\n",
        "#Reset environment\n",
        "%reset -f\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "#Set default seed:\n",
        "random.seed(42)\n",
        "\n",
        "#Main Application\n",
        "folder     = \"./suggestion-mining/data/\"\n",
        "train_file = folder + \"V1.4_Training.csv\" #\"Train_Augmented_03.csv\" # V1.4_Training.csv\" #  \"Train_processed.csv\" /suggestion-mining/data/Train_Augmented_03.csv\n",
        "valid_file = folder + \"SubtaskA_Trial_Test_Labeled.csv\" #\"validation_processed.csv\"\n",
        "test_file  = folder + \"SubtaskA_EvaluationData_labeled.csv\"\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "valid_df = pd.read_csv(valid_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "test_df  = pd.read_csv(test_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "all_df = pd.concat([train_df, valid_df, test_df], axis=0)\n",
        "\n",
        "\n",
        "#Get the labels:\n",
        "y_train_original = train_df['label'].values\n",
        "y_valid_original = valid_df['label'].values\n",
        "y_test_original  = test_df['label'].values\n",
        "y_all_original  = all_df['label'].values\n",
        "train_size = len(train_df['label'])\n",
        "valid_size = len(valid_df['label'])\n",
        "test_size  = len(test_df['label'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsaeHSk9PmO1"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Usages**"
      ],
      "metadata": {
        "id": "PUZfqpTfkuYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "text = \"Microsoft should seriously look into getting rid of Syamentc for all these paying stuff\"\n",
        "replaced_text = replace_with_ner_tags(text)\n",
        "print(\"Replaced Text:\", replaced_text)\n",
        "#all_df[:][1:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aaq7eJV9kryE",
        "outputId": "0feba494-5cb6-487e-c6b2-56fe409bb280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced Text: PERSON should seriously look into getting rid of GPE for all these paying stuff\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "replace_ner_tags = True\n",
        "if replace_ner_tags == True:\n",
        "  #replace named entities with their tag names:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(replace_with_ner_tags)\n",
        "  test_df['sentence'] = test_df['sentence'].apply(replace_with_ner_tags)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(replace_with_ner_tags)\n",
        "  #all_df['sentence'] = all_df['sentence'].apply(replace_hyperlinks)\n",
        "  #all_df['sentence'] = all_df['sentence'].apply(cleantext.clean)"
      ],
      "metadata": {
        "id": "ceUQZN9Ltd8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD9BI9epYsc5"
      },
      "source": [
        "**BERT for Classification**:\n",
        "\n",
        "Import libraries: We import necessary libraries for loading BERT tokenizer and model, processing text, and making predictions.\n",
        "\n",
        "Load BERT tokenizer and model: We load the pre-trained bert-base-uncased tokenizer and model. Replace 'bert-base-uncased' with your desired pre-trained BERT model name.\n",
        "\n",
        "Preprocess text function: This function performs the following:\n",
        "\n",
        "Tokenizes the text using the BERT tokenizer.\n",
        "\n",
        "Adds special tokens ([CLS] and [SEP]) to the beginning and end of the sequence, respectively.\n",
        "\n",
        "Pads the sequence to a maximum length (MAX_LEN) if necessary.\n",
        "\n",
        "Define example text and label: Replace text with your actual text to classify and adjust label based on your classification categories.\n",
        "\n",
        "Preprocess text: Call the preprocess_text function to convert the text into the required format for BERT.\n",
        "\n",
        "Make prediction: Pass the preprocessed text through the model to obtain predictions.\n",
        "\n",
        "Get predicted class and probability: Extract the predicted class index and its corresponding probability from the prediction results.\n",
        "\n",
        "Print results: Print the predicted class and its probability.\n",
        "Note:\n",
        "\n",
        "This is a basic example and can be further customized for specific tasks like sentiment analysis or topic classification.\n",
        "Remember to install the required libraries (transformers and tensorflow) before running the code.\n",
        "Adjust MAX_LEN based on the maximum sentence length in your dataset.\n",
        "Sources\n",
        "github.com/JiaYaobo/toxic_detect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ftiBOLQwaQh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e2ee62c-88c1-45b1-b481-b09cfa1b1a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in c:\\python\\python38\\lib\\site-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in c:\\python\\python38\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\python\\python38\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\python\\python38\\lib\\site-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U tensorflow-text\n",
        "!pip install transformers\n",
        "!pip install -q tf-models-official"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Init Bert**"
      ],
      "metadata": {
        "id": "2PWHVcU6loZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "HF_TOKEN = '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'\n",
        "\n",
        "#Main Application\n",
        "folder     = \"./suggestion-mining/data/\"\n",
        "train_file = folder + \"Train_Augmented_03.csv\" # V1.4_Training.csv\" #  \"Train_processed.csv\"\n",
        "valid_file = folder + \"SubtaskA_Trial_Test_Labeled.csv\" #\"validation_processed.csv\"\n",
        "test_file  = folder + \"SubtaskA_EvaluationData_labeled.csv\"\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])"
      ],
      "metadata": {
        "id": "OBs36NrwlonS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Example data\n",
        "#texts = [\"Text 1\", \"Text 2\", \"Text 3\"]  # Your list of text strings\n",
        "#labels = [0, 1, 0]  # Example labels (binary classification)\n",
        "\n",
        "texts  = train_df['sentence'].tolist()\n",
        "labels = train_df['label'].tolist()\n",
        "\n",
        "# Load pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model.to(device)\n",
        "\n",
        "# Tokenize and encode the texts\n",
        "encoded_inputs = tokenizer(texts, add_special_tokens=True, padding=True, truncation=True, return_tensors='pt')\n",
        "encoded_inputs.to(device)  # Move input tensor to GPU\n",
        "\n",
        "#encoded_inputs.todevice(device)\n",
        "# Split data into mini-batches\n",
        "batch_size = 4\n",
        "num_batches = len(texts) // batch_size + (len(texts) % batch_size != 0)\n",
        "\n",
        "# Initialize empty lists to store embeddings and labels\n",
        "embeddings_list = []\n",
        "labels_list = []\n",
        "\n",
        "# Process each mini-batch\n",
        "for i in range(num_batches):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = min((i + 1) * batch_size, len(texts))\n",
        "    batch_texts = texts[start_idx:end_idx]\n",
        "    batch_labels = labels[start_idx:end_idx]\n",
        "    # Encode the mini-batch\n",
        "    batch_encoded_inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    #batch_encoded_inputs.todevice(device)\n",
        "    print('OK1')\n",
        "    #batch_encoded_inputs = torch.Tensor(batch_encoded_inputs).todevice(device)\n",
        "    # Get the BERT embeddings for the mini-batch\n",
        "    with torch.no_grad():\n",
        "        batch_outputs = model(**batch_encoded_inputs)\n",
        "\n",
        "    print('OK2')\n",
        "    # Extract the embeddings (the last hidden state) and append to the list\n",
        "    batch_embeddings = batch_outputs.last_hidden_state.cpu().numpy()\n",
        "    batch_embeddings = batch_embeddings[:,-1, :]\n",
        "    embeddings_list.append(batch_embeddings)\n",
        "\n",
        "    # Append labels to the list\n",
        "    labels_list.extend(batch_labels)\n",
        "\n",
        "# Concatenate embeddings and labels\n",
        "embeddings = np.concatenate(embeddings_list, axis=0)\n",
        "labels = np.array(labels_list)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression classifier\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = classifier.predict(X_test.reshape(X_test.shape[0], -1))\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p7gB7O_QlHwC",
        "outputId": "14f003d2-64ae-4b46-f62c-147ac806d07a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "OK1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#batch_encoded_inputs = torch.Tensor(batch_encoded_inputs).todevice(device)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Get the BERT embeddings for the mini-batch\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 51\u001b[0m     batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_encoded_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOK2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Extract the embeddings (the last hidden state) and append to the list\u001b[39;00m\n",
            "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1006\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1014\u001b[0m     embedding_output,\n\u001b[0;32m   1015\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1024\u001b[0m )\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:232\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    229\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    235\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
            "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming you have your documents stored in a list of strings named `documents`\n",
        "# Assuming you have your labels stored in a list named `labels` where 0 indicates one class and 1 indicates another class\n",
        "documents  = train_df['sentence'].tolist()\n",
        "labels = train_df['label'].tolist()\n",
        "# Split data into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(documents, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = BertModel.from_pretrained(model_name).to('cuda')  # Move model to GPU\n",
        "\n",
        "# Tokenize and encode the training and testing texts\n",
        "max_length = 128  # Adjust as needed\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
        "\n",
        "# Define DataLoader\n",
        "batch_size = 2  # Adjust as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define classification model\n",
        "class ClassificationModel(torch.nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.fc = torch.nn.Linear(self.bert.config.hidden_size, 2)  # Output size is 2 for binary classification\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Instantiate classification model\n",
        "classification_model = ClassificationModel(bert_model).to('cuda')  # Move model to GPU\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(classification_model.parameters(), lr=2e-5)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3  # Adjust as needed\n",
        "classification_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to('cuda'), attention_mask.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = classification_model(input_ids, attention_mask)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "classification_model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "for batch in tqdm(test_loader):\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids, attention_mask, labels = input_ids.to('cuda'), attention_mask.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = classification_model(input_ids, attention_mask)\n",
        "    _, predicted_labels = torch.max(outputs, 1)\n",
        "    predictions.extend(predicted_labels.cpu().numpy())\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "WBkI97GoI88e",
        "outputId": "8c063cc2-9a9a-467d-92c1-53eed5e01b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|██▊                                                                            | 210/5902 [00:10<04:35, 20.62it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     64\u001b[0m     input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 65\u001b[0m     input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     68\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m classification_model(input_ids, attention_mask)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}