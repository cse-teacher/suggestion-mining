{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cse-teacher/suggestion-mining/blob/main/suggestion_mining_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMyi6aqLVl_v"
      },
      "source": [
        "# Suggestion Mining using BERT\n",
        "Suggestion mining is the task of extracting suggestions from user reviews\n",
        "\n",
        "Developed: 11 Feb 2024 \\\\\n",
        "Last Update: 11 Feb 2024 \\\\\n",
        "Author: Muharram Mansoorizadeh plus Various AI tools (Google search, chatGPT, Gemini , ...)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD9BI9epYsc5"
      },
      "source": [
        "**BERT for Classification**:\n",
        "\n",
        "Import libraries: We import necessary libraries for loading BERT tokenizer and model, processing text, and making predictions.\n",
        "\n",
        "Load BERT tokenizer and model: We load the pre-trained bert-base-uncased tokenizer and model. Replace 'bert-base-uncased' with your desired pre-trained BERT model name.\n",
        "\n",
        "Preprocess text function: This function performs the following:\n",
        "\n",
        "Tokenizes the text using the BERT tokenizer.\n",
        "\n",
        "Adds special tokens ([CLS] and [SEP]) to the beginning and end of the sequence, respectively.\n",
        "\n",
        "Pads the sequence to a maximum length (MAX_LEN) if necessary.\n",
        "\n",
        "Define example text and label: Replace text with your actual text to classify and adjust label based on your classification categories.\n",
        "\n",
        "Preprocess text: Call the preprocess_text function to convert the text into the required format for BERT.\n",
        "\n",
        "Make prediction: Pass the preprocessed text through the model to obtain predictions.\n",
        "\n",
        "Get predicted class and probability: Extract the predicted class index and its corresponding probability from the prediction results.\n",
        "\n",
        "Print results: Print the predicted class and its probability.\n",
        "Note:\n",
        "\n",
        "This is a basic example and can be further customized for specific tasks like sentiment analysis or topic classification.\n",
        "Remember to install the required libraries (transformers and tensorflow) before running the code.\n",
        "Adjust MAX_LEN based on the maximum sentence length in your dataset.\n",
        "Sources\n",
        "github.com/JiaYaobo/toxic_detect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0rqNS_w7Wvo"
      },
      "source": [
        "## Install Required Packagaes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMhPx71AqsaM",
        "outputId": "d5f92e94-657f-4e56-9bf7-f9911a649234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in c:\\python\\python38\\lib\\site-packages (2.10.1)\n",
            "Requirement already satisfied: cleantext in c:\\python\\python38\\lib\\site-packages (1.1.4)\n",
            "Requirement already satisfied: nltk in c:\\python\\python38\\lib\\site-packages (from cleantext) (3.8.1)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from click->nltk->cleantext) (0.4.6)\n",
            "Requirement already satisfied: nltk in c:\\python\\python38\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python38\\lib\\site-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\python\\python38\\lib\\site-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: pyenchant in c:\\python\\python38\\lib\\site-packages (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\python\\python38\\lib\\site-packages (1.3.2)\n",
            "Requirement already satisfied: lightgbm in c:\\python\\python38\\lib\\site-packages (4.3.0)\n",
            "Requirement already satisfied: catboost in c:\\python\\python38\\lib\\site-packages (1.2.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.24.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: graphviz in c:\\python\\python38\\lib\\site-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in c:\\python\\python38\\lib\\site-packages (from catboost) (3.7.5)\n",
            "Requirement already satisfied: pandas>=0.24 in c:\\python\\python38\\lib\\site-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: plotly in c:\\python\\python38\\lib\\site-packages (from catboost) (5.19.0)\n",
            "Requirement already satisfied: six in c:\\python\\python38\\lib\\site-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (6.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\python\\python38\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\python\\python38\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.17.0)\n",
            "Requirement already satisfied: gensim in c:\\python\\python38\\lib\\site-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\python\\python38\\lib\\site-packages (from gensim) (1.24.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\python\\python38\\lib\\site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\python\\python38\\lib\\site-packages (from gensim) (7.0.1)\n",
            "Requirement already satisfied: wrapt in c:\\python\\python38\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: transformers in c:\\python\\python38\\lib\\site-packages (4.38.2)\n",
            "Requirement already satisfied: sentencepiece in c:\\python\\python38\\lib\\site-packages (0.2.0)\n",
            "Requirement already satisfied: sacremoses in c:\\python\\python38\\lib\\site-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in c:\\python\\python38\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\python\\python38\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\python\\python38\\lib\\site-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "#Install required packages and libraries\n",
        "\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install gensim\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUeHgN3BM3x0"
      },
      "source": [
        "## Import data\n",
        "\n",
        "Get the required data files from github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PGYc5OXNBFh",
        "outputId": "e2a66f16-8783-4d11-d43d-7358859c35d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fatal: destination path 'suggestion-mining' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cse-teacher/suggestion-mining.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2FjFI8E7gDn"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xWr3Gqpm9264"
      },
      "outputs": [],
      "source": [
        "# Read data from input files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "#Set default seed:\n",
        "random.seed(42)\n",
        "\n",
        "#Main Application\n",
        "folder     = \"./suggestion-mining/data/\"\n",
        "train_file = folder + \"V1.4_Training.csv\" #\"Train_Augmented_03.csv\" # V1.4_Training.csv\" #  \"Train_processed.csv\" /suggestion-mining/data/Train_Augmented_03.csv\n",
        "valid_file = folder + \"SubtaskA_Trial_Test_Labeled.csv\" #\"validation_processed.csv\"\n",
        "test_file  = folder + \"SubtaskA_EvaluationData_labeled.csv\"\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "valid_df = pd.read_csv(valid_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "test_df  = pd.read_csv(test_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "all_df = pd.concat([train_df, valid_df, test_df], axis=0)\n",
        "\n",
        "\n",
        "#Get the labels:\n",
        "y_train_original = train_df['label'].values\n",
        "y_valid_original = valid_df['label'].values\n",
        "y_test_original  = test_df['label'].values\n",
        "y_all_original  = all_df['label'].values\n",
        "train_size = len(train_df['label'])\n",
        "valid_size = len(valid_df['label'])\n",
        "test_size  = len(test_df['label'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsaeHSk9PmO1"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0263d08-6558-4d8b-ef61-f34ee3ef41c2",
        "id": "BriN_u_BU0oN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced Text: PERSON should seriously look into getting rid of GPE for all these paying stuff\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import re\n",
        "import nltk\n",
        "import cleantext\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "def remove_nonalphanumeric(text):\n",
        "    #text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
        "  text = re.sub(r'\\W+', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "def remove_stopwords_list(tokens):\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  filtered_tokens = remove_stopwords_list(tokens)\n",
        "  return ' '.join(filtered_tokens)\n",
        "\n",
        "#-----------------------------------\n",
        "# Replace hyperlinks\n",
        "#\n",
        "def replace_hyperlinks(text):\n",
        "  text = re.sub(r'^https?:\\/\\/\\S+', 'hyperlink', text, flags=re.MULTILINE)\n",
        "  return text\n",
        "\n",
        "def stem(text):\n",
        "  tokens = word_tokenize(text.strip())\n",
        "  tokens_stem =[stemmer.stem(s) for s in tokens]\n",
        "  return ' '.join(tokens_stem)\n",
        "\n",
        "#----------------------------------------\n",
        "# replace_named_entities:\n",
        "#    Replaces each word or phrase in the input text with its\n",
        "#    Named Entity Recognition (NER) tag label.\n",
        "#    Args:\n",
        "#    text (str): Input text\n",
        "#\n",
        "#    Returns:\n",
        "#    str: Text with named entities replaced by their NER tag labels\n",
        "#\n",
        "def replace_named_entities(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Tag the words with Part-of-Speech (POS) tags\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    # Perform Named Entity Recognition (NER)\n",
        "    named_entities = ne_chunk(tagged_words)\n",
        "\n",
        "    # Replace entities with their NER tag labels\n",
        "    replaced_text = []\n",
        "    for entity in named_entities:\n",
        "        if isinstance(entity, nltk.tree.Tree):\n",
        "            label = entity.label()\n",
        "            named_entity_text = \" \".join([word for word, tag in entity.leaves()])\n",
        "            #replaced_text.append(f'<{label}>{named_entity_text}</{label}>')\n",
        "            replaced_text.append(f'{label}')\n",
        "            #replaced_text.append('')\n",
        "        else:\n",
        "            replaced_text.append(entity[0])\n",
        "\n",
        "    return \" \".join(replaced_text)\n",
        "\n",
        "#----------------------------------\n",
        "# Print results per class\n",
        "#\n",
        "def print_results(y_actual, y_pred, description=''):\n",
        "  v00 = accuracy_score(y_actual, y_pred)\n",
        "  v01 = precision_score(y_actual, y_pred, pos_label=0)\n",
        "  v02 = recall_score(y_actual, y_pred, pos_label=0)\n",
        "  v03 = f1_score(y_actual, y_pred, pos_label=0)\n",
        "\n",
        "  v11 = precision_score(y_actual, y_pred, pos_label=1)\n",
        "  v12 = recall_score(y_actual, y_pred, pos_label=1)\n",
        "  v13 = f1_score(y_actual, y_pred, pos_label=1)\n",
        "\n",
        "  smsg = f\"{description},\\tAccuracy={v00:.2f},\\tC0: Pr={v01:.2f}, Re={v02:.2f}, F1={v03:.2f},\\tC1: Pr={v11:.2f}, Re={v12:.2f}, F1={v13:.2f}\"\n",
        "  print(smsg)\n",
        "  with open(\"results.txt\", \"a\") as myfile:\n",
        "    myfile.write(f\"{datetime.now()}\\t {smsg}\\n\")\n",
        "\n",
        "\n",
        "#Global callings:\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Example usage:\n",
        "text = \"Microsoft should seriously look into getting rid of Syamentc for all these paying stuff\"\n",
        "replaced_text = replace_named_entities(text)\n",
        "print(\"Replaced Text:\", replaced_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op_replace_hyperlinks      = True\n",
        "op_remove_nonalphanumeric  = True\n",
        "op_remove_stopwords        = False\n",
        "op_replace_named_entities  = False\n",
        "op_stem                    = False\n",
        "\n",
        "if op_replace_hyperlinks == True:\n",
        "  #replace named entities with their tag names:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_hyperlinks)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_hyperlinks)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_hyperlinks)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_hyperlinks)\n",
        "\n",
        "if op_remove_nonalphanumeric == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_nonalphanumeric)\n",
        "\n",
        "if op_replace_named_entities == True:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_named_entities)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_named_entities)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_named_entities)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_named_entities)\n",
        "\n",
        "if op_remove_stopwords == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_stopwords)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_stopwords)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_stopwords)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_stopwords)\n",
        "\n",
        "if op_stem == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(stem)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(stem)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(stem)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(stem)\n"
      ],
      "metadata": {
        "id": "ceUQZN9Ltd8s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bert Based Classifier**"
      ],
      "metadata": {
        "id": "2PWHVcU6loZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if GPU is available and move the model and data to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "\n",
        "#Prepare training and testing sets\n",
        "train_texts = train_df['sentence'].tolist(); train_labels = train_df['label'].tolist()\n",
        "test_texts  = test_df['sentence'].tolist();  test_labels  = test_df['label'].tolist()\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = BertModel.from_pretrained(model_name).to(device)  # Move model to GPU\n",
        "\n",
        "# Tokenize and encode the training and testing texts\n",
        "max_length = 64  # Adjust as needed\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
        "\n",
        "# Define DataLoader\n",
        "batch_size = 32  # Adjust as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define classification model\n",
        "class ClassificationModel(torch.nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.fc = torch.nn.Linear(self.bert.config.hidden_size, 2)  # Output size is 2 for binary classification\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Instantiate classification model\n",
        "classification_model = ClassificationModel(bert_model).to(device)  # Move model to GPU\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(classification_model.parameters(), lr=2e-5)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100  # Adjust as needed\n",
        "classification_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = classification_model(input_ids, attention_mask)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} \\tloss={loss}\")\n",
        "\n",
        "# Evaluation\n",
        "classification_model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "for batch in tqdm(test_loader):\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = classification_model(input_ids, attention_mask)\n",
        "    _, predicted_labels = torch.max(outputs, 1)\n",
        "    predictions.extend(predicted_labels.cpu().numpy())\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print_results(true_labels, predictions, description='BERT')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBkI97GoI88e",
        "outputId": "4a19458c-8440-4934-d85b-44fe9f48360b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:01<00:00, 17.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9279711884753902\n",
            "BERT,\tAccuracy=0.93,\tC0: Pr=0.98, Re=0.94, F1=0.96,\tC1: Pr=0.62, Re=0.82, F1=0.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eSamfMxlyfQ",
        "outputId": "557ad847-b525-42c8-846d-dcb78dd00253"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "833"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model if needed\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the current date and time\n",
        "current_datetime = datetime.now()\n",
        "\n",
        "# Format the current date and time into a string\n",
        "model_file_name = 'mlp_model_' + current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\") + \".pth\"\n",
        "\n",
        "torch.save(classification_model, model_file_name)\n"
      ],
      "metadata": {
        "id": "ZnfD6ngcgY1r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model class must be defined somewhere\n",
        "model_file_name = 'mlp_model_2024-03-21_12-37-38.pth'\n",
        "model = torch.load(model_file_name)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "for batch in tqdm(test_loader):\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "    _, predicted_labels = torch.max(outputs, 1)\n",
        "    predictions.extend(predicted_labels.cpu().numpy())\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "print_results(true_labels, predictions, description='BERT')\n",
        "\n"
      ],
      "metadata": {
        "id": "Zm6uDNPvfPB7",
        "outputId": "baf2e37a-3ae6-4573-fe4f-d1e54a91ba68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:01<00:00, 17.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9387755102040817\n",
            "BERT,\tAccuracy=0.94,\tC0: Pr=0.98, Re=0.95, F1=0.97,\tC1: Pr=0.67, Re=0.82, F1=0.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[true_labels, predictions]"
      ],
      "metadata": {
        "id": "5HuLr8s7iqjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the model on all data:\n",
        "#Prepare training and testing sets\n",
        "all_texts = all_df['sentence'].tolist(); all_labels = all_df['label'].tolist()\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = BertModel.from_pretrained(model_name).to(device)  # Move model to GPU\n",
        "\n",
        "# Tokenize and encode the training and testing texts\n",
        "max_length = 64  # Adjust as needed\n",
        "all_encodings  = tokenizer(all_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "# Convert labels to tensors\n",
        "all_labels   = torch.tensor(all_labels)\n",
        "\n",
        "# Create TensorDatasets\n",
        "all_dataset  = TensorDataset(all_encodings['input_ids'], all_encodings['attention_mask'], all_labels)\n",
        "all_loader = DataLoader(all_dataset, batch_size=batch_size, shuffle=False)\n",
        "predictions = []\n",
        "true_labels = []\n",
        "for batch in tqdm(all_loader):\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "    _, predicted_labels = torch.max(outputs, 1)\n",
        "    predictions.extend(predicted_labels.cpu().numpy())\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "print_results(true_labels, predictions, description='BERT, all data')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-36slgBUUaA",
        "outputId": "f75e211c-61f0-41f4-946a-293e65090a24"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████| 311/311 [00:18<00:00, 16.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT, all data,\tAccuracy=0.98,\tC0: Pr=0.99, Re=0.98, F1=0.99,\tC1: Pr=0.95, Re=0.97, F1=0.96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts1 =[\"We need for our applications on each of the mobile platforms to be able to read data on a separate NDEF records contained in an NDEF message.\"\n",
        ",\"Task-based operations would make the code much more clean and readable.\"\n",
        ",\"SharpDX proves that those key technologies can be made available to CSharp: I would suggest to either support the project or to provide your own interface.\"\n",
        ",\"When an application has problems or when a problem is detected in advance by the developer until the publication of the correction version the client is without knowing what is happening, it would be possible for Microsoft to implement a communications resource developer for customers who have installed its application?\"\n",
        ",\"I hope Microsoft can open Asia voice function fastest\"\n",
        ",\"We need support special multitasking tasks like VOIP and IM!\"\n",
        ",\"If Microsoft created some incentives like a free device (Blackberry did this) and a free year of membership (Amazon did this) it could drive a lot of developers to the platform which is crucial to the platform taking off.\"\n",
        ",\"why shouldn't I as user be able to see reviews made by people from other countries?\"\n",
        ",\"And a smart dialer can then be a full alternative to the built-in phone app since the app also can have a quick-button for goto the regular phone app whenever the user wants to do that.\"\n",
        ",\"If the Phone doesn't have an accurate location simply return what it does have.\"\n",
        ",\"There is what I consider a security issue in UWP applications that I feel should be addressed.\"\n",
        ",\"Unfortunately we can't create UWP apps because there is no support for the custom WCF binding - which is needed - to talk to these instruments.\"\n",
        ",\"If an user plans to opt out from a response from developer DO NOT EMAIL THE USER A RESPONSE but STILL ALLOW A RESPONSE TO MADE PUBLICLY VISIBLE.\"\n",
        ",\"Read email access Locally is very important to create professional and amazing apps for UWP apps, please allow us to achieve this!\"\n",
        ",\"The solution is open-sourcing WinDbg and related tools (dbghlp WinDbg extensions etc. ) which will allow the community to fix and improve what the rather small team at Microsoft doesn't manage to.\"\n",
        ",\"It is frustrated for a devoloper then an app is'nt Show in any category and the app can only find per direct search by Name.\"]\n",
        "\n",
        "\n",
        "texts2 = [\"We need for our applications on each of the mobile platforms to be able to read data on a separate records contained in an message \"\n",
        ",\"Task based operations would make the code much more clean and readable \"\n",
        ",\"SharpDX proves that those key technologies can be made available to CSharp I would suggest to either support the project or to provide your own interface \"\n",
        ",\"When an application has problems or when a problem is detected in advance by the developer until the publication of the correction version the client is without knowing what is happening it would be possible for Microsoft to implement a communications resource developer for customers who have installed its application \"\n",
        ",\"I hope Microsoft can open Asia voice function fastest\"\n",
        ",\"We need support special multitasking tasks like VOIP and IM \"\n",
        ",\"If Microsoft created some incentives like a free device Blackberry did this and a free year of membership Amazon did this it could drive a lot of developers to the platform which is crucial to the platform taking off \"\n",
        ",\"why shouldn t I as user be able to see reviews made by people from other countries \"\n",
        ",\"And a smart dialer can then be a full alternative to the built in phone app since the app also can have a quick button for goto the regular phone app whenever the user wants to do that \"\n",
        ",\"If the Phone doesn t have an accurate location simply return what it does have \"\n",
        ",\"There is what I consider a security issue in UWP applications that I feel should be addressed \"\n",
        ",\"Unfortunately we can t create UWP apps because there is no support for the custom WCF binding which is needed to talk to these instruments \"\n",
        ",\"If an user plans to opt out from a response from developer DO NOT EMAIL THE USER A RESPONSE but STILL ALLOW A RESPONSE TO MADE PUBLICLY VISIBLE \"\n",
        ",\"Read email access Locally is very important to create professional and amazing apps for UWP apps please allow us to achieve this \"\n",
        ",\"The solution is open sourcing WinDbg and related tools dbghlp WinDbg extensions etc which will allow the community to fix and improve what the rather small team at Microsoft doesn t manage to \"\n",
        ",\"It is frustrated for a devoloper then an app is nt Show in any category and the app can only find per direct search by Name \"\n",
        ",\"Need each platforms records message\"\n",
        "]\n",
        "\n",
        "texts3 = [\"Ensure that our applications on each mobile platform can access data stored within distinct NDEF records contained in an NDEF message.\"\n",
        ",\"Implement task-based operations to significantly enhance code clarity and readability.\"\n",
        ",\"Consider supporting SharpDX to integrate these essential technologies with CSharp or develop your own interface.\"\n",
        ",\"Implement a communication channel for developers to notify customers who have installed their application when issues arise.\"\n",
        ",\"Expedite the deployment of the Asian voice function.\"\n",
        ",\"Provide support for specialized multitasking functionalities such as VOIP and IM.\"\n",
        ",\"Offer incentives like a complimentary device and a free year of membership to encourage developers to join the platform.\"\n",
        ",\"Allow users to view reviews submitted by individuals from other countries.\"\n",
        ",\"Develop a smart dialer as a comprehensive alternative to the native phone app, with a quick-access button to revert to the standard phone application.\"\n",
        ",\"Ensure the phone provides whatever location information is available, even if it's not precise.\"\n",
        ",\"Address the security vulnerability in UWP applications.\"\n",
        ",\"Enable support for custom WCF bindings necessary for communication with certain instruments to facilitate UWP app development.\"\n",
        ",\"If a user opts out of receiving responses from developers, refrain from emailing the user a response, but still allow the response to be publicly visible.\"\n",
        ",\"Grant access to read emails locally for crafting professional and sophisticated UWP applications.\"\n",
        ",\"Open-source WinDbg and associated tools to empower the community to address and enhance aspects beyond Microsoft's capacity.\"\n",
        ",\"Ensure apps appear in relevant categories and are easily discoverable rather than only being found through direct search by name.\"\n",
        "          ]\n",
        "\n",
        "def get_text(textid):\n",
        "  if textid ==1:\n",
        "    return texts1\n",
        "\n",
        "  if textid ==2:\n",
        "    return texts2\n",
        "\n",
        "  if textid ==3:\n",
        "    return texts3\n",
        "\n"
      ],
      "metadata": {
        "id": "4khMGduOU9I2"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Do  a simple test on errors\n",
        "\n",
        "texts = get_text(3)\n",
        "lables =[1]*len(texts)\n",
        "for i in range(len(texts)):\n",
        "  texts[i] = convert_to_passive_voice(texts[i])\n",
        "\n",
        "encodings  = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "# Convert labels to tensors\n",
        "labels   = torch.tensor([1]*len(texts))\n",
        "\n",
        "# Create TensorDatasets\n",
        "dataset      = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels)\n",
        "dataloader   = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "for batch in tqdm(dataloader):\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "    _, predicted_labels = torch.max(outputs, 1)\n",
        "    predictions.extend(predicted_labels.cpu().numpy())\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "print_results(true_labels, predictions, description='BERT')\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "r3oOQCG5ir1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f792dbc-2342-4f48-919a-97ba2e39c7e4"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 111.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT,\tAccuracy=0.44,\tC0: Pr=0.00, Re=0.00, F1=0.00,\tC1: Pr=1.00, Re=0.44, F1=0.61\n",
            "[1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(texts[11])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnvY9kQkfA5R",
        "outputId": "4caf8932-87f9-48e8-e776-7992dea2e3b7"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create which is instruments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = classification_model\n",
        "\n",
        "x = torch.randn(1, 8)\n",
        "y = model(train_encodings['input_ids'][1], train_encodings['attention_mask'][1], 1)\n",
        "\n",
        "make_dot(y.mean(), params=dict(model.named_parameters()))"
      ],
      "metadata": {
        "id": "WFpV-k_Xpv6s",
        "outputId": "1c7fe6b6-bbe8-48e4-ecf6-df66fdddbb64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ClassificationModel.forward() takes 3 positional arguments but 4 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-1212747157f2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ClassificationModel.forward() takes 3 positional arguments but 4 were given"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}