{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cse-teacher/suggestion-mining/blob/main/suggestion_mining_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMyi6aqLVl_v"
      },
      "source": [
        "# Suggestion Mining using BERT\n",
        "Suggestion mining is the task of extracting suggestions from user reviews\n",
        "\n",
        "Developed: 11 Feb 2024 \\\\\n",
        "Last Update: 11 Feb 2024 \\\\\n",
        "Author: Muharram Mansoorizadeh plus Various AI tools (Google search, chatGPT, Gemini , ...)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0rqNS_w7Wvo"
      },
      "source": [
        "## Install Required Packagaes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMhPx71AqsaM",
        "outputId": "a3d50155-95ee-4195-9b17-5f5680ab7462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in c:\\python\\python38\\lib\\site-packages (2.10.1)\n",
            "Requirement already satisfied: cleantext in c:\\python\\python38\\lib\\site-packages (1.1.4)\n",
            "Requirement already satisfied: nltk in c:\\python\\python38\\lib\\site-packages (from cleantext) (3.8.1)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from click->nltk->cleantext) (0.4.6)\n",
            "Requirement already satisfied: nltk in c:\\python\\python38\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python38\\lib\\site-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\python\\python38\\lib\\site-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: pyenchant in c:\\python\\python38\\lib\\site-packages (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\python\\python38\\lib\\site-packages (1.3.2)\n",
            "Requirement already satisfied: lightgbm in c:\\python\\python38\\lib\\site-packages (4.3.0)\n",
            "Requirement already satisfied: catboost in c:\\python\\python38\\lib\\site-packages (1.2.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.24.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: graphviz in c:\\python\\python38\\lib\\site-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in c:\\python\\python38\\lib\\site-packages (from catboost) (3.7.5)\n",
            "Requirement already satisfied: pandas>=0.24 in c:\\python\\python38\\lib\\site-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: plotly in c:\\python\\python38\\lib\\site-packages (from catboost) (5.19.0)\n",
            "Requirement already satisfied: six in c:\\python\\python38\\lib\\site-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (6.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\python\\python38\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\python\\python38\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.17.0)\n",
            "Requirement already satisfied: gensim in c:\\python\\python38\\lib\\site-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\python\\python38\\lib\\site-packages (from gensim) (1.24.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\python\\python38\\lib\\site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\python\\python38\\lib\\site-packages (from gensim) (7.0.1)\n",
            "Requirement already satisfied: wrapt in c:\\python\\python38\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: transformers in c:\\python\\python38\\lib\\site-packages (4.38.2)\n",
            "Requirement already satisfied: sentencepiece in c:\\python\\python38\\lib\\site-packages (0.2.0)\n",
            "Requirement already satisfied: sacremoses in c:\\python\\python38\\lib\\site-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in c:\\python\\python38\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\python\\python38\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\python\\python38\\lib\\site-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "#Install required packages and libraries\n",
        "\n",
        "!apt-get install libenchant-2-2\n",
        "!pip install emoji\n",
        "!pip install cleantext\n",
        "!pip install nltk\n",
        "!pip install pyenchant\n",
        "!pip install scikit-learn lightgbm catboost\n",
        "!pip install gensim\n",
        "!pip install transformers sentencepiece sacremoses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUeHgN3BM3x0"
      },
      "source": [
        "## Import data\n",
        "\n",
        "Get the required data files from github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PGYc5OXNBFh",
        "outputId": "f1e09cdd-9278-4fd3-eb8b-e9adda88df89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fatal: destination path 'suggestion-mining' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cse-teacher/suggestion-mining.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2FjFI8E7gDn"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xWr3Gqpm9264"
      },
      "outputs": [],
      "source": [
        "# Read data from input files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "#Set default seed:\n",
        "random.seed(42)\n",
        "\n",
        "#Main Application\n",
        "folder     = \"./suggestion-mining/data/\"\n",
        "train_file = folder + \"V1.4_Training.csv\" #\"Train_Augmented_03.csv\" # V1.4_Training.csv\" #  \"Train_processed.csv\" /suggestion-mining/data/Train_Augmented_03.csv\n",
        "valid_file = folder + \"SubtaskA_Trial_Test_Labeled.csv\" #\"validation_processed.csv\"\n",
        "test_file  = folder + \"SubtaskA_EvaluationData_labeled.csv\"\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "valid_df = pd.read_csv(valid_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "test_df  = pd.read_csv(test_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "all_df = pd.concat([train_df, valid_df, test_df], axis=0)\n",
        "\n",
        "\n",
        "#Get the labels:\n",
        "y_train_original = train_df['label'].values\n",
        "y_valid_original = valid_df['label'].values\n",
        "y_test_original  = test_df['label'].values\n",
        "y_all_original  = all_df['label'].values\n",
        "train_size = len(train_df['label'])\n",
        "valid_size = len(valid_df['label'])\n",
        "test_size  = len(test_df['label'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsaeHSk9PmO1"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a84ef8-fa33-473f-d6ac-9cefd7a29475",
        "id": "BriN_u_BU0oN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import re\n",
        "import nltk\n",
        "import cleantext\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_nonalphanumeric(text):\n",
        "    #text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
        "  text = re.sub(r'\\W+', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "def remove_stopwords_list(tokens):\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  filtered_tokens = remove_stopwords_list(tokens)\n",
        "  return ' '.join(filtered_tokens)\n",
        "\n",
        "#-----------------------------------\n",
        "# Replace hyperlinks\n",
        "#\n",
        "def replace_hyperlinks(text):\n",
        "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "  return text\n",
        "\n",
        "def stem(text):\n",
        "  tokens = word_tokenize(text.strip())\n",
        "  tokens_stem =[stemmer.stem(s) for s in tokens]\n",
        "  return ' '.join(tokens_stem)\n",
        "\n",
        "#----------------------------------------\n",
        "# replace_named_entities:\n",
        "#    Replaces each word or phrase in the input text with its\n",
        "#    Named Entity Recognition (NER) tag label.\n",
        "#    Args:\n",
        "#    text (str): Input text\n",
        "#\n",
        "#    Returns:\n",
        "#    str: Text with named entities replaced by their NER tag labels\n",
        "#\n",
        "def replace_named_entities(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Tag the words with Part-of-Speech (POS) tags\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    # Perform Named Entity Recognition (NER)\n",
        "    named_entities = ne_chunk(tagged_words)\n",
        "\n",
        "    # Replace entities with their NER tag labels\n",
        "    replaced_text = []\n",
        "    for entity in named_entities:\n",
        "        if isinstance(entity, nltk.tree.Tree):\n",
        "            label = entity.label()\n",
        "            named_entity_text = \" \".join([word for word, tag in entity.leaves()])\n",
        "            #replaced_text.append(f'<{label}>{named_entity_text}</{label}>')\n",
        "            replaced_text.append(f'{label}')\n",
        "            #replaced_text.append('')\n",
        "        else:\n",
        "            replaced_text.append(entity[0])\n",
        "\n",
        "    return \" \".join(replaced_text)\n",
        "\n",
        "#Global callings:\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Example usage:\n",
        "text = \"Microsoft should seriously look into getting rid of Syamentc for all these paying stuff\"\n",
        "replaced_text = replace_named_entities(text)\n",
        "print(\"Replaced Text:\", replaced_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Usages**"
      ],
      "metadata": {
        "id": "PUZfqpTfkuYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "op_replace_hyperlinks      = True\n",
        "op_remove_nonalphanumeric  = True\n",
        "op_remove_stopwords        = True\n",
        "op_replace_named_entities  = True\n",
        "op_stem                    = True\n",
        "\n",
        "if op_replace_hyperlinks == True:\n",
        "  #replace named entities with their tag names:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_hyperlinks)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_hyperlinks)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_hyperlinks)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_hyperlinks)\n",
        "\n",
        "if op_remove_nonalphanumeric == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_nonalphanumeric)\n",
        "\n",
        "if op_replace_named_entities == True:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_named_entities)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_named_entities)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_named_entities)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_named_entities)\n",
        "\n",
        "if op_remove_stopwords == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_stopwords)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_stopwords)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_stopwords)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_stopwords)\n",
        "\n",
        "if op_stem == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(stem)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(stem)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(stem)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(stem)\n"
      ],
      "metadata": {
        "id": "ceUQZN9Ltd8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD9BI9epYsc5"
      },
      "source": [
        "**BERT for Classification**:\n",
        "\n",
        "Import libraries: We import necessary libraries for loading BERT tokenizer and model, processing text, and making predictions.\n",
        "\n",
        "Load BERT tokenizer and model: We load the pre-trained bert-base-uncased tokenizer and model. Replace 'bert-base-uncased' with your desired pre-trained BERT model name.\n",
        "\n",
        "Preprocess text function: This function performs the following:\n",
        "\n",
        "Tokenizes the text using the BERT tokenizer.\n",
        "\n",
        "Adds special tokens ([CLS] and [SEP]) to the beginning and end of the sequence, respectively.\n",
        "\n",
        "Pads the sequence to a maximum length (MAX_LEN) if necessary.\n",
        "\n",
        "Define example text and label: Replace text with your actual text to classify and adjust label based on your classification categories.\n",
        "\n",
        "Preprocess text: Call the preprocess_text function to convert the text into the required format for BERT.\n",
        "\n",
        "Make prediction: Pass the preprocessed text through the model to obtain predictions.\n",
        "\n",
        "Get predicted class and probability: Extract the predicted class index and its corresponding probability from the prediction results.\n",
        "\n",
        "Print results: Print the predicted class and its probability.\n",
        "Note:\n",
        "\n",
        "This is a basic example and can be further customized for specific tasks like sentiment analysis or topic classification.\n",
        "Remember to install the required libraries (transformers and tensorflow) before running the code.\n",
        "Adjust MAX_LEN based on the maximum sentence length in your dataset.\n",
        "Sources\n",
        "github.com/JiaYaobo/toxic_detect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ftiBOLQwaQh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e2ee62c-88c1-45b1-b481-b09cfa1b1a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in c:\\python\\python38\\lib\\site-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in c:\\python\\python38\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\python\\python38\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\python\\python38\\lib\\site-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U tensorflow-text\n",
        "!pip install transformers\n",
        "!pip install -q tf-models-official"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Init Bert**"
      ],
      "metadata": {
        "id": "2PWHVcU6loZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "HF_TOKEN = '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'\n",
        "\n",
        "#Main Application\n",
        "folder     = \"./suggestion-mining/data/\"\n",
        "train_file = folder + \"Train_Augmented_03.csv\" # V1.4_Training.csv\" #  \"Train_processed.csv\"\n",
        "valid_file = folder + \"SubtaskA_Trial_Test_Labeled.csv\" #\"validation_processed.csv\"\n",
        "test_file  = folder + \"SubtaskA_EvaluationData_labeled.csv\"\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])"
      ],
      "metadata": {
        "id": "OBs36NrwlonS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Npr8F9hTQzwG",
        "outputId": "40a84ef8-fa33-473f-d6ac-9cefd7a29475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import re\n",
        "import nltk\n",
        "import cleantext\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_nonalphanumeric(text):\n",
        "    #text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
        "  text = re.sub(r'\\W+', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "def remove_stopwords_list(tokens):\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  filtered_tokens = remove_stopwords_list(tokens)\n",
        "  return ' '.join(filtered_tokens)\n",
        "\n",
        "#-----------------------------------\n",
        "# Replace hyperlinks\n",
        "#\n",
        "def replace_hyperlinks(text):\n",
        "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "  return text\n",
        "\n",
        "def stem(text):\n",
        "  tokens = word_tokenize(text.strip())\n",
        "  tokens_stem =[stemmer.stem(s) for s in tokens]\n",
        "  return ' '.join(tokens_stem)\n",
        "\n",
        "#----------------------------------------\n",
        "# replace_named_entities:\n",
        "#    Replaces each word or phrase in the input text with its\n",
        "#    Named Entity Recognition (NER) tag label.\n",
        "#    Args:\n",
        "#    text (str): Input text\n",
        "#\n",
        "#    Returns:\n",
        "#    str: Text with named entities replaced by their NER tag labels\n",
        "#\n",
        "def replace_named_entities(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Tag the words with Part-of-Speech (POS) tags\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    # Perform Named Entity Recognition (NER)\n",
        "    named_entities = ne_chunk(tagged_words)\n",
        "\n",
        "    # Replace entities with their NER tag labels\n",
        "    replaced_text = []\n",
        "    for entity in named_entities:\n",
        "        if isinstance(entity, nltk.tree.Tree):\n",
        "            label = entity.label()\n",
        "            named_entity_text = \" \".join([word for word, tag in entity.leaves()])\n",
        "            #replaced_text.append(f'<{label}>{named_entity_text}</{label}>')\n",
        "            replaced_text.append(f'{label}')\n",
        "            #replaced_text.append('')\n",
        "        else:\n",
        "            replaced_text.append(entity[0])\n",
        "\n",
        "    return \" \".join(replaced_text)\n",
        "\n",
        "#Global callings:\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Example usage:\n",
        "text = \"Microsoft should seriously look into getting rid of Syamentc for all these paying stuff\"\n",
        "replaced_text = replace_named_entities(text)\n",
        "print(\"Replaced Text:\", replaced_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op_replace_hyperlinks      = True\n",
        "op_remove_nonalphanumeric  = True\n",
        "op_remove_stopwords        = True\n",
        "op_replace_named_entities  = True\n",
        "op_stem                    = True\n",
        "\n",
        "if op_replace_hyperlinks == True:\n",
        "  #replace named entities with their tag names:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_hyperlinks)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_hyperlinks)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_hyperlinks)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_hyperlinks)\n",
        "\n",
        "if op_remove_nonalphanumeric == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_nonalphanumeric)\n",
        "\n",
        "if op_replace_named_entities == True:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_named_entities)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_named_entities)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_named_entities)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_named_entities)\n",
        "\n",
        "if op_remove_stopwords == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_stopwords)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_stopwords)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_stopwords)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_stopwords)\n",
        "\n",
        "if op_stem == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(stem)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(stem)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(stem)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(stem)\n"
      ],
      "metadata": {
        "id": "ZYWIFWb3TyLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming you have your documents stored in a list of strings named `documents`\n",
        "# Assuming you have your labels stored in a list named `labels` where 0 indicates one class and 1 indicates another class\n",
        "documents  = train_df['sentence'].tolist()\n",
        "labels = train_df['label'].tolist()\n",
        "# Split data into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(documents, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = BertModel.from_pretrained(model_name).to('cuda')  # Move model to GPU\n",
        "\n",
        "# Tokenize and encode the training and testing texts\n",
        "max_length = 128  # Adjust as needed\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
        "\n",
        "# Define DataLoader\n",
        "batch_size = 2  # Adjust as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define classification model\n",
        "class ClassificationModel(torch.nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.fc = torch.nn.Linear(self.bert.config.hidden_size, 2)  # Output size is 2 for binary classification\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Instantiate classification model\n",
        "classification_model = ClassificationModel(bert_model).to('cuda')  # Move model to GPU\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(classification_model.parameters(), lr=2e-5)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3  # Adjust as needed\n",
        "classification_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to('cuda'), attention_mask.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = classification_model(input_ids, attention_mask)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "classification_model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "for batch in tqdm(test_loader):\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids, attention_mask, labels = input_ids.to('cuda'), attention_mask.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = classification_model(input_ids, attention_mask)\n",
        "    _, predicted_labels = torch.max(outputs, 1)\n",
        "    predictions.extend(predicted_labels.cpu().numpy())\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "WBkI97GoI88e",
        "outputId": "8c063cc2-9a9a-467d-92c1-53eed5e01b66"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|██▊                                                                            | 210/5902 [00:10<04:35, 20.62it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     64\u001b[0m     input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 65\u001b[0m     input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     68\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m classification_model(input_ids, attention_mask)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}