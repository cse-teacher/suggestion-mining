{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cse-teacher/suggestion-mining/blob/main/suggestion_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMyi6aqLVl_v"
      },
      "source": [
        "# Suggestion Mining\n",
        "Suggestion mining is the task of extracting suggestions from user reviews\n",
        "\n",
        "Developed: 11 Feb 2024 \\\\\n",
        "Last Update: 11 Feb 2024 \\\\\n",
        "Author: Muharram Mansoorizadeh plus Various AI tools (Google search, chatGPT, Gemini , ...)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0rqNS_w7Wvo"
      },
      "source": [
        "## Install Required Packagaes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMhPx71AqsaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00276c9-6029-4db2-c8d4-229f85897611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common enchant-2 hunspell-en-us libaspell15 libhunspell-1.7-0\n",
            "  libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell | openoffice.org-core\n",
            "  libenchant-2-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common enchant-2 hunspell-en-us libaspell15 libenchant-2-2\n",
            "  libhunspell-1.7-0 libtext-iconv-perl\n",
            "0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,431 kB of archives.\n",
            "After this operation, 5,501 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtext-iconv-perl amd64 1.7-7build3 [14.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaspell15 amd64 0.60.8-4build1 [325 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 dictionaries-common all 1.28.14 [185 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 aspell amd64 0.60.8-4build1 [87.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 aspell-en all 2018.04.16-0-1 [299 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-en-us all 1:2020.12.07-2 [280 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhunspell-1.7-0 amd64 1.7.0-4build1 [175 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libenchant-2-2 amd64 2.3.2-1ubuntu2 [50.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 enchant-2 amd64 2.3.2-1ubuntu2 [13.0 kB]\n",
            "Fetched 1,431 kB in 0s (5,992 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-7build3_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-7build3) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.8-4build1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.8-4build1) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../2-dictionaries-common_1.28.14_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.28.14) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../3-aspell_0.60.8-4build1_amd64.deb ...\n",
            "Unpacking aspell (0.60.8-4build1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../4-aspell-en_2018.04.16-0-1_all.deb ...\n",
            "Unpacking aspell-en (2018.04.16-0-1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../5-hunspell-en-us_1%3a2020.12.07-2_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2020.12.07-2) ...\n",
            "Selecting previously unselected package libhunspell-1.7-0:amd64.\n",
            "Preparing to unpack .../6-libhunspell-1.7-0_1.7.0-4build1_amd64.deb ...\n",
            "Unpacking libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Selecting previously unselected package libenchant-2-2:amd64.\n",
            "Preparing to unpack .../7-libenchant-2-2_2.3.2-1ubuntu2_amd64.deb ...\n",
            "Unpacking libenchant-2-2:amd64 (2.3.2-1ubuntu2) ...\n",
            "Selecting previously unselected package enchant-2.\n",
            "Preparing to unpack .../8-enchant-2_2.3.2-1ubuntu2_amd64.deb ...\n",
            "Unpacking enchant-2 (2.3.2-1ubuntu2) ...\n",
            "Setting up libtext-iconv-perl (1.7-7build3) ...\n",
            "Setting up dictionaries-common (1.28.14) ...\n",
            "Setting up libaspell15:amd64 (0.60.8-4build1) ...\n",
            "Setting up aspell (0.60.8-4build1) ...\n",
            "Setting up hunspell-en-us (1:2020.12.07-2) ...\n",
            "Setting up libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Setting up libenchant-2-2:amd64 (2.3.2-1ubuntu2) ...\n",
            "Setting up aspell-en (2018.04.16-0-1) ...\n",
            "Setting up enchant-2 (2.3.2-1ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dictionaries-common (1.28.14) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.11.0\n"
          ]
        }
      ],
      "source": [
        "#Install required packages and libraries\n",
        "\n",
        "!apt-get install libenchant-2-2\n",
        "!pip install emoji\n",
        "!pip install cleantext\n",
        "!pip install nltk\n",
        "!pip install pyenchant\n",
        "!pip install scikit-learn lightgbm catboost\n",
        "!pip install gensim\n",
        "!pip install transformers sentencepiece sacremoses\n",
        "!pip install ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "FJz6I9CXqXzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568ad7e6-f407-4c84-9d73-22e040d87f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "ner_categories =['PERSON', 'PRDOUCT' , 'ORG', 'GPE']\n"
      ],
      "metadata": {
        "id": "8n2Bl0jfrA8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'John drives to Sidny school every day with his windows phone made by microsoft'\n",
        "doc = nlp(text)\n",
        "print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iao_ESjr0Sp",
        "outputId": "6537fa4b-4dff-474b-f262-044a3de5b23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John drives to Sidny school every day with his windows phone made by microsoft\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "  print(ent.text , ent.label, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2HMPUImsHht",
        "outputId": "59731bf0-e1ff-4192-c122-63c6ba16a9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John 380 PERSON\n",
            "Sidny 384 GPE\n",
            "microsoft 383 ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUeHgN3BM3x0"
      },
      "source": [
        "## Import data\n",
        "\n",
        "Get the required data files from github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PGYc5OXNBFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe9f94e-115f-4df7-db8e-c1c09dcde62d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'suggestion-mining'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 126 (delta 33), reused 0 (delta 0), pack-reused 72\u001b[K\n",
            "Receiving objects: 100% (126/126), 2.50 MiB | 21.49 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cse-teacher/suggestion-mining.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2FjFI8E7gDn"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWr3Gqpm9264"
      },
      "outputs": [],
      "source": [
        "# Read data from input files\n",
        "#Reset environment\n",
        "%reset -f\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "\n",
        "#Set default seed:\n",
        "random.seed(42)\n",
        "\n",
        "#Main Application\n",
        "folder     = \"./suggestion-mining/data/\"\n",
        "train_file = folder + \"V1.4_Training.csv\" #\"Train_Augmented_03.csv\" # V1.4_Training.csv\" #  \"Train_processed.csv\" /suggestion-mining/data/Train_Augmented_03.csv\n",
        "valid_file = folder + \"SubtaskA_Trial_Test_Labeled.csv\" #\"validation_processed.csv\"\n",
        "test_file  = folder + \"SubtaskA_EvaluationData_labeled.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "valid_df = pd.read_csv(valid_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "test_df  = pd.read_csv(test_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "all_df = pd.concat([train_df, valid_df, test_df], axis=0)\n",
        "\n",
        "\n",
        "#Get the labels:\n",
        "y_train_original = train_df['label'].values\n",
        "y_valid_original = valid_df['label'].values\n",
        "y_test_original  = test_df['label'].values\n",
        "y_all_original  = all_df['label'].values\n",
        "train_size = len(train_df['label'])\n",
        "valid_size = len(valid_df['label'])\n",
        "test_size  = len(test_df['label'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsaeHSk9PmO1"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npr8F9hTQzwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99b05f79-43e0-432d-ba7d-c1756231b4ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced Text: PERSON should seriously look into getting rid of GPE for all these paying stuff\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import re\n",
        "import nltk\n",
        "import cleantext\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_nonalpha(text):\n",
        "    #text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
        "  text = re.sub(r'[^A-Za-z]+', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "def remove_nonalphanumeric(text):\n",
        "    #text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
        "  text = re.sub(r'\\W+', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "def remove_stopwords_list(tokens):\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  filtered_tokens = remove_stopwords_list(tokens)\n",
        "  return ' '.join(filtered_tokens)\n",
        "\n",
        "#-----------------------------------\n",
        "# Replace hyperlinks\n",
        "#\n",
        "def replace_hyperlinks(text):\n",
        "  text = re.sub(r'https?:\\/\\/\\S+', 'hyperlink', text)\n",
        "  return text\n",
        "\n",
        "def stem(text):\n",
        "  tokens = word_tokenize(text.strip())\n",
        "  tokens_stem =[stemmer.stem(s) for s in tokens]\n",
        "  return ' '.join(tokens_stem)\n",
        "\n",
        "#----------------------------------------\n",
        "# replace_named_entities:\n",
        "#    Replaces each word or phrase in the input text with its\n",
        "#    Named Entity Recognition (NER) tag label.\n",
        "#    Args:\n",
        "#    text (str): Input text\n",
        "#\n",
        "#    Returns:\n",
        "#    str: Text with named entities replaced by their NER tag labels\n",
        "#\n",
        "def replace_named_entities(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Tag the words with Part-of-Speech (POS) tags\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    # Perform Named Entity Recognition (NER)\n",
        "    named_entities = ne_chunk(tagged_words)\n",
        "\n",
        "    # Replace entities with their NER tag labels\n",
        "    replaced_text = []\n",
        "    for entity in named_entities:\n",
        "        if isinstance(entity, nltk.tree.Tree):\n",
        "            label = entity.label()\n",
        "            named_entity_text = \" \".join([word for word, tag in entity.leaves()])\n",
        "            #replaced_text.append(f'<{label}>{named_entity_text}</{label}>')\n",
        "            replaced_text.append(f'{label}')\n",
        "            #replaced_text.append('')\n",
        "        else:\n",
        "            replaced_text.append(entity[0])\n",
        "\n",
        "    return \" \".join(replaced_text)\n",
        "\n",
        "#Global callings:\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Example usage:\n",
        "text = \"Microsoft should seriously look into getting rid of Syamentc for all these paying stuff\"\n",
        "replaced_text = replace_named_entities(text)\n",
        "print(\"Replaced Text:\", replaced_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op_replace_hyperlinks      = True\n",
        "op_remove_nonalphanumeric  = True\n",
        "op_remove_nonalpha         = True\n",
        "op_remove_stopwords        = False\n",
        "op_replace_named_entities  = False\n",
        "op_stem                    = False\n",
        "\n",
        "if op_replace_hyperlinks == True:\n",
        "  #replace named entities with their tag names:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_hyperlinks)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_hyperlinks)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_hyperlinks)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_hyperlinks)\n",
        "\n",
        "if op_remove_nonalphanumeric == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_nonalphanumeric)\n",
        "\n",
        "if op_remove_nonalpha  == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_nonalpha)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_nonalpha)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_nonalpha)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_nonalpha)\n",
        "\n",
        "\n",
        "if op_replace_named_entities == True:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_named_entities)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_named_entities)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_named_entities)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_named_entities)\n",
        "\n",
        "if op_remove_stopwords == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_stopwords)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_stopwords)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_stopwords)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_stopwords)\n",
        "\n",
        "if op_stem == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(stem)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(stem)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(stem)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(stem)\n"
      ],
      "metadata": {
        "id": "ceUQZN9Ltd8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['sentence'][195:200].tolist()\n"
      ],
      "metadata": {
        "id": "JFHg96jPWWUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97011f81-d7ff-4406-facf-340b212cc826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' When creating an app that uses MediaStreamSource to stream audio in Windows Phone everything works just great ',\n",
              " ' When porting this app to Windows Phone the sound is flickering and its components are stack overflow ing ',\n",
              " ' Here is a discussion on MSDN forums hyperlink And here is a sample project source code hyperlink',\n",
              " ' we are publishing the same apps in Windows Phone Store and Windows App Store ',\n",
              " ' Now we want to bundle these Apps ']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqJSjOY3bZPl"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMghoc88kqxr"
      },
      "outputs": [],
      "source": [
        "#Extract BOW feature test\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "#----------------------\n",
        "# BOW Features\n",
        "bow_vectorizer = CountVectorizer(analyzer='word',\n",
        "                                 stop_words=None,\n",
        "                                 lowercase=True,\n",
        "                                 encoding='utf-8',\n",
        "                                 min_df = 3 ,\n",
        "                                 max_df = 0.975,\n",
        "                                 ngram_range =(1,5))\n",
        "\n",
        "bow_vectorizer.fit(all_df['sentence'])\n",
        "train_bow_features = bow_vectorizer.transform(train_df['sentence']).toarray()\n",
        "valid_bow_features = bow_vectorizer.transform(valid_df['sentence']).toarray()\n",
        "test_bow_features  = bow_vectorizer.transform(test_df['sentence']).toarray()\n",
        "all_bow_features   = bow_vectorizer.transform(all_df['sentence']).toarray()\n",
        "\n",
        "#----------------------\n",
        "# TF-IDF Features\n",
        "\n",
        "# Fit the vectorizer on the sentences to learn vocabulary and IDF weights\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=None,\n",
        "                                 lowercase=True,\n",
        "                                 encoding='utf-8',\n",
        "                                 min_df = 3 ,\n",
        "                                 max_df = 0.95, #                                 max_features = 5000,\n",
        "                                 ngram_range =(1,5))\n",
        "\n",
        "tfidf_vectorizer.fit(all_df['sentence'])\n",
        "\n",
        "# Transform the sentences into tf-idf vectors\n",
        "train_tfidf_features = tfidf_vectorizer.transform(train_df['sentence']).toarray()\n",
        "test_tfidf_features  = tfidf_vectorizer.transform(test_df['sentence']).toarray()\n",
        "valid_tfidf_features = tfidf_vectorizer.transform(valid_df['sentence']).toarray()\n",
        "all_tfidf_features   = tfidf_vectorizer.transform(all_df['sentence']).toarray()\n",
        "\n",
        "#------------------------------------------------\n",
        "# word2vec features\n",
        "#\n",
        "docs = [wordpunct_tokenize(doc) for doc in all_df['sentence']]\n",
        "docs1 = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
        "model = Doc2Vec(docs1, vector_size=300, window=4, min_count=1, workers=4, epochs=100)\n",
        "\n",
        "#Get the features:\n",
        "vectors = [model.infer_vector(doc) for doc in(docs)]\n",
        "all_d2v_features = np.array(vectors)\n",
        "train_d2v_features = all_d2v_features[0:train_size,:]\n",
        "valid_d2v_features = all_d2v_features[train_size:train_size+valid_size,:]\n",
        "test_d2v_features  = all_d2v_features[train_size+valid_size:,:]\n",
        "\n",
        "#define global features, empty at first:\n",
        "X_train     = np.empty([])\n",
        "X_test      = np.empty([])\n",
        "X_valid     = np.empty([])\n",
        "X_all       = np.empty([])\n",
        "X_train_val = np.empty([])\n",
        "\n",
        "y_train = y_train_original\n",
        "y_valid = y_valid_original\n",
        "y_test  = y_test_original\n",
        "y_all   = y_all_original\n",
        "y_train_val = np.concatenate((y_train , y_valid), axis= 0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7Yunwtnxl3c"
      },
      "outputs": [],
      "source": [
        "#===============================================\n",
        "# Utility functions\n",
        "#\n",
        "\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from nltk.tokenize import word_tokenize\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "#------------------------------------------------\n",
        "# apply the given option\n",
        "#\n",
        "def select_optional_features(feature_group,\n",
        "                 op_scale_features  = False,\n",
        "                 op_upsample_smote  = False,\n",
        "                 op_upsample_over   = False,\n",
        "                 op_transform_pca   = False,\n",
        "                 op_downsample_majority = False\n",
        "                 ):\n",
        "  global X_train, X_valid, X_test, X_all\n",
        "  global y_train, y_valid, y_test, y_all\n",
        "  global X_train_val , y_train_val\n",
        "  global results_df, start_time_str\n",
        "\n",
        "  description = feature_group\n",
        "  y_train = y_train_original\n",
        "  y_valid = y_valid_original\n",
        "  y_test  = y_test_original\n",
        "  # Convert the dictionary into DataFrame\n",
        "  results_df = pd.DataFrame({'labels': y_test})\n",
        "  start_time_str = f\"{datetime.now()}\"\n",
        "\n",
        "\n",
        "  if feature_group == 'tfidf' :\n",
        "    X_train = train_tfidf_features\n",
        "    X_test  = test_tfidf_features\n",
        "    X_valid = valid_tfidf_features\n",
        "    X_all   = all_tfidf_features\n",
        "  elif feature_group == 'bow':\n",
        "    X_train = train_bow_features\n",
        "    X_test  = test_bow_features\n",
        "    X_valid = valid_bow_features\n",
        "    X_all   = all_bow_features\n",
        "  elif feature_group == 'd2v':\n",
        "    X_train = train_d2v_features\n",
        "    X_test  = test_d2v_features\n",
        "    X_valid = valid_d2v_features\n",
        "    X_all   = all_d2v_features\n",
        "\n",
        "  if op_scale_features == True: # Scale numerical features\n",
        "     scaler  = StandardScaler().fit(X_all); description += ', Standard Scaler'\n",
        "     X_all   = scaler.transform(X_all)\n",
        "     X_train = scaler.transform(X_train)\n",
        "     X_test  = scaler.transform(X_test)\n",
        "     X_valid = scaler.transform(X_valid)\n",
        "\n",
        "  if op_upsample_smote == True: # SMOTE oversampling\n",
        "    smote = SMOTE(sampling_strategy=\"minority\") ; description += ', SMOTE Augmentation'\n",
        "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "  if op_upsample_over == True: # Random oversampling\n",
        "    oversampler = RandomOverSampler(random_state=42); description += ', oversampling Augmentation'\n",
        "    X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "  if op_transform_pca == True:  # Do PCA\n",
        "    #n_comps = min(500 , 0.1 * X_all.shape[1])\n",
        "    pca = PCA(n_components=0.95).fit(X_all);  description += ', PCA'\n",
        "    X_train = pca.transform(X_train) ; X_test = pca.transform(X_test)\n",
        "    X_valid = pca.transform(X_valid) ; X_all = pca.transform(X_all)\n",
        "\n",
        "  if op_downsample_majority == True:  # Down sample majority class\n",
        "      # Separate instances for class 1\n",
        "    class_1_instances = X_train[y_train == 1,:]\n",
        "    class_0_instances = X_train[y_train == 0,:]\n",
        "    number_of_samples = class_1_instances.shape[0]\n",
        "    indices = np.random.choice(class_0_instances.shape[0], number_of_samples, replace=False)\n",
        "    sampled_class_0_instances = class_0_instances[indices,:]\n",
        "\n",
        "    # Combine instances for class 1 and sampled instances from class 0\n",
        "    X_train = np.concatenate([class_1_instances, sampled_class_0_instances])\n",
        "    y_train = np.concatenate([np.ones(class_1_instances.shape[0]), np.zeros(sampled_class_0_instances.shape[0])])\n",
        "\n",
        "\n",
        "    # Train + Validation data\n",
        "  X_train_val = np.concatenate((X_train, X_valid) , axis=0)\n",
        "  y_train_val = np.concatenate((y_train, y_valid) , axis=0)\n",
        "  return description\n",
        "\n",
        "#----------------------------------\n",
        "# Print results per class\n",
        "#\n",
        "def print_per_class_results(y_actual, y_pred, description=''):\n",
        "  for label in (0,1):\n",
        "    v0 = accuracy_score(y_actual, y_pred)\n",
        "    v1 = precision_score(y_actual, y_pred, pos_label=label)\n",
        "    v2 = recall_score(y_actual, y_pred, pos_label=label)\n",
        "    v3 = f1_score(y_actual, y_pred, pos_label=label)\n",
        "    print(f\"{description},\\t class={label}\\tAccuracy={v0:.2f},\\t Precision={v1:.2f},\\tRecall={v2:.2f}\\tF1-score={v3:.2f}\")\n",
        "\n",
        "\n",
        "#----------------------------------\n",
        "# Print results per class\n",
        "#\n",
        "def print_results(y_actual, y_pred, description=''):\n",
        "  try:\n",
        "    v00 = accuracy_score(y_actual, y_pred)\n",
        "    v01 = precision_score(y_actual, y_pred, pos_label=0)\n",
        "    v02 = recall_score(y_actual, y_pred, pos_label=0)\n",
        "    v03 = f1_score(y_actual, y_pred, pos_label=0)\n",
        "\n",
        "    v11 = precision_score(y_actual, y_pred, pos_label=1)\n",
        "    v12 = recall_score(y_actual, y_pred, pos_label=1)\n",
        "    v13 = f1_score(y_actual, y_pred, pos_label=1)\n",
        "\n",
        "    smsg = f\"{description},\\tAccuracy={v00:.2f},\\tC0: Pr={v01:.2f}, Re={v02:.2f}, F1={v03:.2f},\\tC1: Pr={v11:.2f}, Re={v12:.2f}, F1={v13:.2f}\"\n",
        "    print(smsg)\n",
        "    with open(f\"results_{start_time_str}.txt\", \"a\") as myfile:\n",
        "      myfile.write(f\"{datetime.now()}\\t {smsg}\\n\")\n",
        "\n",
        "    results_df.insert(len(results_df.columns),description, y_pred)\n",
        "  except Exception as error:\n",
        "      print(f\"something went wrong {error}\")\n",
        "\n",
        "#cutoff probability to make a binary value\n",
        "def prob2label (y, threshold=0.5):\n",
        "  y[y <  threshold] = 0\n",
        "  y[y >= threshold] = 1\n",
        "  return y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ttest based keywords selection:\n",
        "# Import the library\n",
        "import scipy.stats as stats\n",
        "\n",
        "def ttest2(X,y):\n",
        "  X1 = X[y==1, :]; X2 =X[y!=1,:] ;\n",
        "  numcols = X.shape[1]\n",
        "  sval = np.zeros(numcols, float)\n",
        "  pval = np.zeros(numcols, float)\n",
        "  for k in range(0,numcols):\n",
        "    test_result = stats.ttest_ind(a=X1[:,k], b=X2[:,k], equal_var=True)\n",
        "    sval[k] = test_result.statistic\n",
        "    pval[k] = test_result.pvalue\n",
        "  return sval, pval\n",
        "\n",
        "# filter bow features\n",
        "cutoff = 0.22\n",
        "[A,B] = ttest2(train_bow_features , y_train)\n",
        "best_bow_features = np.argsort(B, axis=-1, kind=None, order=None)\n",
        "last_bow_feature = np.min(np.argwhere(B[best_bow_features] > cutoff))\n",
        "best_bow_feature_names = bow_vectorizer.get_feature_names_out()[best_bow_features[0:last_bow_feature]]\n",
        "\n",
        "train_bow_features= train_bow_features[:,best_bow_features[0:last_bow_feature]]\n",
        "test_bow_features = test_bow_features[:,best_bow_features[0:last_bow_feature]]\n",
        "valid_bow_features = valid_bow_features[:,best_bow_features[0:last_bow_feature]]\n",
        "all_bow_features   = all_bow_features[:,best_bow_features[0:last_bow_feature]]\n",
        "\n",
        "# filter tfidf features\n",
        "[A,B] = ttest2(train_tfidf_features , y_train)\n",
        "best_tfidf_features = np.argsort(B, axis=-1, kind=None, order=None)\n",
        "last_tfidf_feature  = np.min(np.argwhere(B[best_tfidf_features] > cutoff))\n",
        "best_tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()[best_tfidf_features[0:last_tfidf_feature]]\n",
        "\n",
        "train_tfidf_features= train_tfidf_features[:,best_tfidf_features[0:last_tfidf_feature]]\n",
        "test_tfidf_features = test_tfidf_features[:,best_tfidf_features[0:last_tfidf_feature]]\n",
        "valid_tfidf_features = valid_tfidf_features[:,best_tfidf_features[0:last_tfidf_feature]]\n",
        "all_tfidf_features   = all_tfidf_features[:,best_tfidf_features[0:last_tfidf_feature]]\n"
      ],
      "metadata": {
        "id": "07ktlt3mJnCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Top 10 bow features{best_bow_feature_names[0:10]}\")\n",
        "print(f\"Top 10 tfidf features{best_tfidf_feature_names[0:10]}\")\n",
        "results_df.to_csv(f\"labels_{start_time_str}.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUGUdvdfh9Uz",
        "outputId": "aa7e1b85-64b6-4211-d4cb-6de1718abb8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 bow features['be' 'please' 'would' 'should' 'would be' 'it would be' 'it would' 'add'\n",
            " 'should be' 'to']\n",
            "Top 10 tfidf features['be' 'should' 'it would be' 'please' 'it would' 'would be' 'would' 'add'\n",
            " 'should be' 'to']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srQ0TELENoKf"
      },
      "source": [
        "# Experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzpAtHXe5qsC"
      },
      "source": [
        "## Utility Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8KPz9KChLrA"
      },
      "source": [
        "**Experimental Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7M08KHYgBc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3898002a-88da-4680-efa1-81ec2978bec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf\n"
          ]
        }
      ],
      "source": [
        " #select options here and run classifiers as you like:\n",
        "\n",
        " current_options = select_optional_features(feature_group = 'tfidf',\n",
        "                 op_scale_features  = False,\n",
        "                 op_upsample_smote  = False,\n",
        "                 op_upsample_over   = False,\n",
        "                 op_transform_pca   = False ,\n",
        "                 op_downsample_majority = False,\n",
        "\n",
        "                                            )\n",
        "\n",
        " print(current_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rule Based Methods##\n",
        "\n",
        "This section contains several rule based methods."
      ],
      "metadata": {
        "id": "MEBPxuRbgF3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "suggestion_keywords = [\"should\", \"could\", \"might\", \"ought to\", \"would\", \"recommend\", \"suggest\", \"consider\", \"better\", \"allow\" ]\n",
        "polite_phrases = [\"would you mind\", \"could you please\", \"I suggest\", \"please\", \"if you want to\", \"be able to\", \"it would be\"]\n",
        "learned_keywords =best_bow_feature_names[0:1]\n",
        "\n",
        "def contains_suggestion(paragraph):\n",
        "    for keyword in suggestion_keywords:\n",
        "        if keyword in paragraph.lower():\n",
        "            return True\n",
        "    for phrase in polite_phrases:\n",
        "        if phrase in paragraph.lower():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def classify_paragraphs(paragraphs):\n",
        "    y_pred = []\n",
        "    for paragraph in paragraphs:\n",
        "        if contains_suggestion(paragraph):\n",
        "            y_pred.append(1)\n",
        "        else:\n",
        "            y_pred.append(0)\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "y_pred = classify_paragraphs(test_df['sentence'])\n",
        "print_results(y_test, y_pred, 'keywords ' )"
      ],
      "metadata": {
        "id": "kq0dRjOaPM3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b653c50f-ea6b-48d3-d6e7-ff4ab4aed352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keywords ,\tAccuracy=0.87,\tC0: Pr=0.97, Re=0.89, F1=0.93,\tC1: Pr=0.44, Re=0.76, F1=0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Discrimination Analysis**"
      ],
      "metadata": {
        "id": "AQroVkX_1_eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = lda.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "print_results(y_test , y_pred, 'LDA, ' + current_options )\n"
      ],
      "metadata": {
        "id": "m7DrD9Zx0_fD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a19b52-22b9-4568-b0b5-5c8148961c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA, tfidf,\tAccuracy=0.76,\tC0: Pr=0.93, Re=0.79, F1=0.86,\tC1: Pr=0.22, Re=0.51, F1=0.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = RandomForestClassifier(n_estimators=501, n_jobs=-1,verbose=1)\n",
        "clf.fit(X_train_val, y_train_val)\n",
        "y_pred = clf.predict(X_test)\n",
        "print_results(y_test , y_pred, 'random forest' + ', '+ current_options)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMNW8t0ME3_x",
        "outputId": "f0439cc6-ea74-424a-ef7d-451a505750b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   11.7s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   47.8s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=-1)]: Done 501 out of 501 | elapsed:  2.0min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random forest, tfidf,\tAccuracy=0.93,\tC0: Pr=0.95, Re=0.97, F1=0.96,\tC1: Pr=0.72, Re=0.60, F1=0.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 501 out of 501 | elapsed:    0.4s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypMLOuIfkaoB"
      },
      "source": [
        "**Basic Methods**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create MinMaxScaler instance with feature_range=(0, 10)\n",
        "scaler = MinMaxScaler(feature_range=(10, 20))\n",
        "\n",
        "# Fit the scaler to your data\n",
        "scaler.fit(X_all)\n",
        "\n",
        "# Transform your data\n",
        "X1 = scaler.transform(X_train_val)\n",
        "X2 = scaler.transform(X_test)\n",
        "\n",
        "# Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X1, y_train_val)\n",
        "nb_predictions = nb_classifier.predict(X2)\n",
        "\n",
        "# Bayesian classifier\n",
        "bayesian_classifier = BayesianRidge()\n",
        "bayesian_classifier.fit(X1, y_train_val)\n",
        "bayesian_predictions = bayesian_classifier.predict(X2)\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "print_results(y_test , nb_predictions >=0.5, 'Naive Bayes, '+ current_options)\n",
        "print_results(y_test , bayesian_predictions>=0.5, 'BayesianRidge, '+ current_options)\n",
        "\n"
      ],
      "metadata": {
        "id": "EIeAyjlbUl1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a0605b-56ac-4c76-9e4e-958b3f469485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes, tfidf,\tAccuracy=0.90,\tC0: Pr=0.90, Re=1.00, F1=0.95,\tC1: Pr=0.57, Re=0.05, F1=0.09\n",
            "BayesianRidge, tfidf,\tAccuracy=0.91,\tC0: Pr=0.95, Re=0.95, F1=0.95,\tC1: Pr=0.57, Re=0.60, F1=0.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLzrQpgakgHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0efacfa2-053c-4bcb-f5f3-bfd6ebd9c106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, tfidf,\tAccuracy=0.91,\tC0: Pr=0.92, Re=0.99, F1=0.95,\tC1: Pr=0.68, Re=0.26, F1=0.38\n",
            "basic Logistic Regression, tfidf,\tAccuracy=0.92,\tC0: Pr=0.94, Re=0.98, F1=0.96,\tC1: Pr=0.68, Re=0.45, F1=0.54\n",
            "basic Support Vector Machine-L, tfidf,\tAccuracy=0.92,\tC0: Pr=0.95, Re=0.96, F1=0.96,\tC1: Pr=0.63, Re=0.60, F1=0.62\n",
            "basic Support Vector Machine-R, tfidf,\tAccuracy=0.89,\tC0: Pr=0.95, Re=0.93, F1=0.94,\tC1: Pr=0.48, Re=0.55, F1=0.51\n",
            "basic Support Vector Machine-S, tfidf,\tAccuracy=0.91,\tC0: Pr=0.96, Re=0.94, F1=0.95,\tC1: Pr=0.57, Re=0.67, F1=0.62\n",
            "basic Support Vector Machine-WL, tfidf,\tAccuracy=0.84,\tC0: Pr=0.98, Re=0.83, F1=0.90,\tC1: Pr=0.37, Re=0.85, F1=0.52\n",
            "basic Support Vector Machine-WR, tfidf,\tAccuracy=0.89,\tC0: Pr=0.97, Re=0.90, F1=0.93,\tC1: Pr=0.47, Re=0.74, F1=0.57\n",
            "basic Support Vector Machine-WS, tfidf,\tAccuracy=0.76,\tC0: Pr=0.98, Re=0.75, F1=0.85,\tC1: Pr=0.29, Re=0.89, F1=0.44\n",
            "basic Decision Tree classifier, tfidf,\tAccuracy=0.90,\tC0: Pr=0.95, Re=0.93, F1=0.94,\tC1: Pr=0.51, Re=0.62, F1=0.56\n"
          ]
        }
      ],
      "source": [
        "#Some Useful classifiers\n",
        "def test_basic_models(X1,y1,X2,y2, description):\n",
        "  classifiers = {\n",
        "      'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=21, metric=\"cosine\"),\n",
        "      'Logistic Regression': sklearn.linear_model.LogisticRegression(random_state=42),\n",
        "      'Support Vector Machine-L': sklearn.svm.SVC(kernel='linear', random_state=42),\n",
        "      'Support Vector Machine-R': sklearn.svm.SVC(kernel='rbf', random_state=42),\n",
        "      'Support Vector Machine-S': sklearn.svm.SVC(kernel='sigmoid', random_state=42),\n",
        "      'Support Vector Machine-WL': sklearn.svm.SVC(kernel=\"linear\", class_weight={1: 10}, random_state=42),\n",
        "      'Support Vector Machine-WR': sklearn.svm.SVC(kernel=\"rbf\", class_weight={1: 10}, random_state=42),\n",
        "      'Support Vector Machine-WS': sklearn.svm.SVC(kernel=\"sigmoid\", class_weight={1: 10}, random_state=42),\n",
        "      'Decision Tree classifier': DecisionTreeClassifier(max_depth=15, random_state=42),\n",
        "  }\n",
        "\n",
        "  # Loop through each classifier and evaluate performance\n",
        "  for name, clf in classifiers.items():\n",
        "      clf.fit(X1, y1)\n",
        "      y_pred = clf.predict(X2)\n",
        "      print_results(y2 , y_pred, 'basic ' + name + ', '+ description)\n",
        "#---------------------\n",
        "test_basic_models (X_train_val , y_train_val , X_test , y_test, current_options)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdGv3RxdMA5r"
      },
      "source": [
        "**Ensemble Models**\n",
        "\n",
        "This experiment trains well-known ensemble methods on the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0xQuTCmMCAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9de2d2b-33f2-47f7-ec32-772f5a7a38cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    9.9s\n",
            "[Parallel(n_jobs=-1)]: Done 101 out of 101 | elapsed:   20.9s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 101 out of 101 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble, Random Forest, tfidf,\tAccuracy=0.94,\tC0: Pr=0.96, Re=0.97, F1=0.97,\tC1: Pr=0.73, Re=0.67, F1=0.70\n",
            "Ensemble, AdaBoost, tfidf,\tAccuracy=0.91,\tC0: Pr=0.95, Re=0.94, F1=0.95,\tC1: Pr=0.54, Re=0.60, F1=0.57\n",
            "Ensemble, Gradient Boosting, tfidf,\tAccuracy=0.92,\tC0: Pr=0.95, Re=0.96, F1=0.96,\tC1: Pr=0.62, Re=0.61, F1=0.61\n",
            "Ensemble, Extra Trees, tfidf,\tAccuracy=0.92,\tC0: Pr=0.95, Re=0.96, F1=0.96,\tC1: Pr=0.65, Re=0.59, F1=0.62\n",
            "[LightGBM] [Info] Number of positive: 2381, number of negative: 6711\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 26574\n",
            "[LightGBM] [Info] Number of data points in the train set: 9092, number of used features: 860\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.261879 -> initscore=-1.036227\n",
            "[LightGBM] [Info] Start training from score -1.036227\n",
            "Ensemble, LightGBM, tfidf,\tAccuracy=0.91,\tC0: Pr=0.96, Re=0.95, F1=0.95,\tC1: Pr=0.58, Re=0.64, F1=0.61\n",
            "Ensemble, CatBoost, tfidf,\tAccuracy=0.92,\tC0: Pr=0.96, Re=0.95, F1=0.96,\tC1: Pr=0.62, Re=0.63, F1=0.62\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def test_ensemble_models(X1,y1,X2,y2, description):\n",
        "  # Initialize classifiers\n",
        "  classifiers = {\n",
        "      \"Random Forest\"     : RandomForestClassifier(n_estimators=101,class_weight={0:1,1:10}, n_jobs=-1,verbose=1),\n",
        "      \"AdaBoost\"          : AdaBoostClassifier(n_estimators=101),\n",
        "      \"Gradient Boosting\" : GradientBoostingClassifier(),\n",
        "      \"Extra Trees\"       : ExtraTreesClassifier(),\n",
        "      \"LightGBM\"          : LGBMClassifier(),\n",
        "      \"CatBoost\"          : CatBoostClassifier(verbose=0)\n",
        "  }\n",
        "\n",
        "  # Loop through each classifier and evaluate performance\n",
        "  for name, clf in classifiers.items():\n",
        "      clf.fit(X1, y1)\n",
        "      y_pred = clf.predict(X2)\n",
        "      print_results(y2 , y_pred, 'Ensemble, ' + name + ', ' + description)\n",
        "\n",
        "\n",
        "# Train and evaluate ensemble models\n",
        "test_ensemble_models (X_train_val , y_train_val , X_test , y_test, current_options)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "D0p6161Zx-Pq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a2b5b3-b7bd-49f9-d986-d663df411fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8500, 6351)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHxOrXSPovV0"
      },
      "source": [
        "**Neural Networks**\n",
        "\n",
        "This network is trained on the training and validation sets and\n",
        "tested on the testing set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datetime import datetime\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # Input layer\n",
        "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "        self.layers.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
        "        self.layers.append(nn.Tanh())\n",
        "        self.layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(hidden_sizes)):\n",
        "            self.layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
        "            self.layers.append(nn.BatchNorm1d(hidden_sizes[i]))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            self.layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
        "        self.layers.append(nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "input_size = X_train_val.shape[1]  # Adjust this based on your input features\n",
        "hidden_sizes = [500, 250, 100, 50]\n",
        "\n",
        "# Instantiate the model\n",
        "model_mlp2 = MLPModel(input_size, hidden_sizes)\n",
        "\n",
        "# Check if GPU is available and move the model and data to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_mlp2.to(device)\n",
        "print(device)\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()  #nn.BCEWithLogitsLoss() #nn.CrossEntropyLoss() #nn.MSELoss() #nn.KLDivLoss() # nn.BCELoss()\n",
        "optimizer = optim.Adam(model_mlp2.parameters(), lr=0.001)\n",
        "\n",
        "#Generate balanced dataset for training:\n",
        "# Separate instances for class 1\n",
        "class_1_instances = X_train_val[y_train_val == 1,:]\n",
        "class_0_instances = X_train_val[y_train_val == 0,:]\n",
        "number_of_samples = class_1_instances.shape[0]\n",
        "\n",
        "index             = np.random.choice(class_0_instances.shape[0], number_of_samples, replace=False)\n",
        "sampled_class_0_instances = class_0_instances[index,:]\n",
        "\n",
        "# Combine instances for class 1 and sampled instances from class 0\n",
        "#balanced_X = np.concatenate([class_1_instances, sampled_class_0_instances])\n",
        "#balanced_y = np.concatenate([np.ones(class_1_instances.shape[0]), np.zeros(sampled_class_0_instances.shape[0])])\n",
        "balanced_X = X_train_val ; balanced_y = y_train_val\n",
        "\n",
        "\n",
        "# Dummy data (replace this with your actual dataset)\n",
        "# Assuming you have X_train and y_train as your training data and labels\n",
        "data_X = torch.Tensor(balanced_X).to(device)\n",
        "data_y = torch.Tensor(balanced_y).view(-1, 1).to(device)\n",
        "\n",
        "# Create DataLoader for the dataset\n",
        "dataset = TensorDataset(data_X, data_y)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "top_n      = 101 # top n better models\n",
        "num_epochs = 250\n",
        "losses     = [10000]* top_n\n",
        "# Get the current date and time\n",
        "current_datetime     = datetime.now()\n",
        "current_datetime_str = f\"{current_datetime.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_mlp2(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch %10 == 0 :\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "    # Save better models:\n",
        "    cur_loss =loss.item()\n",
        "    for k in range(len(losses)):\n",
        "        if cur_loss < losses[k]:\n",
        "            losses[k] = cur_loss\n",
        "            if k < top_n:\n",
        "                model_file_name = f\"mlp_model_best_{k:02d}_{current_datetime_str}.pth\"\n",
        "                torch.save(model_mlp2.state_dict(), model_file_name)\n",
        "                #print(losses)\n",
        "                break\n",
        "\n",
        "\n",
        "#save the last model\n",
        "print(losses)\n",
        "model_file_name = f\"mlp_model_last_{current_datetime_str}.pth\"\n",
        "torch.save(model_mlp2.state_dict(), model_file_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------------------\n",
        "# Lets get training accuracy\n",
        "# Set the model to evaluation mode and evaluate it on train data:\n",
        "model_mlp2.eval()\n",
        "predictions=[]\n",
        "with torch.no_grad():\n",
        "  for inputs in dataloader: #remember from the earlier cell that this is the train dataloader\n",
        "    outputs = model_mlp2(inputs[0])\n",
        "    predictions.append(outputs.cpu().data.numpy())\n",
        "# Calculate accuracy\n",
        "predictions = np.concatenate(predictions)\n",
        "y_pred = predictions >= 0.5\n",
        "\n",
        "print_results(y_train_val , y_pred, 'torch nn, training ' + current_options)\n",
        "\n"
      ],
      "metadata": {
        "id": "mjhGuKclSinl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368a7346-9364-409b-934d-6e458adcc372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.620389856630936e-06, 9.040949407790322e-06, 1.196373068523826e-05, 2.4004553779377602e-05, 3.4125296224374324e-05, 3.956385626224801e-05, 5.206716014072299e-05, 6.47710548946634e-05, 9.248861169908196e-05, 9.431212674826384e-05, 0.0001482561056036502, 0.00045639934251084924, 0.0005832063034176826, 0.006052209530025721, 0.009372045285999775, 0.015000290237367153, 0.024201955646276474, 0.06612623482942581, 0.18303440511226654, 0.24143077433109283, 0.7864460945129395, 1.163494348526001, 1.5431393384933472, 1.8736683130264282, 2.3808646202087402, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000]\n",
            "torch nn, training tfidf,\tAccuracy=0.61,\tC0: Pr=0.74, Re=0.74, F1=0.74,\tC1: Pr=0.27, Re=0.27, F1=0.27\n",
            "something went wrong Length of values (9092) does not match length of index (833)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# print output to the console\n",
        "print(os.getcwd())\n",
        "os.chdir('c:/users/mmr')\n",
        "print(os.getcwd())\n",
        "\n",
        "\n",
        "# output will look something similar to this on a macOS system\n",
        "# /Users/dionysialemonaki/Documents/my-projects/python-proje"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFObKJipx1PS",
        "outputId": "26357670-6c64-4459-fdbf-6811f11d3014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c:\\users\\mmr\n",
            "c:\\users\\mmr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------\n",
        "# Test the network\n",
        "fname = f\"mlp_model_best_00_{current_datetime_str}.pth\"\n",
        "model = MLPModel(input_size, hidden_sizes)\n",
        "model.load_state_dict(torch.load(fname))\n",
        "\n",
        "#Prepare test data:\n",
        "new_data = torch.Tensor(X_test) #.to(device)\n",
        "\n",
        "# Create DataLoader for the new dataset\n",
        "new_dataset = TensorDataset(new_data)\n",
        "new_dataloader = DataLoader(new_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "predictions = []\n",
        "model.eval()\n",
        "# Make predictions on the test data\n",
        "with torch.no_grad():\n",
        "  for inputs in new_dataloader:\n",
        "    outputs = model(inputs[0])#(torch.tensor(X_test))\n",
        "    #predictions = torch.round(outputs)\n",
        "    predictions.append(outputs.cpu().data.numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "predictions = np.concatenate(predictions)\n",
        "y_pred = predictions >= 0.5\n",
        "\n",
        "print_results(y_test , y_pred, 'torch nn best, ' + current_options)"
      ],
      "metadata": {
        "id": "2rgKAAuUE6kg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749f4d62-b46b-4c49-816e-effa8e4e3532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch nn best, tfidf,\tAccuracy=0.84,\tC0: Pr=0.95, Re=0.87, F1=0.91,\tC1: Pr=0.35, Re=0.61, F1=0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T44vTY5NowqV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "a29854e7-bead-47eb-c9bf-791b157aadf9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.keras'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[196], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#'MLP Network': MLPClassifier(hidden_layer_sizes=(150, 100,50), activation='relu', solver='adam', max_iter=1000),\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Conv1D, MaxPooling1D, Embedding, LSTM, Flatten\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
          ]
        }
      ],
      "source": [
        "    #'MLP Network': MLPClassifier(hidden_layer_sizes=(150, 100,50), activation='relu', solver='adam', max_iter=1000),\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Embedding, LSTM, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Define common parameters\n",
        "train_size, vocab_size  = X_train.shape\n",
        "print(X_train.shape)\n",
        "  # Adjust based on your data\n",
        "max_len  = vocab_size  # Adjust based on your data\n",
        "numepochs  = 100\n",
        "\n",
        "model_mlp1 = MLPClassifier(random_state=42, max_iter=50)\n",
        "\n",
        "\n",
        "model_mlp2 = Sequential()\n",
        "model_mlp2.add(Dense(500, input_dim=X_train.shape[1]))\n",
        "model_mlp2.add(BatchNormalization())\n",
        "model_mlp2.add(Activation(activation='sigmoid'))\n",
        "model_mlp2.add(Dropout(0.2))\n",
        "model_mlp2.add(Dense(250))\n",
        "model_mlp2.add(BatchNormalization())\n",
        "model_mlp2.add(Activation(activation='relu'))\n",
        "model_mlp2.add(Dropout(0.2))\n",
        "model_mlp2.add(Dense(100))\n",
        "model_mlp2.add(BatchNormalization())\n",
        "model_mlp2.add(Activation(activation='sigmoid'))\n",
        "model_mlp2.add(Dropout(0.2))\n",
        "model_mlp2.add(Dense(50))\n",
        "model_mlp2.add(BatchNormalization())\n",
        "model_mlp2.add(Activation(activation='sigmoid'))\n",
        "model_mlp2.add(Dropout(0.2))\n",
        "model_mlp2.add(Dense(1,activation=tf.keras.activations.sigmoid))\n",
        "model_mlp2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define CNN model\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Embedding(vocab_size, 128, input_length=max_len))\n",
        "model_cnn.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn.add(Flatten())\n",
        "model_cnn.add(Dense(128, activation='relu'))\n",
        "model_cnn.add(Dense(len(set(y_train)), activation='softmax'))\n",
        "model_cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define RNN model\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(vocab_size, 128, input_length=max_len))\n",
        "model_rnn.add(LSTM(64, return_sequences=True))\n",
        "model_rnn.add(LSTM(32))\n",
        "model_rnn.add(Dense(128, activation='relu'))\n",
        "model_rnn.add(Dense(len(set(y_train)), activation='softmax'))\n",
        "model_rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_size, 128, input_length=max_len))\n",
        "model_lstm.add(LSTM(128))\n",
        "model_lstm.add(Dense(64, activation='relu'))\n",
        "model_lstm.add(Dense(len(set(y_train)), activation='softmax'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Evaluate and compare models\n",
        "# Initialize classifiers\n",
        "NN = {\n",
        "    \"Modern MLPt\": model_mlp2,\n",
        "    \"CNN\": model_cnn,\n",
        "    \"Recurrent NN\": model_rnn,\n",
        "    \"LSTM\": model_lstm,\n",
        "}\n",
        "\n",
        "# Loop through each classifier and evaluate performance\n",
        "for name, clf in NN.items():\n",
        "  clf.fit(X_train_val, y_train_val, epochs=numepochs)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  if (y_pred.ndim > 1): y_pred = np.argmax(y_pred , axis=1)\n",
        "\n",
        "\n",
        "  print_results(y_test , y_pred, name + ', '+ current_options)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchvision.transforms import Lambda\n",
        "from torch.nn.functional import softmax\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dropout_rate=0.5):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.embedding(inputs).permute(0, 2, 1)  # Permute to (batch_size, embedding_dim, sequence_length)\n",
        "        conv_outputs = [conv(x) for conv in self.conv_layers]\n",
        "        pooled_outputs = [torch.max(conv_output, dim=2)[0] for conv_output in conv_outputs]\n",
        "        concatenated = torch.cat(pooled_outputs, dim=1)\n",
        "        concatenated = self.dropout(concatenated)\n",
        "        output = self.fc(concatenated)\n",
        "        return softmax(output, dim=1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hazFqs_6e-kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "\n",
        "# Load data from CSV file\n",
        "df = all_df\n",
        "\n",
        "# Example: Preprocess data\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "counter = Counter()\n",
        "for line in df['sentence']:\n",
        "    counter.update(tokenizer(line))\n",
        "vocab = build_vocab_from_iterator([tokenizer(line) for line in df['sentence']], specials=['<unk>', '<pad>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "label_encoder = LabelEncoder()\n",
        "x_data = []\n",
        "y_data = label_encoder.fit_transform(df['label'])  # Replace 'label_column' with the name of your label column\n",
        "for line in df['sentence']:\n",
        "    x_data.append(torch.tensor([vocab[token] for token in tokenizer(line)]))\n",
        "\n",
        "# Pad sequences and convert to tensors\n",
        "x_data = nn.utils.rnn.pad_sequence(x_data, batch_first=True)\n",
        "y_data = torch.tensor(y_data)\n",
        "\n",
        "# Split data into train/validation sets\n",
        "x_data_train, x_data_val, y_data_train, y_data_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Example: Instantiate the TextCNN model\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 128\n",
        "num_filters = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_classes = 2\n",
        "dropout_rate = 0.5\n",
        "\n",
        "model = TextCNN(vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dropout_rate)\n",
        "\n",
        "# Example: Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Example: Train the model\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for i in range(0, len(x_data_train), batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        batch_x, batch_y = x_data_train[i:i+batch_size], y_data_train[i:i+batch_size]\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i+1) % 2 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(x_train)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Example: Evaluate the model on validation data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(x_data_val)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_data_val).sum().item()\n",
        "    accuracy = correct / len(y_data_val)\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "    print_results(y_data_val,predicted , 'Text CNN, ' + current_options )"
      ],
      "metadata": {
        "id": "YKeKBAFafCn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d669384-7e4a-461f-e041-06bf45f0b57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7461\n",
            "Text CNN, tfidf,\tAccuracy=0.75,\tC0: Pr=0.75, Re=1.00, F1=0.85,\tC1: Pr=0.00, Re=0.00, F1=0.00\n",
            "something went wrong Length of values (1985) does not match length of index (833)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mixture of Experts**"
      ],
      "metadata": {
        "id": "2pKLzhsFlDWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Instantiate base classifiers\n",
        "rf_classifier = RandomForestClassifier()\n",
        "gb_classifier = GradientBoostingClassifier()\n",
        "lr_classifier = LogisticRegression()\n",
        "mlp_classifier = MLPClassifier()\n",
        "\n",
        "# Train base classifiers on your imbalanced dataset\n",
        "rf_classifier.fit(X_train_val, y_train_val)\n",
        "gb_classifier.fit(X_train_val, y_train_val)\n",
        "lr_classifier.fit(X_train_val, y_train_val)\n",
        "mlp_classifier.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Make predictions on the test set\n",
        "rf_preds = rf_classifier.predict(X_test)\n",
        "gb_preds = gb_classifier.predict(X_test)\n",
        "lr_preds = lr_classifier.predict(X_test)\n",
        "mlp_preds = mlp_classifier.predict(X_test)\n",
        "\n",
        "# Combine predictions using weighted voting\n",
        "ensemble_preds = (0.25 * rf_preds + 0.25 * gb_preds + 0.25 * lr_preds + 0.25 * mlp_preds)\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "y_pred[ensemble_preds >= 0.5] =1\n",
        "# Evaluate the ensemble\n",
        "print(classification_report(y_test, y_pred))\n",
        "print_results(y_test, y_pred, 'MoE, Eq. Weight, ' + current_options )\n",
        "\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_preds_rf  = rf_classifier.predict(X_train_val).reshape(-1,1)\n",
        "y_preds_gb  = gb_classifier.predict(X_train_val).reshape(-1,1)\n",
        "y_preds_lr  = lr_classifier.predict(X_train_val).reshape(-1,1)\n",
        "y_preds_mlp = mlp_classifier.predict(X_train_val).reshape(-1,1)\n",
        "y_pred_train_val = np.concatenate((y_preds_rf,y_preds_gb, y_preds_lr, y_preds_mlp ), axis=1)\n",
        "\n",
        "# Instantiate base classifiers\n",
        "rf_combiner = RandomForestClassifier()\n",
        "gb_combiner = GradientBoostingClassifier()\n",
        "lr_combiner = LogisticRegression()\n",
        "mlp_combiner = MLPClassifier()\n",
        "\n",
        "rf_combiner.fit(y_pred_train_val, y_train_val)\n",
        "gb_combiner.fit(y_pred_train_val, y_train_val)\n",
        "lr_combiner.fit(y_pred_train_val, y_train_val)\n",
        "mlp_combiner.fit(y_pred_train_val, y_train_val)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_test_rf  = rf_classifier.predict(X_test).reshape(-1,1)\n",
        "y_pred_test_gb  = gb_classifier.predict(X_test).reshape(-1,1)\n",
        "y_pred_test_lr  = lr_classifier.predict(X_test).reshape(-1,1)\n",
        "y_pred_test_mlp = mlp_classifier.predict(X_test).reshape(-1,1)\n",
        "y_pred_test = np.concatenate((y_pred_test_rf, y_pred_test_gb, y_pred_test_lr, y_pred_test_mlp ), axis=1)\n",
        "\n",
        "rfc_preds = rf_combiner.predict(y_pred_test)\n",
        "gbc_preds = gb_combiner.predict(y_pred_test)\n",
        "lrc_preds = lr_combiner.predict(y_pred_test)\n",
        "mlpc_preds = mlp_combiner.predict(y_pred_test)\n",
        "\n",
        "ensemblec_preds = (0.25 * rfc_preds + 0.25 * gbc_preds + 0.25 * lrc_preds + 0.25 * mlpc_preds)\n",
        "\n",
        "# Evaluate the ensemble\n",
        "print_results(y_test, prob2label(rfc_preds),  'MoE, rf-cmb, ' + current_options )\n",
        "print_results(y_test, prob2label(gbc_preds),  'MoE, gb-cmb, ' + current_options )\n",
        "print_results(y_test, prob2label(lrc_preds),  'MoE, lr-cmb, ' + current_options )\n",
        "print_results(y_test, prob2label(mlpc_preds), 'MoE, mlpc-cmb, ' + current_options )\n",
        "print_results(y_test, prob2label(ensemblec_preds), 'MoE, all-cmb, ' + current_options )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-Jmzi2glCyQ",
        "outputId": "f9804532-bf21-46df-f619-5ac0b2af5c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96       746\n",
            "           1       0.64      0.67      0.65        87\n",
            "\n",
            "    accuracy                           0.93       833\n",
            "   macro avg       0.80      0.81      0.81       833\n",
            "weighted avg       0.93      0.93      0.93       833\n",
            "\n",
            "MoE, Eq. Weight, tfidf,\tAccuracy=0.93,\tC0: Pr=0.96, Re=0.96, F1=0.96,\tC1: Pr=0.64, Re=0.67, F1=0.65\n",
            "MoE, rf-cmb, tfidf,\tAccuracy=0.83,\tC0: Pr=0.95, Re=0.86, F1=0.90,\tC1: Pr=0.34, Re=0.63, F1=0.44\n",
            "MoE, gb-cmb, tfidf,\tAccuracy=0.84,\tC0: Pr=0.96, Re=0.86, F1=0.91,\tC1: Pr=0.36, Re=0.67, F1=0.47\n",
            "MoE, lr-cmb, tfidf,\tAccuracy=0.92,\tC0: Pr=0.96, Re=0.96, F1=0.96,\tC1: Pr=0.63, Re=0.63, F1=0.63\n",
            "MoE, mlpc-cmb, tfidf,\tAccuracy=0.84,\tC0: Pr=0.96, Re=0.85, F1=0.90,\tC1: Pr=0.36, Re=0.72, F1=0.48\n",
            "MoE, all-cmb, tfidf,\tAccuracy=0.84,\tC0: Pr=0.96, Re=0.85, F1=0.90,\tC1: Pr=0.35, Re=0.69, F1=0.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,prob2label( mlpc_preds)))"
      ],
      "metadata": {
        "id": "dWjP4VNjw07j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9o02SgiTxpA"
      },
      "source": [
        "**Word2vec Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this set of experiments we build and test several well-known\n",
        "word2vec and doc2vec models"
      ],
      "metadata": {
        "id": "zbCmHPIRx1Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#basic functions\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "#Get embedding of a document as the mean of embeddings of its words\n",
        "def get_doc2vec(model ,doc ):\n",
        "  tokens = doc.lower().split()\n",
        "  vec = np.zeros(model.vector_size)\n",
        "  num_tokens = 0\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      vec += model.get_vector(token)\n",
        "      num_tokens += 1\n",
        "    except:\n",
        "      token = 'unk' #print(f\"{token} not found in vocab\")\n",
        "\n",
        "  if num_tokens > 0:\n",
        "    vec /= num_tokens\n",
        "  return vec.reshape(1,-1)\n",
        "\n",
        "#Generate document vectors for all of the sentences:\n",
        "def get_corpus_embeddings(model , documents):\n",
        "  X = get_doc2vec(model, documents[0] )\n",
        "  for i in range(1,len(documents)):\n",
        "    X = np.append(X, get_doc2vec(model, documents[i]).reshape(1,-1), axis=0)\n",
        "\n",
        "  return X"
      ],
      "metadata": {
        "id": "xz4qDEhR4MFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gensim doc2vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(all_df['sentence'].tolist())]\n",
        "model = Doc2Vec(documents, vector_size=300, window=4, min_count=1, workers=100, dbow_words =1, epochs=100)\n",
        "\n",
        "#Persist a model to disk:\n",
        "fname = get_tmpfile(\"gensim_doc2vec_model\")\n",
        "model.save(fname)\n",
        "model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n",
        "\n",
        "#Generate document vectors for all of the sentences:\n",
        "X_d2v = get_corpus_embeddings (model.wv, all_df['sentence'].tolist())\n",
        "\n",
        "test_basic_models(X_d2v[:y_train_val.shape[0],:], y_train_val,\n",
        "                  X_d2v[y_train_val.shape[0]:,:], y_test, 'gensim_doc2vec, ')\n",
        "\n"
      ],
      "metadata": {
        "id": "ixV3toP99VFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2085afb9-507f-4f41-96bd-4b7c4b566d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, gensim_doc2vec, ,\tAccuracy=0.88,\tC0: Pr=0.95, Re=0.92, F1=0.93,\tC1: Pr=0.44, Re=0.55, F1=0.49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Logistic Regression, gensim_doc2vec, ,\tAccuracy=0.89,\tC0: Pr=0.94, Re=0.95, F1=0.94,\tC1: Pr=0.49, Re=0.44, F1=0.46\n",
            "basic Support Vector Machine-L, gensim_doc2vec, ,\tAccuracy=0.89,\tC0: Pr=0.94, Re=0.95, F1=0.94,\tC1: Pr=0.49, Re=0.45, F1=0.47\n",
            "basic Support Vector Machine-R, gensim_doc2vec, ,\tAccuracy=0.91,\tC0: Pr=0.94, Re=0.95, F1=0.95,\tC1: Pr=0.56, Re=0.52, F1=0.54\n",
            "basic Support Vector Machine-S, gensim_doc2vec, ,\tAccuracy=0.76,\tC0: Pr=0.91, Re=0.81, F1=0.86,\tC1: Pr=0.15, Re=0.30, F1=0.20\n",
            "basic Support Vector Machine-WL, gensim_doc2vec, ,\tAccuracy=0.63,\tC0: Pr=0.98, Re=0.60, F1=0.75,\tC1: Pr=0.21, Re=0.92, F1=0.34\n",
            "basic Support Vector Machine-WR, gensim_doc2vec, ,\tAccuracy=0.73,\tC0: Pr=0.99, Re=0.71, F1=0.83,\tC1: Pr=0.27, Re=0.93, F1=0.42\n",
            "basic Support Vector Machine-WS, gensim_doc2vec, ,\tAccuracy=0.51,\tC0: Pr=0.97, Re=0.46, F1=0.63,\tC1: Pr=0.16, Re=0.86, F1=0.27\n",
            "basic Decision Tree classifier, gensim_doc2vec, ,\tAccuracy=0.80,\tC0: Pr=0.94, Re=0.82, F1=0.88,\tC1: Pr=0.27, Re=0.57, F1=0.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs=[]\n",
        "for doc in documents:\n",
        "  #print(doc.split())\n",
        "  docs += [doc.split()]\n",
        "\n",
        "print(docs)\n",
        "#alldocs =[[doc.split()], for doc in documents]\n",
        "print(len(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8smSL_1OhuTK",
        "outputId": "e7ae277c-28fa-4f36-91ba-958d07f269ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gensim FastText\n",
        "from gensim.models import FastText\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "documents = all_df['sentence'].tolist()\n",
        "docs=[]\n",
        "for doc in documents:\n",
        "  #print(doc.split())\n",
        "  docs += [doc.split()]\n",
        "\n",
        "model_fasttext = FastText(vector_size=40, window=3, min_count=1, sentences=docs, epochs=100)\n",
        "\n",
        "fname = get_tmpfile(\"suggestion_fasttext.model\")\n",
        "model_fasttext.save(fname)\n",
        "model_fasttext = FastText.load(fname)\n",
        "\n",
        "#Generate document vectors for all of the sentences:\n",
        "X_ft = get_corpus_embeddings (model_fasttext.wv, all_df['sentence'].tolist())\n",
        "\n",
        "test_basic_models(X_ft[:y_train_val.shape[0],:], y_train_val,\n",
        "                  X_ft[y_train_val.shape[0]:,:], y_test, 'gensim-fasttext-words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzVwSiFk2Qtj",
        "outputId": "5bb268bf-0302-4437-c90b-5616e3b736c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, gensim-fasttext-words,\tAccuracy=0.86,\tC0: Pr=0.94, Re=0.90, F1=0.92,\tC1: Pr=0.39, Re=0.52, F1=0.44\n",
            "basic Logistic Regression, gensim-fasttext-words,\tAccuracy=0.87,\tC0: Pr=0.92, Re=0.94, F1=0.93,\tC1: Pr=0.37, Re=0.31, F1=0.34\n",
            "basic Support Vector Machine-L, gensim-fasttext-words,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.95, F1=0.93,\tC1: Pr=0.38, Re=0.24, F1=0.29\n",
            "basic Support Vector Machine-R, gensim-fasttext-words,\tAccuracy=0.91,\tC0: Pr=0.94, Re=0.96, F1=0.95,\tC1: Pr=0.58, Re=0.47, F1=0.52\n",
            "basic Support Vector Machine-S, gensim-fasttext-words,\tAccuracy=0.77,\tC0: Pr=0.92, Re=0.82, F1=0.87,\tC1: Pr=0.20, Re=0.40, F1=0.27\n",
            "basic Support Vector Machine-WL, gensim-fasttext-words,\tAccuracy=0.46,\tC0: Pr=0.99, Re=0.40, F1=0.57,\tC1: Pr=0.16, Re=0.95, F1=0.27\n",
            "basic Support Vector Machine-WR, gensim-fasttext-words,\tAccuracy=0.61,\tC0: Pr=0.99, Re=0.57, F1=0.73,\tC1: Pr=0.20, Re=0.93, F1=0.33\n",
            "basic Support Vector Machine-WS, gensim-fasttext-words,\tAccuracy=0.47,\tC0: Pr=0.96, Re=0.42, F1=0.59,\tC1: Pr=0.15, Re=0.85, F1=0.25\n",
            "basic Decision Tree classifier, gensim-fasttext-words,\tAccuracy=0.81,\tC0: Pr=0.94, Re=0.84, F1=0.88,\tC1: Pr=0.28, Re=0.55, F1=0.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test pretrained word2vec models:\n",
        "import gensim.downloader\n",
        "models =['fasttext-wiki-news-subwords-300',\n",
        "         'conceptnet-numberbatch-17-06-300',\n",
        "         'word2vec-ruscorpora-300',\n",
        "         'word2vec-google-news-300',\n",
        "         'glove-wiki-gigaword-50',\n",
        "         'glove-wiki-gigaword-100',\n",
        "         'glove-wiki-gigaword-200',\n",
        "         'glove-wiki-gigaword-300',\n",
        "         'glove-twitter-25',\n",
        "         'glove-twitter-50',\n",
        "         'glove-twitter-100',\n",
        "         'glove-twitter-200',\n",
        "         '__testing_word2vec-matrix-synopsis',\n",
        "         ]\n",
        "\n",
        "#Play with pretrained word2vec embeddings\n",
        "for model_name in models:\n",
        "  print (model_name)\n",
        "  model_fname = model_name + \".model\"\n",
        "  model_pretrained = gensim.downloader.load(model_name)\n",
        "\n",
        "\n",
        "  #Generate document vectors for all of the sentences:\n",
        "  X_gl = get_corpus_embeddings (model_pretrained, all_df['sentence'].tolist())\n",
        "\n",
        "  test_basic_models(X_gl[:y_train_val.shape[0],:], y_train_val,\n",
        "                    X_gl[y_train_val.shape[0]:,:], y_test, model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "96kq9mftvxvr",
        "outputId": "5cbb198c-2908-43dc-b6dc-d8bd5f0033e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fasttext-wiki-news-subwords-300\n",
            "[==============================================----] 93.8% 899.1/958.4MB downloaded"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-0280d76aed58>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mmodel_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mmodel_pretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{fname}.gz\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_calculate_md5_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                     \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mread\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_progress\u001b[0;34m(chunks_downloaded, chunk_size, total_size, part, total_parts)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0msize_downloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks_downloaded\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mfilled_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbar_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize_downloaded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mpercent_downloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_downloaded\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfilled_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbar_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfilled_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_parts\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MvUlQpRx46zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = MLPClassifier(hidden_layer_sizes=(50,25,20,10,5),\n",
        "                           max_iter=100,activation = 'relu',\n",
        "                           solver='adam',random_state=100).fit(X_d2v[0:y_train_val.shape[0],:], y_train_val)\n",
        "\n",
        "y_pred = classifier.predict(X_d2v)\n",
        "print(confusion_matrix(y_all,y_pred))\n",
        "print(classification_report(y_all,y_pred))\n",
        "print_results(y_all , y_pred , 'NN-all')\n",
        "\n",
        "\n",
        "y_pred = classifier.predict(X_d2v[y_train_val.shape[0]:,:])\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print_results(y_test , y_pred , 'NN-test')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH5DSOIj2w6M",
        "outputId": "58b805b3-262b-4964-b122-0efd8e6e3017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7347  110]\n",
            " [  65 2403]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      7457\n",
            "           1       0.96      0.97      0.96      2468\n",
            "\n",
            "    accuracy                           0.98      9925\n",
            "   macro avg       0.97      0.98      0.98      9925\n",
            "weighted avg       0.98      0.98      0.98      9925\n",
            "\n",
            "NN-all,\tAccuracy=0.98,\tC0: Pr=0.99, Re=0.99, F1=0.99,\tC1: Pr=0.96, Re=0.97, F1=0.96\n",
            "something went wrong Length of values (9925) does not match length of index (833)\n",
            "[[676  70]\n",
            " [ 38  49]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.91      0.93       746\n",
            "           1       0.41      0.56      0.48        87\n",
            "\n",
            "    accuracy                           0.87       833\n",
            "   macro avg       0.68      0.73      0.70       833\n",
            "weighted avg       0.89      0.87      0.88       833\n",
            "\n",
            "NN-test,\tAccuracy=0.87,\tC0: Pr=0.95, Re=0.91, F1=0.93,\tC1: Pr=0.41, Re=0.56, F1=0.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2419250893.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = MLPClassifier(hidden_layer_sizes=(50,25,20,10,5),\n",
        "                           max_iter=100,activation = 'relu',\n",
        "                           solver='adam',random_state=100).fit(X_train_val, y_train_val)\n",
        "\n",
        "y_pred = classifier.predict(X_all)\n",
        "print(confusion_matrix(y_all,y_pred))\n",
        "print(classification_report(y_all,y_pred))\n",
        "print_results(y_all , y_pred , 'NN-all')\n",
        "\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print_results(y_test , y_pred , 'NN-test_orig')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mXXn_56Jseg",
        "outputId": "fdf40261-2c88-4df7-e978-06db7f5a1923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7335  122]\n",
            " [  81 2387]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99      7457\n",
            "           1       0.95      0.97      0.96      2468\n",
            "\n",
            "    accuracy                           0.98      9925\n",
            "   macro avg       0.97      0.98      0.97      9925\n",
            "weighted avg       0.98      0.98      0.98      9925\n",
            "\n",
            "NN-all,\tAccuracy=0.98,\tC0: Pr=0.99, Re=0.98, F1=0.99,\tC1: Pr=0.95, Re=0.97, F1=0.96\n",
            "something went wrong Length of values (9925) does not match length of index (833)\n",
            "[[650  96]\n",
            " [ 31  56]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.87      0.91       746\n",
            "           1       0.37      0.64      0.47        87\n",
            "\n",
            "    accuracy                           0.85       833\n",
            "   macro avg       0.66      0.76      0.69       833\n",
            "weighted avg       0.89      0.85      0.86       833\n",
            "\n",
            "NN-test_orig,\tAccuracy=0.85,\tC0: Pr=0.95, Re=0.87, F1=0.91,\tC1: Pr=0.37, Re=0.64, F1=0.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuzTr3MJKbYw",
        "outputId": "b473f0b5-3130-4fff-e35c-723d77c98a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.13.1-cp38-cp38-win_amd64.whl.metadata (2.6 kB)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached tensorflow-2.13.0-cp38-cp38-win_amd64.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\python\\python38\\lib\\site-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (24.3.7)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (24.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
            "  Using cached protobuf-4.25.3-cp38-cp38-win_amd64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (41.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.4.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)\n",
            "  Using cached typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.62.1)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)\n",
            "  Using cached tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
            "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
            "  Using cached keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python\\python38\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\python\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.28.2)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
            "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\python\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python\\python38\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\python\\python38\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (7.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python\\python38\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\python\\python38\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\python\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python\\python38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
            "Using cached tensorflow-2.13.0-cp38-cp38-win_amd64.whl (1.9 kB)\n",
            "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "Using cached protobuf-4.25.3-cp38-cp38-win_amd64.whl (413 kB)\n",
            "Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Installing collected packages: typing-extensions, tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.10.0\n",
            "    Uninstalling typing_extensions-4.10.0:\n",
            "      Successfully uninstalled typing_extensions-4.10.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.10.0\n",
            "    Uninstalling tensorflow-estimator-2.10.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.10.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.10.0\n",
            "    Uninstalling keras-2.10.0:\n",
            "      Successfully uninstalled keras-2.10.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.10.1\n",
            "    Uninstalling tensorboard-2.10.1:\n",
            "      Successfully uninstalled tensorboard-2.10.1\n",
            "Successfully installed google-auth-oauthlib-1.0.0 keras-2.13.1 protobuf-4.25.3 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.0 tensorflow-estimator-2.13.0 typing-extensions-4.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sqlalchemy 2.0.28 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic 2.6.4 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.16.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow-metadata 1.13.0 requires protobuf<4,>=3.13, but you have protobuf 4.25.3 which is incompatible.\n",
            "tensorflow-text 2.10.0 requires tensorflow<2.11,>=2.10.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.13.0 which is incompatible.\n",
            "tf-models-official 2.10.1 requires tensorflow~=2.10.0, but you have tensorflow 2.13.0 which is incompatible.\n",
            "torch 2.2.1+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define model architecture\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(256, activation=\"relu\", input_shape=(embedding_dim,)),\n",
        "  layers.Dense(128, activation=\"relu\"),\n",
        "  layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "class_weights ={0:1,1:100}\n",
        "# Compile model with WBCE loss\n",
        "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train model with potentially oversampled/undersampled data\n",
        "model.fit(X_d2v[0:y_train_val.shape[0],:], y_train_val, epochs=100, class_weight=class_weights)\n",
        "\n",
        "# Classify new documents\n",
        "y_pred = model.predict(X_d2v) >= 0.5\n",
        "print(confusion_matrix(y_all,y_pred))\n",
        "print(classification_report(y_all,y_pred))\n",
        "print_results(y_all , y_pred , 'NN-all2')\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_d2v[y_train_val.shape[0]:,:]) >= 0.5\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print_results(y_test , y_pred , 'NN-test2')\n"
      ],
      "metadata": {
        "id": "-j1gOhxmPf49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "e5995ef0-72cb-4abc-ce26-65bdd078cb8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'keras' from 'tensorflow' (unknown location)",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[313], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define model architecture\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'keras' from 'tensorflow' (unknown location)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "dir(tensorflow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r495roEQ80YV",
        "outputId": "be949886-a12b-45be-ce4a-acd12e470850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__']"
            ]
          },
          "metadata": {},
          "execution_count": 321
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRhDG6ZIRWMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce04154-03d1-4637-f109-7c7caa41d3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[685  61]\n",
            " [ 27  60]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.92      0.94       746\n",
            "           1       0.50      0.69      0.58        87\n",
            "\n",
            "    accuracy                           0.89       833\n",
            "   macro avg       0.73      0.80      0.76       833\n",
            "weighted avg       0.91      0.89      0.90       833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classifier = MLPClassifier(hidden_layer_sizes=(150,100,50),\n",
        "                           max_iter=100,activation = 'relu',\n",
        "                           solver='adam',random_state=100).fit(X_train_val, y_train_val)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "classifier.fit(X_train,y_train); plt.plot(classifier.loss_curve_,label=\"train\")\n",
        "classifier.fit(X_valid,y_valid); plt.plot(classifier.loss_curve_,label=\"validation\")\n",
        "classifier.fit(X_test,y_test); plt.plot(classifier.loss_curve_,label=\"test\")\n",
        "\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Misclassification Rate/Loss\");\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('mlp-tfidf-training')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balanced Classification**\n",
        "\n",
        "This experiments trains an ensemble of random forests on the balanced subsets"
      ],
      "metadata": {
        "id": "vS--U_kAR4xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Separate instances for class 1\n",
        "class_1_instances = X_train_val[y_train_val == 1,:]\n",
        "class_0_instances = X_train_val[y_train_val == 0,:]\n",
        "num_classifiers = 15\n",
        "classifiers = []\n",
        "number_of_samples = class_1_instances.shape[0]\n",
        "# Build an ensemble of classifiers (Random Forests in this example)\n",
        "\n",
        "for ks in range(1,num_classifiers+1):\n",
        "  # Randomly sample 2000 instances from class 0\n",
        "  indices = np.random.choice(class_0_instances.shape[0], number_of_samples, replace=True)\n",
        "  sampled_class_0_instances = class_0_instances[indices,:]\n",
        "\n",
        "  # Combine instances for class 1 and sampled instances from class 0\n",
        "  balanced_X = np.concatenate([class_1_instances, sampled_class_0_instances])\n",
        "  balanced_y = np.concatenate([np.ones(class_1_instances.shape[0]), np.zeros(sampled_class_0_instances.shape[0])])\n",
        "\n",
        "  classifier = RandomForestClassifier(n_estimators=101, random_state=100)\n",
        "  #classifier = sklearn.linear_model.LogisticRegression(random_state=42)\n",
        "  classifier.fit(balanced_X, balanced_y)\n",
        "  classifiers.append(classifier)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  print_results(y_test , y_pred,  f'ensemble{ks} ' + current_options)\n",
        "\n",
        "# Make predictions on the test set using each classifier\n",
        "predictions = [classifier.predict(X_test) for classifier in classifiers]\n",
        "\n",
        "# Take a majority vote to get the final ensemble prediction\n",
        "ensemble_predictions = np.mean(predictions, axis=0) > 0.5\n",
        "\n",
        "# Evaluate the ensemble performance\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
        "print(f'Ensemble Accuracy: {ensemble_accuracy}')\n",
        "print_results(y_test , ensemble_predictions,  'ensemble_total ' + current_options)\n"
      ],
      "metadata": {
        "id": "R-gOUwRaR5VG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae6c4313-c063-49e4-9577-cf79faf6af28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble1 bow,\tAccuracy=0.89,\tC0: Pr=0.98, Re=0.89, F1=0.93,\tC1: Pr=0.48, Re=0.87, F1=0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble2 bow,\tAccuracy=0.89,\tC0: Pr=0.98, Re=0.89, F1=0.93,\tC1: Pr=0.48, Re=0.87, F1=0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble3 bow,\tAccuracy=0.88,\tC0: Pr=0.98, Re=0.88, F1=0.93,\tC1: Pr=0.45, Re=0.82, F1=0.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble4 bow,\tAccuracy=0.89,\tC0: Pr=0.98, Re=0.90, F1=0.94,\tC1: Pr=0.49, Re=0.86, F1=0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble5 bow,\tAccuracy=0.89,\tC0: Pr=0.98, Re=0.89, F1=0.93,\tC1: Pr=0.47, Re=0.87, F1=0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble6 bow,\tAccuracy=0.89,\tC0: Pr=0.98, Re=0.89, F1=0.93,\tC1: Pr=0.48, Re=0.84, F1=0.61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble7 bow,\tAccuracy=0.88,\tC0: Pr=0.98, Re=0.88, F1=0.93,\tC1: Pr=0.47, Re=0.87, F1=0.61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble8 bow,\tAccuracy=0.89,\tC0: Pr=0.98, Re=0.89, F1=0.94,\tC1: Pr=0.48, Re=0.86, F1=0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble9 bow,\tAccuracy=0.88,\tC0: Pr=0.98, Re=0.89, F1=0.93,\tC1: Pr=0.47, Re=0.85, F1=0.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble10 bow,\tAccuracy=0.89,\tC0: Pr=0.98, Re=0.90, F1=0.94,\tC1: Pr=0.49, Re=0.85, F1=0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble11 bow,\tAccuracy=0.87,\tC0: Pr=0.98, Re=0.87, F1=0.92,\tC1: Pr=0.43, Re=0.86, F1=0.57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble12 bow,\tAccuracy=0.89,\tC0: Pr=0.98, Re=0.89, F1=0.93,\tC1: Pr=0.48, Re=0.85, F1=0.61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble13 bow,\tAccuracy=0.87,\tC0: Pr=0.98, Re=0.88, F1=0.93,\tC1: Pr=0.44, Re=0.83, F1=0.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble14 bow,\tAccuracy=0.88,\tC0: Pr=0.98, Re=0.89, F1=0.93,\tC1: Pr=0.46, Re=0.84, F1=0.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble15 bow,\tAccuracy=0.88,\tC0: Pr=0.98, Re=0.88, F1=0.93,\tC1: Pr=0.46, Re=0.84, F1=0.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Accuracy: 0.8883553421368547\n",
            "ensemble_total bow,\tAccuracy=0.89,\tC0: Pr=0.98, Re=0.89, F1=0.93,\tC1: Pr=0.48, Re=0.87, F1=0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18084\\2986776091.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Longest Common Subsequence**"
      ],
      "metadata": {
        "id": "3fF6EUfhxT42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def longest_common_subsequence(str1, str2):\n",
        "    words1 = str1.split()\n",
        "    words2 = str2.split()\n",
        "\n",
        "    m = len(words1)\n",
        "    n = len(words2)\n",
        "\n",
        "    # Initializing the dp table with zeros\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Building the dp table\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if words1[i - 1] == words2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
        "\n",
        "    # Backtracking to find the longest common subsequence\n",
        "    lcs_length = dp[m][n]\n",
        "    lcs = []\n",
        "    i = m\n",
        "    j = n\n",
        "    while i > 0 and j > 0:\n",
        "        if words1[i - 1] == words2[j - 1]:\n",
        "            lcs.append ( words1[i - 1])\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "            lcs_length -= 1\n",
        "        elif dp[i - 1][j] > dp[i][j - 1]:\n",
        "            i -= 1\n",
        "        else:\n",
        "            j -= 1\n",
        "\n",
        "    lcs.reverse()\n",
        "    return lcs , (len(lcs))/ (0.0001+m) # 0.0001 is denom is there to prevent div by 0\n",
        "# Example usage:\n",
        "str1        = \"roses are red. violets are blue\"\n",
        "str2        = \"the garden is full of roses and violets that are blue \"\n",
        "lcs12 , r12 = longest_common_subsequence(str1, str2)\n",
        "print(f\"Longest Common Subsequence:{r12}: {lcs12} \")\n"
      ],
      "metadata": {
        "id": "cKfUYz7WxTPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9a0a25-bec8-4cb2-914b-b3fec5324029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest Common Subsequence:0.6666555557407376: ['roses', 'violets', 'are', 'blue'] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords_list(tokens):\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "def remove_stopwords_str(str):\n",
        "  str = re.sub(r'[^A-Za-z0-9]+', ' ', str)\n",
        "  str = re.sub(r'\\W+', ' ', str)\n",
        "  str = re.sub(r'\\s+', ' ', str)\n",
        "\n",
        "  tokens = str.split()\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return ' '.join(filtered_tokens)\n",
        "\n",
        "#--------------------\n",
        "#\n",
        "\n",
        "do_lcs = True # switch this flag if you want to run lcs\n",
        "\n",
        "if do_lcs:\n",
        "  train = train_df['sentence'].tolist(); train  = [remove_stopwords_str(s) for s in train ]\n",
        "  test  = test_df['sentence'].tolist();  test   = [remove_stopwords_str(s) for s in test ]\n",
        "  valid = valid_df['sentence'].tolist(); valid  = [remove_stopwords_str(s) for s in valid ]\n",
        "\n",
        "  X = train + valid ;   y = y_train_val\n",
        "  num_train = len(X);   num_test  = len(test)\n",
        "\n",
        "  with open('train_test_lcs03.txt', 'w') as f:\n",
        "    for m in range(num_train):\n",
        "      if y[m] == 1:\n",
        "        for n in range(num_test):\n",
        "          temp, r = longest_common_subsequence (X[m], test[n])\n",
        "          if r >= 0.01:\n",
        "            print(f\"{m}\\t{n}\\t{y[m]}\\t{y_test[n]}\\t{r:0.02f}\\t{len(temp)}\\t{temp}\",file=f)\n"
      ],
      "metadata": {
        "id": "LzLh7vJ0xZpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05414054-3496-41ab-d9c0-5e14e42dcf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def wordnet_path_similarity(word1, word2):\n",
        "    synsets1 = wn.synsets(word1)\n",
        "    synsets2 = wn.synsets(word2)\n",
        "    if not synsets1 or not synsets2:\n",
        "        return 0\n",
        "    else:\n",
        "        max_similarity = 0\n",
        "        for synset1 in synsets1:\n",
        "            for synset2 in synsets2:\n",
        "                path_similarity = synset1.path_similarity(synset2)\n",
        "                if path_similarity is not None and path_similarity > max_similarity:\n",
        "                    max_similarity = path_similarity\n",
        "        return max_similarity\n",
        "\n",
        "def dtw_distance(s1, s2, similarity_function=wordnet_path_similarity):\n",
        "    len_s1, len_s2 = len(s1), len(s2)\n",
        "    dtw_matrix = np.zeros((len_s1 + 1, len_s2 + 1))\n",
        "\n",
        "    # Initialize the DTW matrix with infinity\n",
        "    for i in range(len_s1 + 1):\n",
        "        for j in range(len_s2 + 1):\n",
        "            dtw_matrix[i, j] = float('inf')\n",
        "\n",
        "    dtw_matrix[0, 0] = 0\n",
        "\n",
        "    # Calculate DTW matrix\n",
        "    for i in range(1, len_s1 + 1):\n",
        "        for j in range(1, len_s2 + 1):\n",
        "            cost = 1 - similarity_function(s1[i - 1], s2[j - 1])  # Using WordNet similarity as cost\n",
        "            dtw_matrix[i, j] = cost + min(dtw_matrix[i - 1, j], dtw_matrix[i, j - 1], dtw_matrix[i - 1, j - 1])\n",
        "\n",
        "    return dtw_matrix[len_s1, len_s2]\n",
        "\n",
        "def dtw_distance_str(s1, s2):\n",
        "  return dtw_distance(s1.split() , s2.split())\n",
        "\n",
        "# Example lists of strings\n",
        "list1 = ['cat', 'dog', 'fish']\n",
        "list2 = ['cat', 'fish', 'bird']\n",
        "\n",
        "# Calculate DTW similarity\n",
        "dtw_distance_score = dtw_distance(list1, list2)\n",
        "\n",
        "print(\"DTW Similarity:\", dtw_distance_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAJ17NQjFayF",
        "outputId": "2d342b07-67bb-431f-ff99-1ef8859b57c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DTW Similarity: 1.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------\n",
        "#\n",
        "\n",
        "do_dtw = True # switch this flag if you want to run lcs\n",
        "\n",
        "if do_dtw:\n",
        "  train = train_df['sentence'].tolist(); train  = [remove_stopwords_str(s) for s in train ]\n",
        "  test  = test_df['sentence'].tolist();  test   = [remove_stopwords_str(s) for s in test ]\n",
        "  valid = valid_df['sentence'].tolist(); valid  = [remove_stopwords_str(s) for s in valid ]\n",
        "\n",
        "  X = train + valid ;   y = y_train_val\n",
        "  num_train = len(X);   num_test  = len(test)\n",
        "\n",
        "  with open('train_test_dtw02.txt', 'w') as f2:\n",
        "    for m in range(num_train):\n",
        "      if y[m] == 1:\n",
        "        s1 = X[m].split() ; n1 = len(s1)\n",
        "        for n in range(num_test):\n",
        "          s2 = test[n].split() ; n2 = len(s2)\n",
        "          r = 1- (dtw_distance (s1, s2) /(n1+n2 + 0.00000001))\n",
        "          #print(f\"{m}\\t{n}\\t{y[m]}\\t{y_test[n]}\\t{r:0.02f}\")\n",
        "          print(f\"{m}\\t{n}\\t{y[m]}\\t{n1}\\t{n2}\\t{y_test[n]}\\t{r:0.02f}\",file=f2)\n"
      ],
      "metadata": {
        "id": "67boEFXX1xqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**A teste of ResNet**"
      ],
      "metadata": {
        "id": "gYDNUM-aoPJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Define BiLSTM model\n",
        "# Define BiLSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        output, _ = self.lstm(text)\n",
        "        hidden = torch.cat((output[:, -1, :hidden_dim], output[:, 0, hidden_dim:]), dim=1)\n",
        "        return self.fc(hidden)\n",
        "\n",
        "\n",
        "# Define model parameters\n",
        "input_dim = X_train_vec.shape[1]\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 2  # Assuming binary classification\n",
        "dropout = 0.5\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = BiLSTM(input_dim, hidden_dim, output_dim, dropout)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        print(type(inputs))\n",
        "        outputs = model(inputs)\n",
        "        print('OK2')\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_val_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_val_tensor).sum().item() / len(y_val_tensor)\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "-lQqDBXfqYP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b01cce70-8e43-42ef-cb7d-1dbe5c760ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_vec' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[321], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(hidden)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Define model parameters\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_vec\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     36\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     37\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_train_vec' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "def create_lstm_model(max_len, vocab_size, embedding_dim, num_lstm_units):\n",
        "  \"\"\"\n",
        "  Creates a BiLSTM model for imbalanced document classification.\n",
        "\n",
        "  Args:\n",
        "      max_len: Maximum sequence length of documents.\n",
        "      vocab_size: Size of the vocabulary.\n",
        "      embedding_dim: Dimensionality of word2vec embeddings.\n",
        "      num_lstm_units: Number of units in the LSTM layer.\n",
        "\n",
        "  Returns:\n",
        "      A compiled TensorFlow Keras model.\n",
        "  \"\"\"\n",
        "\n",
        "  # Embedding layer for word2vec vectors\n",
        "  inputs = tf.keras.Input(shape=(max_len,))\n",
        "  embeddings = Embedding(vocab_size, embedding_dim, input_length=max_len)(inputs)\n",
        "\n",
        "  # Bidirectional LSTM layer for capturing long-range dependencies in both directions\n",
        "  lstm = Bidirectional(LSTM(num_lstm_units, return_sequences=True))(embeddings)\n",
        "\n",
        "  # Global max pooling to extract the most informative features\n",
        "  x = tf.keras.layers.GlobalMaxPooling1D()(lstm)\n",
        "\n",
        "  # Dense layer for classification\n",
        "  outputs = Dense(1, activation='sigmoid')(x)  # Sigmoid for binary classification\n",
        "\n",
        "  # Model with Adam optimizer (consider experimenting with other optimizers)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(loss='binary_crossentropy',  # For imbalanced classes\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Example usage (replace with your actual data)\n",
        "max_len = 100  # Adjust based on your data\n",
        "vocab_size = 10000  # Adjust based on your vocabulary\n",
        "embedding_dim = 300  # Adjust based on your word2vec embeddings\n",
        "num_lstm_units = 128  # Adjust based on your dataset complexity\n",
        "\n",
        "# Load your pre-trained word2vec embeddings (not shown here)\n",
        "word2vec_embeddings = X_d2v\n",
        "\n",
        "# Prepare your imbalanced training data (X: sequences, y: labels)\n",
        "X_train, y_train = X_d2v[0:y_train_val.shape[0],:], y_train_val\n",
        "\n",
        "# Class weights for handling imbalanced data (optional)\n",
        "class_weights = {0:1,1:10} #compute_class_weights(y_train)  # Replace with your weight calculation\n",
        "\n",
        "model = create_lstm_model(max_len, vocab_size, embedding_dim, num_lstm_units)\n",
        "\n",
        "# Train the model with appropriate class weights (if applicable)\n",
        "model.fit(X_train, y_train, epochs=10, class_weight=class_weights)  # Adjust epochs\n",
        "\n",
        "# Evaluate the model on your validation or test set\n",
        "model.evaluate(X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "wt_aqs8ywnzv",
        "outputId": "cc8bd448-4fd8-4af2-a3a7-b7793c8274bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        \n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        \n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        \n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        \n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        \n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 100), found shape=(None, 300)\n",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[326], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m model \u001b[38;5;241m=\u001b[39m create_lstm_model(max_len, vocab_size, embedding_dim, num_lstm_units)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Train the model with appropriate class weights (if applicable)\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust epochs\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Evaluate the model on your validation or test set\u001b[39;00m\n\u001b[0;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
            "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filegcphbqrd.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        \n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        \n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        \n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        \n    File \"c:\\python\\python38\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        \n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 100), found shape=(None, 300)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}