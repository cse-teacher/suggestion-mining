{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cse-teacher/suggestion-mining/blob/main/suggestion_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMyi6aqLVl_v"
      },
      "source": [
        "# Suggestion Mining\n",
        "Suggestion mining is the task of extracting suggestions from user reviews\n",
        "\n",
        "Developed: 11 Feb 2024 \\\\\n",
        "Last Update: 11 Feb 2024 \\\\\n",
        "Author: Muharram Mansoorizadeh plus Various AI tools (Google search, chatGPT, Gemini , ...)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0rqNS_w7Wvo"
      },
      "source": [
        "## Install Required Packagaes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMhPx71AqsaM"
      },
      "outputs": [],
      "source": [
        "#Install required packages and libraries\n",
        "\n",
        "!apt-get install libenchant-2-2\n",
        "!pip install emoji\n",
        "!pip install cleantext\n",
        "!pip install nltk\n",
        "!pip install pyenchant\n",
        "!pip install scikit-learn lightgbm catboost\n",
        "!pip install gensim\n",
        "!pip install transformers sentencepiece sacremoses\n",
        "!pip install ekphrasis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUeHgN3BM3x0"
      },
      "source": [
        "## Import data\n",
        "\n",
        "Get the required data files from github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PGYc5OXNBFh"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cse-teacher/suggestion-mining.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2FjFI8E7gDn"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xWr3Gqpm9264"
      },
      "outputs": [],
      "source": [
        "# Read data from input files\n",
        "#Reset environment\n",
        "%reset -f\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "\n",
        "#Set default seed:\n",
        "random.seed(42)\n",
        "\n",
        "#Main Application\n",
        "folder     = \"./suggestion-mining/data/\"\n",
        "train_file = folder + \"V1.4_Training.csv\" #\"Train_Augmented_03.csv\" # V1.4_Training.csv\" #  \"Train_processed.csv\" /suggestion-mining/data/Train_Augmented_03.csv\n",
        "valid_file = folder + \"SubtaskA_Trial_Test_Labeled.csv\" #\"validation_processed.csv\"\n",
        "test_file  = folder + \"SubtaskA_EvaluationData_labeled.csv\"\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "valid_df = pd.read_csv(valid_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "test_df  = pd.read_csv(test_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "all_df = pd.concat([train_df, valid_df, test_df], axis=0)\n",
        "\n",
        "\n",
        "#Get the labels:\n",
        "y_train_original = train_df['label'].values\n",
        "y_valid_original = valid_df['label'].values\n",
        "y_test_original  = test_df['label'].values\n",
        "y_all_original  = all_df['label'].values\n",
        "train_size = len(train_df['label'])\n",
        "valid_size = len(valid_df['label'])\n",
        "test_size  = len(test_df['label'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsaeHSk9PmO1"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Npr8F9hTQzwG",
        "outputId": "933a24f2-d00b-4840-9eb7-63163a769fa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced Text: PERSON should seriously look into getting rid of GPE for all these paying stuff\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import re\n",
        "import nltk\n",
        "import cleantext\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_nonalphanumeric(text):\n",
        "    #text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
        "  text = re.sub(r'\\W+', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "def remove_stopwords_list(tokens):\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  filtered_tokens = remove_stopwords_list(tokens)\n",
        "  return ' '.join(filtered_tokens)\n",
        "\n",
        "#-----------------------------------\n",
        "# Replace hyperlinks\n",
        "#\n",
        "def replace_hyperlinks(text):\n",
        "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "  return text\n",
        "\n",
        "def stem(text):\n",
        "  tokens = word_tokenize(text.strip())\n",
        "  tokens_stem =[stemmer.stem(s) for s in tokens]\n",
        "  return ' '.join(tokens_stem)\n",
        "\n",
        "#----------------------------------------\n",
        "# replace_named_entities:\n",
        "#    Replaces each word or phrase in the input text with its\n",
        "#    Named Entity Recognition (NER) tag label.\n",
        "#    Args:\n",
        "#    text (str): Input text\n",
        "#\n",
        "#    Returns:\n",
        "#    str: Text with named entities replaced by their NER tag labels\n",
        "#\n",
        "def replace_named_entities(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Tag the words with Part-of-Speech (POS) tags\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    # Perform Named Entity Recognition (NER)\n",
        "    named_entities = ne_chunk(tagged_words)\n",
        "\n",
        "    # Replace entities with their NER tag labels\n",
        "    replaced_text = []\n",
        "    for entity in named_entities:\n",
        "        if isinstance(entity, nltk.tree.Tree):\n",
        "            label = entity.label()\n",
        "            named_entity_text = \" \".join([word for word, tag in entity.leaves()])\n",
        "            #replaced_text.append(f'<{label}>{named_entity_text}</{label}>')\n",
        "            replaced_text.append(f'{label}')\n",
        "            #replaced_text.append('')\n",
        "        else:\n",
        "            replaced_text.append(entity[0])\n",
        "\n",
        "    return \" \".join(replaced_text)\n",
        "\n",
        "#Global callings:\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Example usage:\n",
        "text = \"Microsoft should seriously look into getting rid of Syamentc for all these paying stuff\"\n",
        "replaced_text = replace_named_entities(text)\n",
        "print(\"Replaced Text:\", replaced_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op_replace_hyperlinks      = True\n",
        "op_remove_nonalphanumeric  = True\n",
        "op_remove_stopwords        = False\n",
        "op_replace_named_entities  = True\n",
        "op_stem                    = False\n",
        "\n",
        "if op_replace_hyperlinks == True:\n",
        "  #replace named entities with their tag names:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_hyperlinks)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_hyperlinks)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_hyperlinks)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_hyperlinks)\n",
        "\n",
        "if op_remove_nonalphanumeric == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_nonalphanumeric)\n",
        "\n",
        "if op_replace_named_entities == True:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_named_entities)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_named_entities)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_named_entities)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_named_entities)\n",
        "\n",
        "if op_remove_stopwords == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_stopwords)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_stopwords)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_stopwords)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_stopwords)\n",
        "\n",
        "if op_stem == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(stem)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(stem)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(stem)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(stem)\n"
      ],
      "metadata": {
        "id": "ceUQZN9Ltd8s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.to_csv('all.csv')"
      ],
      "metadata": {
        "id": "JFHg96jPWWUO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# Define a function to extract aspects from the review\n",
        "def extract_aspects(review_text):\n",
        "    # Tokenize the review text\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(review_text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Part-of-speech tagging\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    # Define a chunking pattern to identify noun phrases\n",
        "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "    # Create a chunk parser\n",
        "    chunk_parser = RegexpParser(grammar)\n",
        "\n",
        "    # Chunk the tagged tokens\n",
        "    chunked_tokens = chunk_parser.parse(tagged_tokens)\n",
        "\n",
        "    # Extract noun phrases (aspects)\n",
        "    aspects = []\n",
        "    for subtree in chunked_tokens.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "        aspect = ' '.join(word for word, tag in subtree.leaves())\n",
        "        aspects.append(aspect)\n",
        "\n",
        "    return aspects\n",
        "\n",
        "# Define a function to analyze sentiment of each aspect\n",
        "def analyze_sentiment(aspects):\n",
        "    aspect_sentiments = {}\n",
        "    for aspect in aspects:\n",
        "        aspect_blob = TextBlob(aspect)\n",
        "        aspect_sentiments[aspect] = aspect_blob.sentiment.polarity\n",
        "\n",
        "    return aspect_sentiments\n",
        "\n",
        "# Sample user review\n",
        "review = \"I absolutely loved the camera quality of this phone, but the battery life was disappointing.\"\n",
        "\n",
        "\n",
        "# Extract aspects from the review\n",
        "aspects = extract_aspects(review)\n",
        "print(\"Aspects:\", aspects)\n",
        "\n",
        "# Analyze sentiment of each aspect\n",
        "aspect_sentiments = analyze_sentiment(aspects)\n",
        "print(\"Sentiments on the aspects:\")\n",
        "for aspect, sentiment in aspect_sentiments.items():\n",
        "    print(f\"{aspect}: {sentiment}\")\n"
      ],
      "metadata": {
        "id": "lG6jk8GZEOCz",
        "outputId": "f1f1d529-1e3d-4dc3-a950-fea275329b3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aspects: ['camera', 'quality', 'phone', 'battery', 'life']\n",
            "Sentiments on the aspects:\n",
            "camera: 0.0\n",
            "quality: 0.0\n",
            "phone: 0.0\n",
            "battery: 0.0\n",
            "life: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"The sound quality of these headphones is amazing, but the design is not very comfortable.\"\n",
        "\n",
        "# Extract aspects from the review\n",
        "aspects = extract_aspects(review)\n",
        "print(\"Aspects:\", aspects)\n",
        "\n",
        "# Analyze sentiment of each aspect\n",
        "aspect_sentiments = analyze_sentiment(aspects)\n",
        "print(\"Sentiments on the aspects:\")\n",
        "for aspect, sentiment in aspect_sentiments.items():\n",
        "    print(f\"{aspect}: {sentiment}\")\n"
      ],
      "metadata": {
        "id": "uNvQN-K_EZTI",
        "outputId": "10a2168f-1488-4bef-c5f2-7e916c5bc20b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aspects: ['sound quality', 'design']\n",
            "Sentiments on the aspects:\n",
            "sound quality: 0.4\n",
            "design: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqJSjOY3bZPl"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMghoc88kqxr"
      },
      "outputs": [],
      "source": [
        "#Extract BOW feature test\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "#----------------------\n",
        "# BOW Features\n",
        "bow_vectorizer = CountVectorizer(analyzer='word',\n",
        "                                 stop_words=None,\n",
        "                                 lowercase=True,\n",
        "                                 encoding='utf-8',\n",
        "                                 min_df = 5 , #                                 max_features = 5000,\n",
        "                                 max_df = 0.975,\n",
        "                                 ngram_range =(1,5))\n",
        "\n",
        "bow_vectorizer.fit(all_df['sentence'])\n",
        "train_bow_features = bow_vectorizer.transform(train_df['sentence']).toarray()\n",
        "valid_bow_features = bow_vectorizer.transform(valid_df['sentence']).toarray()\n",
        "test_bow_features  = bow_vectorizer.transform(test_df['sentence']).toarray()\n",
        "all_bow_features   = bow_vectorizer.transform(all_df['sentence']).toarray()\n",
        "\n",
        "#----------------------\n",
        "# TF-IDF Features\n",
        "\n",
        "# Fit the vectorizer on the sentences to learn vocabulary and IDF weights\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=None,\n",
        "                                 lowercase=True,\n",
        "                                 encoding='utf-8',\n",
        "                                 min_df = 5 ,\n",
        "                                 max_df = 0.95, #                                 max_features = 5000,\n",
        "                                 ngram_range =(1,5))\n",
        "\n",
        "tfidf_vectorizer.fit(all_df['sentence'])\n",
        "\n",
        "# Transform the sentences into tf-idf vectors\n",
        "train_tfidf_features = tfidf_vectorizer.transform(train_df['sentence']).toarray()\n",
        "test_tfidf_features  = tfidf_vectorizer.transform(test_df['sentence']).toarray()\n",
        "valid_tfidf_features = tfidf_vectorizer.transform(valid_df['sentence']).toarray()\n",
        "all_tfidf_features   = tfidf_vectorizer.transform(all_df['sentence']).toarray()\n",
        "\n",
        "'''\n",
        "#------------------------------------------------\n",
        "# word2vec features\n",
        "#\n",
        "docs = [wordpunct_tokenize(doc) for doc in all_df['sentence']]\n",
        "docs1 = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
        "model = Doc2Vec(docs1, vector_size=50, window=2, min_count=1, workers=4)\n",
        "\n",
        "#Get the features:\n",
        "vectors = [model.infer_vector(doc) for doc in(docs)]\n",
        "all_d2v_features = np.array(vectors)\n",
        "train_d2v_features = all_d2v_features[0:train_size,:]\n",
        "valid_d2v_features = all_d2v_features[train_size:train_size+valid_size,:]\n",
        "test_d2v_features  = all_d2v_features[train_size+valid_size:,:]\n",
        "'''\n",
        "\n",
        "#define global features, empty at first:\n",
        "X_train     = np.empty([])\n",
        "X_test      = np.empty([])\n",
        "X_valid     = np.empty([])\n",
        "X_all       = np.empty([])\n",
        "X_train_val = np.empty([])\n",
        "\n",
        "y_train = y_train_original\n",
        "y_valid = y_valid_original\n",
        "y_test  = y_test_original\n",
        "y_all   = y_all_original\n",
        "y_train_val = np.concatenate((y_train , y_valid), axis= 0 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srQ0TELENoKf"
      },
      "source": [
        "# Experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzpAtHXe5qsC"
      },
      "source": [
        "## Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7Yunwtnxl3c"
      },
      "outputs": [],
      "source": [
        "#===============================================\n",
        "# Utility functions\n",
        "#\n",
        "\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "import csv\n",
        "from tensorflow import keras\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, BatchNormalization ,Dropout\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "#------------------------------------------------\n",
        "# apply the given option\n",
        "#\n",
        "def select_optional_features(feature_group,\n",
        "                 op_scale_features  = False,\n",
        "                 op_upsample_smote  = False,\n",
        "                 op_upsample_over   = False,\n",
        "                 op_transform_pca   = False,\n",
        "                 op_downsample_majority = False\n",
        "                 ):\n",
        "  global X_train, X_valid, X_test, X_all\n",
        "  global y_train, y_valid, y_test, y_all\n",
        "  global X_train_val , y_train_val\n",
        "\n",
        "  description = feature_group\n",
        "  y_train = y_train_original\n",
        "  y_valid = y_valid_original\n",
        "  y_test  = y_test_original\n",
        "\n",
        "  if feature_group == 'tfidf' :\n",
        "    X_train = train_tfidf_features\n",
        "    X_test  = test_tfidf_features\n",
        "    X_valid = valid_tfidf_features\n",
        "    X_all   = all_tfidf_features\n",
        "  elif feature_group == 'bow':\n",
        "    X_train = train_bow_features\n",
        "    X_test  = test_bow_features\n",
        "    X_valid = valid_bow_features\n",
        "    X_all   = all_bow_features\n",
        "  elif feature_group == 'd2v':\n",
        "    X_train = train_d2v_features\n",
        "    X_test  = test_d2v_features\n",
        "    X_valid = valid_d2v_features\n",
        "    X_all   = all_d2v_features\n",
        "\n",
        "  if op_scale_features == True: # Scale numerical features\n",
        "     scaler  = StandardScaler().fit(X_all); description += ', Standard Scaler'\n",
        "     X_all   = scaler.transform(X_all)\n",
        "     X_train = scaler.transform(X_train)\n",
        "     X_test  = scaler.transform(X_test)\n",
        "     X_valid = scaler.transform(X_valid)\n",
        "\n",
        "  if op_upsample_smote == True: # SMOTE oversampling\n",
        "    smote = SMOTE(sampling_strategy=\"minority\") ; description += ', SMOTE Augmentation'\n",
        "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "  if op_upsample_over == True: # Random oversampling\n",
        "    oversampler = RandomOverSampler(random_state=42); description += ', oversampling Augmentation'\n",
        "    X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "  if op_transform_pca == True:  # Do PCA\n",
        "    pca = PCA(n_components=100).fit(X_all);  description += ', PCA'\n",
        "    X_train = pca.transform(X_train) ; X_test = pca.transform(X_test)\n",
        "    X_valid = pca.transform(X_valid) ; X_all = pca.transform(X_all)\n",
        "\n",
        "  if op_downsample_majority == True:  # Down sample majority class\n",
        "      # Separate instances for class 1\n",
        "    class_1_instances = X_train[y_train == 1,:]\n",
        "    class_0_instances = X_train[y_train == 0,:]\n",
        "    number_of_samples = class_1_instances.shape[0]\n",
        "    indices = np.random.choice(class_0_instances.shape[0], number_of_samples, replace=False)\n",
        "    sampled_class_0_instances = class_0_instances[indices,:]\n",
        "\n",
        "    # Combine instances for class 1 and sampled instances from class 0\n",
        "    X_train = np.concatenate([class_1_instances, sampled_class_0_instances])\n",
        "    y_train = np.concatenate([np.ones(class_1_instances.shape[0]), np.zeros(sampled_class_0_instances.shape[0])])\n",
        "\n",
        "\n",
        "    # Train + Validation data\n",
        "  X_train_val = np.concatenate((X_train, X_valid) , axis=0)\n",
        "  y_train_val = np.concatenate((y_train, y_valid) , axis=0)\n",
        "  return description\n",
        "\n",
        "#----------------------------------\n",
        "# Print results per class\n",
        "#\n",
        "def print_per_class_results(y_actual, y_pred, description=''):\n",
        "  for label in (0,1):\n",
        "    v0 = accuracy_score(y_actual, y_pred)\n",
        "    v1 = precision_score(y_actual, y_pred, pos_label=label)\n",
        "    v2 = recall_score(y_actual, y_pred, pos_label=label)\n",
        "    v3 = f1_score(y_actual, y_pred, pos_label=label)\n",
        "    print(f\"{description},\\t class={label}\\tAccuracy={v0:.2f},\\t Precision={v1:.2f},\\tRecall={v2:.2f}\\tF1-score={v3:.2f}\")\n",
        "\n",
        "\n",
        "#----------------------------------\n",
        "# Print results per class\n",
        "#\n",
        "def print_results(y_actual, y_pred, description=''):\n",
        "  v00 = accuracy_score(y_actual, y_pred)\n",
        "  v01 = precision_score(y_actual, y_pred, pos_label=0)\n",
        "  v02 = recall_score(y_actual, y_pred, pos_label=0)\n",
        "  v03 = f1_score(y_actual, y_pred, pos_label=0)\n",
        "\n",
        "  v11 = precision_score(y_actual, y_pred, pos_label=1)\n",
        "  v12 = recall_score(y_actual, y_pred, pos_label=1)\n",
        "  v13 = f1_score(y_actual, y_pred, pos_label=1)\n",
        "\n",
        "  smsg = f\"{description},\\tAccuracy={v00:.2f},\\tC0: Pr={v01:.2f}, Re={v02:.2f}, F1={v03:.2f},\\tC1: Pr={v11:.2f}, Re={v12:.2f}, F1={v13:.2f}\"\n",
        "  print(smsg)\n",
        "  with open(\"results.txt\", \"a\") as myfile:\n",
        "    myfile.write(f\"{datetime.now()}\\t {smsg}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8KPz9KChLrA"
      },
      "source": [
        "**Experimental Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7M08KHYgBc5"
      },
      "outputs": [],
      "source": [
        " #select options here and run classifiers as you like:\n",
        "\n",
        " current_options = select_optional_features(feature_group = 'bow',\n",
        "                 op_scale_features  = False,\n",
        "                 op_upsample_smote  = False,\n",
        "                 op_upsample_over   = False,\n",
        "                 op_transform_pca   = True ,\n",
        "                 op_downsample_majority = False,\n",
        "\n",
        "                                            )\n",
        "\n",
        " print(current_options)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_df['sentence'])"
      ],
      "metadata": {
        "id": "QE0s8MDM9UiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Discrimination Analysis**"
      ],
      "metadata": {
        "id": "AQroVkX_1_eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = lda.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "print_results(y_test , y_pred, 'LDA, ' + current_options )\n"
      ],
      "metadata": {
        "id": "m7DrD9Zx0_fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypMLOuIfkaoB"
      },
      "source": [
        "**Basic Methods**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLzrQpgakgHf"
      },
      "outputs": [],
      "source": [
        "#Some Useful classifiers\n",
        "classifiers = {\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=21, metric=\"cosine\"),\n",
        "    'Logistic Regression': sklearn.linear_model.LogisticRegression(random_state=42),\n",
        "    'Support Vector Machine-L': sklearn.svm.SVC(kernel='linear', random_state=42),\n",
        "    'Support Vector Machine-R': sklearn.svm.SVC(kernel='rbf', random_state=42),\n",
        "    'Support Vector Machine-S': sklearn.svm.SVC(kernel='sigmoid', random_state=42),\n",
        "    'Support Vector Machine-WL': sklearn.svm.SVC(kernel=\"linear\", class_weight={1: 10}, random_state=42),\n",
        "    'Support Vector Machine-WR': sklearn.svm.SVC(kernel=\"rbf\", class_weight={1: 10}, random_state=42),\n",
        "    'Support Vector Machine-WS': sklearn.svm.SVC(kernel=\"sigmoid\", class_weight={1: 10}, random_state=42),\n",
        "    'Decision Tree classifier': DecisionTreeClassifier(max_depth=15, random_state=42),\n",
        "}\n",
        "\n",
        "# Loop through each classifier and evaluate performance\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train_val, y_train_val)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print_results(y_test , y_pred, name + ', '+ current_options)\n",
        "\n",
        "\n",
        "#---------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdGv3RxdMA5r"
      },
      "source": [
        "**Ensemble Models**\n",
        "\n",
        "This experiment trains well-known ensemble methods on the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0xQuTCmMCAp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    \"Random Forest\"     : RandomForestClassifier(n_estimators=1001,class_weight={0:1,1:10}, n_jobs=-1,verbose=1),\n",
        "    \"AdaBoost\"          : AdaBoostClassifier(n_estimators=1001),\n",
        "    \"Gradient Boosting\" : GradientBoostingClassifier(),\n",
        "    \"Extra Trees\"       : ExtraTreesClassifier(),\n",
        "    \"LightGBM\"          : LGBMClassifier(),\n",
        "    \"CatBoost\"          : CatBoostClassifier(verbose=0)\n",
        "}\n",
        "\n",
        "# Train and evaluate each classifier\n",
        "X_train_val = np.concatenate((X_train, X_valid) , axis=0)\n",
        "y_train_val = np.concatenate((y_train, y_valid) , axis=0)\n",
        "\n",
        "# Loop through each classifier and evaluate performance\n",
        "for name, clf in classifiers.items():\n",
        "    description = name + ', '+ current_options\n",
        "    clf.fit(X_train_val, y_train_val)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print_results(y_test , y_pred, description)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "D0p6161Zx-Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHxOrXSPovV0"
      },
      "source": [
        "**Neural Networks**\n",
        "\n",
        "This network is trained on the training and validation sets and\n",
        "tested on the testing set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datetime import datetime\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # Input layer\n",
        "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "        self.layers.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
        "        self.layers.append(nn.Tanh())\n",
        "        self.layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(hidden_sizes)):\n",
        "            self.layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
        "            self.layers.append(nn.BatchNorm1d(hidden_sizes[i]))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            self.layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
        "        self.layers.append(nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "input_size = X_train_val.shape[1]  # Adjust this based on your input features\n",
        "hidden_sizes = [1000, 500, 250, 100, 50]\n",
        "\n",
        "# Instantiate the model\n",
        "model_mlp2 = MLPModel(input_size, hidden_sizes)\n",
        "\n",
        "# Check if GPU is available and move the model and data to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_mlp2.to(device)\n",
        "print(device)\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()  #nn.BCEWithLogitsLoss() #nn.CrossEntropyLoss() #nn.MSELoss() #nn.KLDivLoss() # nn.BCELoss()\n",
        "optimizer = optim.Adam(model_mlp2.parameters(), lr=0.001)\n",
        "\n",
        "#Generate balanced dataset for training:\n",
        "# Separate instances for class 1\n",
        "class_1_instances = X_train_val[y_train_val == 1,:]\n",
        "class_0_instances = X_train_val[y_train_val == 0,:]\n",
        "number_of_samples = class_1_instances.shape[0]\n",
        "\n",
        "index             = np.random.choice(class_0_instances.shape[0], number_of_samples, replace=False)\n",
        "sampled_class_0_instances = class_0_instances[index,:]\n",
        "\n",
        "# Combine instances for class 1 and sampled instances from class 0\n",
        "#balanced_X = np.concatenate([class_1_instances, sampled_class_0_instances])\n",
        "#balanced_y = np.concatenate([np.ones(class_1_instances.shape[0]), np.zeros(sampled_class_0_instances.shape[0])])\n",
        "balanced_X = X_train_val ; balanced_y = y_train_val\n",
        "\n",
        "\n",
        "# Dummy data (replace this with your actual dataset)\n",
        "# Assuming you have X_train and y_train as your training data and labels\n",
        "data_X = torch.Tensor(balanced_X).to(device)\n",
        "data_y = torch.Tensor(balanced_y).view(-1, 1).to(device)\n",
        "\n",
        "# Create DataLoader for the dataset\n",
        "dataset = TensorDataset(data_X, data_y)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "top_n      = 101 # top n better models\n",
        "num_epochs = 1000\n",
        "losses     = [10000]* top_n\n",
        "# Get the current date and time\n",
        "current_datetime     = datetime.now()\n",
        "current_datetime_str = f\"{current_datetime.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_mlp2(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch %10 == 0 :\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "    # Save better models:\n",
        "    cur_loss =loss.item()\n",
        "    for k in range(len(losses)):\n",
        "        if cur_loss < losses[k]:\n",
        "            losses[k] = cur_loss\n",
        "            if k < top_n:\n",
        "                model_file_name = f\"mlp_model_best_{k:02d}_{current_datetime_str}.pth\"\n",
        "                torch.save(model_mlp2.state_dict(), model_file_name)\n",
        "                #print(losses)\n",
        "                break\n",
        "\n",
        "\n",
        "#save the last model\n",
        "print(losses)\n",
        "model_file_name = f\"mlp_model_last_{current_datetime.strftime('%Y-%m-%d_%H-%M-%S')}.pth\"\n",
        "torch.save(model_mlp2.state_dict(), model_file_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------------------\n",
        "# Lets get training accuracy\n",
        "# Set the model to evaluation mode and evaluate it on train data:\n",
        "model_mlp2.eval()\n",
        "predictions=[]\n",
        "with torch.no_grad():\n",
        "  for inputs in dataloader: #remember from the earlier cell that this is the train dataloader\n",
        "    outputs = model_mlp2(inputs[0])\n",
        "    predictions.append(outputs.cpu().data.numpy())\n",
        "# Calculate accuracy\n",
        "predictions = np.concatenate(predictions)\n",
        "y_pred = predictions >= 0.5\n",
        "\n",
        "print_results(y_train_val , y_pred, 'torch nn, training ' + current_options)\n",
        "\n"
      ],
      "metadata": {
        "id": "mjhGuKclSinl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------\n",
        "# Test the network\n",
        "fname = f\"mlp_model_best_05_{current_datetime_str}.pth\"\n",
        "model = MLPModel(input_size, hidden_sizes)\n",
        "model.load_state_dict(torch.load(fname))\n",
        "\n",
        "#Prepare test data:\n",
        "new_data = torch.Tensor(X_test) #.to(device)\n",
        "\n",
        "# Create DataLoader for the new dataset\n",
        "new_dataset = TensorDataset(new_data)\n",
        "new_dataloader = DataLoader(new_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "predictions = []\n",
        "model.eval()\n",
        "# Make predictions on the test data\n",
        "with torch.no_grad():\n",
        "  for inputs in new_dataloader:\n",
        "    outputs = model(inputs[0])#(torch.tensor(X_test))\n",
        "    #predictions = torch.round(outputs)\n",
        "    predictions.append(outputs.cpu().data.numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "predictions = np.concatenate(predictions)\n",
        "y_pred = predictions >= 0.5\n",
        "\n",
        "print_results(y_test , y_pred, 'torch nn ' + current_options)"
      ],
      "metadata": {
        "id": "2rgKAAuUE6kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "1bkBCPnoSnp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T44vTY5NowqV"
      },
      "outputs": [],
      "source": [
        "    #'MLP Network': MLPClassifier(hidden_layer_sizes=(150, 100,50), activation='relu', solver='adam', max_iter=1000),\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Embedding, LSTM, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Define common parameters\n",
        "train_size, vocab_size  = X_train.shape\n",
        "print(X_train.shape)\n",
        "  # Adjust based on your data\n",
        "max_len  = vocab_size  # Adjust based on your data\n",
        "numepochs  = 100\n",
        "\n",
        "model_mlp1 = MLPClassifier(random_state=42, max_iter=50)\n",
        "\n",
        "\n",
        "model_mlp2 = Sequential()\n",
        "model_mlp2.add(Dense(500, input_dim=X_train.shape[1]))\n",
        "model_mlp2.add(BatchNormalization())\n",
        "model_mlp2.add(Activation(activation='sigmoid'))\n",
        "model_mlp2.add(Dropout(0.2))\n",
        "model_mlp2.add(Dense(250))\n",
        "model_mlp2.add(BatchNormalization())\n",
        "model_mlp2.add(Activation(activation='relu'))\n",
        "model_mlp2.add(Dropout(0.2))\n",
        "model_mlp2.add(Dense(100))\n",
        "model_mlp2.add(BatchNormalization())\n",
        "model_mlp2.add(Activation(activation='sigmoid'))\n",
        "model_mlp2.add(Dropout(0.2))\n",
        "model_mlp2.add(Dense(50))\n",
        "model_mlp2.add(BatchNormalization())\n",
        "model_mlp2.add(Activation(activation='sigmoid'))\n",
        "model_mlp2.add(Dropout(0.2))\n",
        "model_mlp2.add(Dense(1,activation=tf.keras.activations.sigmoid))\n",
        "model_mlp2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define CNN model\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Embedding(vocab_size, 128, input_length=max_len))\n",
        "model_cnn.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn.add(Flatten())\n",
        "model_cnn.add(Dense(128, activation='relu'))\n",
        "model_cnn.add(Dense(len(set(y_train)), activation='softmax'))\n",
        "model_cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define RNN model\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(vocab_size, 128, input_length=max_len))\n",
        "model_rnn.add(LSTM(64, return_sequences=True))\n",
        "model_rnn.add(LSTM(32))\n",
        "model_rnn.add(Dense(128, activation='relu'))\n",
        "model_rnn.add(Dense(len(set(y_train)), activation='softmax'))\n",
        "model_rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_size, 128, input_length=max_len))\n",
        "model_lstm.add(LSTM(128))\n",
        "model_lstm.add(Dense(64, activation='relu'))\n",
        "model_lstm.add(Dense(len(set(y_train)), activation='softmax'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Evaluate and compare models\n",
        "# Initialize classifiers\n",
        "NN = {\n",
        "    \"Modern MLPt\": model_mlp2,\n",
        "    \"CNN\": model_cnn,\n",
        "    \"Recurrent NN\": model_rnn,\n",
        "    \"LSTM\": model_lstm,\n",
        "}\n",
        "\n",
        "# Loop through each classifier and evaluate performance\n",
        "for name, clf in NN.items():\n",
        "  clf.fit(X_train_val, y_train_val, epochs=numepochs)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  if (y_pred.ndim > 1): y_pred = np.argmax(y_pred , axis=1)\n",
        "\n",
        "\n",
        "  print_results(y_test , y_pred, name + ', '+ current_options)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_suggestion(paragraph):\n",
        "    suggestion_keywords = [\"should\", \"could\", \"might\", \"ought to\", \"would\", \"recommend\", \"suggest\", \"consider\"]\n",
        "    polite_phrases = [\"would you mind\", \"could you please\", \"I suggest\", \"please\", \"if you want to\"]\n",
        "    for keyword in suggestion_keywords:\n",
        "        if keyword in paragraph.lower():\n",
        "            return True\n",
        "    for phrase in polite_phrases:\n",
        "        if phrase in paragraph.lower():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def classify_paragraphs(paragraphs):\n",
        "    suggestions = []\n",
        "    other_paragraphs = []\n",
        "    for paragraph in paragraphs:\n",
        "        if contains_suggestion(paragraph):\n",
        "            suggestions.append(paragraph)\n",
        "        else:\n",
        "            other_paragraphs.append(paragraph)\n",
        "    return suggestions, other_paragraphs\n",
        "\n",
        "# Example list of paragraphs\n",
        "paragraphs = [\n",
        "    \"You should try the new restaurant.\",\n",
        "    \"I suggest you take a break and relax.\",\n",
        "    \"The meeting lasted two hours.\",\n",
        "    \"I believe that exercise is important for maintaining good health.\"\n",
        "]\n",
        "\n",
        "suggestions, other_paragraphs = classify_paragraphs(test_df['sentence'])\n",
        "\n",
        "print(\"Paragraphs containing suggestions:\")\n",
        "for suggestion in suggestions:\n",
        "    print(\"-\", suggestion)\n",
        "\n",
        "print(\"\\nOther paragraphs:\")\n",
        "for paragraph in other_paragraphs:\n",
        "    print(\"-\", paragraph)\n"
      ],
      "metadata": {
        "id": "kq0dRjOaPM3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9o02SgiTxpA"
      },
      "source": [
        "**Word2vec Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edGIdtSDWH_H"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGd22J1bWTKf"
      },
      "outputs": [],
      "source": [
        "wv = api.load('word2vec-google-news-300')\n",
        "#wv.save('/content/drive/MyDrive/Content Creation/YASH/vectors.kv')\n",
        "wv['apple']#vector represnation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRhDG6ZIRWMG"
      },
      "outputs": [],
      "source": [
        "classifier = MLPClassifier(hidden_layer_sizes=(150,100,50),\n",
        "                           max_iter=40,activation = 'relu',\n",
        "                           solver='adam',random_state=1).fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "classifier.fit(X_train,y_train); plt.plot(classifier.loss_curve_,label=\"train\")\n",
        "classifier.fit(X_valid,y_valid); plt.plot(classifier.loss_curve_,label=\"validation\")\n",
        "classifier.fit(X_test,y_test); plt.plot(classifier.loss_curve_,label=\"test\")\n",
        "\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Misclassification Rate/Loss\");\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('mlp-tfidf-training')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.stdout =sys.__stdout__\n",
        "print(sys.__stdout__, 'hello')"
      ],
      "metadata": {
        "id": "srLyr5o-dVVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balanced Classification**\n",
        "\n",
        "This experiments trains an ensemble of random forests on the balanced subsets"
      ],
      "metadata": {
        "id": "vS--U_kAR4xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Separate instances for class 1\n",
        "class_1_instances = X_train_val[y_train_val == 1,:]\n",
        "class_0_instances = X_train_val[y_train_val == 0,:]\n",
        "num_classifiers = 15\n",
        "classifiers = []\n",
        "number_of_samples = class_1_instances.shape[0]\n",
        "# Build an ensemble of classifiers (Random Forests in this example)\n",
        "\n",
        "for ks in range(1,num_classifiers+1):\n",
        "  # Randomly sample 2000 instances from class 0\n",
        "  indices = np.random.choice(class_0_instances.shape[0], number_of_samples, replace=True)\n",
        "\n",
        "  sampled_class_0_instances = class_0_instances[indices,:]\n",
        "\n",
        "  # Combine instances for class 1 and sampled instances from class 0\n",
        "  balanced_X = np.concatenate([class_1_instances, sampled_class_0_instances])\n",
        "  balanced_y = np.concatenate([np.ones(class_1_instances.shape[0]), np.zeros(sampled_class_0_instances.shape[0])])\n",
        "\n",
        "  classifier = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
        "  #classifier = sklearn.linear_model.LogisticRegression(random_state=42)\n",
        "  classifier.fit(balanced_X, balanced_y)\n",
        "  classifiers.append(classifier)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  print_results(y_test , y_pred,  current_options)\n",
        "\n",
        "# Make predictions on the test set using each classifier\n",
        "predictions = [classifier.predict(X_test) for classifier in classifiers]\n",
        "\n",
        "# Take a majority vote to get the final ensemble prediction\n",
        "ensemble_predictions = np.mean(predictions, axis=0) > 0.5\n",
        "\n",
        "# Evaluate the ensemble performance\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
        "print(f'Ensemble Accuracy: {ensemble_accuracy}')\n"
      ],
      "metadata": {
        "id": "R-gOUwRaR5VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Longest Common Subsequence**"
      ],
      "metadata": {
        "id": "3fF6EUfhxT42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def longest_common_subsequence(str1, str2):\n",
        "    words1 = str1.split()\n",
        "    words2 = str2.split()\n",
        "\n",
        "    m = len(words1)\n",
        "    n = len(words2)\n",
        "\n",
        "    # Initializing the dp table with zeros\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Building the dp table\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if words1[i - 1] == words2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
        "\n",
        "    # Backtracking to find the longest common subsequence\n",
        "    lcs_length = dp[m][n]\n",
        "    lcs = []\n",
        "    i = m\n",
        "    j = n\n",
        "    while i > 0 and j > 0:\n",
        "        if words1[i - 1] == words2[j - 1]:\n",
        "            lcs.append ( words1[i - 1])\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "            lcs_length -= 1\n",
        "        elif dp[i - 1][j] > dp[i][j - 1]:\n",
        "            i -= 1\n",
        "        else:\n",
        "            j -= 1\n",
        "\n",
        "    lcs.reverse()\n",
        "    return lcs , (2*len(lcs))/ (1+m+n) # 1 is denom is there to prevent div by 0\n",
        "# Example usage:\n",
        "str1        = \"roses are red. violets are blue\"\n",
        "str2        = \"the garden is full of roses and violets that are blue \"\n",
        "lcs12 , r12 = longest_common_subsequence(str1, str2)\n",
        "print(f\"Longest Common Subsequence:{r12}: {lcs12} \")\n"
      ],
      "metadata": {
        "id": "cKfUYz7WxTPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords_list(tokens):\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "def remove_stopwords_str(str):\n",
        "  str = re.sub(r'[^A-Za-z0-9]+', ' ', str)\n",
        "  str = re.sub(r'\\W+', ' ', str)\n",
        "  str = re.sub(r'\\s+', ' ', str)\n",
        "\n",
        "  tokens = str.split()\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return ' '.join(filtered_tokens)\n",
        "\n",
        "#--------------------\n",
        "#\n",
        "\n",
        "do_lcs = True # switch this flag if you want to run lcs\n",
        "\n",
        "if do_lcs:\n",
        "  train = train_df['sentence'].tolist(); train  = [remove_stopwords_str(s) for s in train ]\n",
        "  test  = test_df['sentence'].tolist();  test   = [remove_stopwords_str(s) for s in test ]\n",
        "  valid = valid_df['sentence'].tolist(); valid  = [remove_stopwords_str(s) for s in valid ]\n",
        "\n",
        "  orig_stdout = sys.stdout;  f = open('out05.txt', 'w'); sys.stdout = f\n",
        "\n",
        "  num_train = len(train); num_valid = len(valid); num_test  = len(test)\n",
        "  for m in range(num_train):\n",
        "    for n in range(num_valid):\n",
        "      temp, r = longest_common_subsequence (train[m], valid[n])\n",
        "      if r >= 0.1:\n",
        "        print(f\"({m}, {n})\\t({y_train[m]}, {y_valid[n]}, ),\\t{r:0.02f},\\t{len(temp)},\\t{temp}\")\n",
        "  #---\n",
        "  sys.stdout = orig_stdout ;   f.close()\n"
      ],
      "metadata": {
        "id": "LzLh7vJ0xZpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_str(str):\n",
        "  re.sub(r'[^A-Za-z0-9]+', ' ', str)\n",
        "  re.sub(r'\\W+', ' ', str)\n",
        "  re.sub(r'\\s+', ' ', str)\n",
        "\n",
        "s = train_df['sentence'][7]\n",
        "print(s, remove_stopwords_str(s))"
      ],
      "metadata": {
        "id": "GI9hpnwlBxsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**A teste of ResNet**"
      ],
      "metadata": {
        "id": "gYDNUM-aoPJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Assuming X_train contains your text data and y_train contains the corresponding labels\n",
        "num_classes = 2\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_val, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_val, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Load pre-trained ResNet\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "# Remove the fully connected layer\n",
        "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "# Freeze ResNet parameters\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define classifier on top of ResNet\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(X_train_val.shape[0], 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, num_classes)  # num_classes is the number of classes in your classification task\n",
        ")\n",
        "\n",
        "# Combine ResNet and classifier\n",
        "model = nn.Sequential(\n",
        "    resnet,\n",
        "    nn.Flatten(),\n",
        "    classifier\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Adjust as needed\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Print average loss for each epoch\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_val_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_val_tensor).sum().item() / len(y_val_tensor)\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "bzZ3VGqxoQEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Define BiLSTM model\n",
        "# Define BiLSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        output, _ = self.lstm(text)\n",
        "        hidden = torch.cat((output[:, -1, :hidden_dim], output[:, 0, hidden_dim:]), dim=1)\n",
        "        return self.fc(hidden)\n",
        "\n",
        "\n",
        "# Define model parameters\n",
        "input_dim = X_train_vec.shape[1]\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 2  # Assuming binary classification\n",
        "dropout = 0.5\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = BiLSTM(input_dim, hidden_dim, output_dim, dropout)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        print(type(inputs))\n",
        "        outputs = model(inputs)\n",
        "        print('OK2')\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_val_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_val_tensor).sum().item() / len(y_val_tensor)\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "-lQqDBXfqYP9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}