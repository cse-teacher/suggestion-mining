{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cse-teacher/suggestion-mining/blob/main/suggestion_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMyi6aqLVl_v"
      },
      "source": [
        "# Suggestion Mining\n",
        "Suggestion mining is the task of extracting suggestions from user reviews\n",
        "\n",
        "Developed: 11 Feb 2024 \\\\\n",
        "Last Update: 11 Feb 2024 \\\\\n",
        "Author: Muharram Mansoorizadeh plus Various AI tools (Google search, chatGPT, Gemini , ...)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0rqNS_w7Wvo"
      },
      "source": [
        "## Install Required Packagaes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMhPx71AqsaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78022af-9cca-43d1-8b99-5a2bf26e6ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in c:\\python\\python38\\lib\\site-packages (2.10.1)\n",
            "Requirement already satisfied: cleantext in c:\\python\\python38\\lib\\site-packages (1.1.4)\n",
            "Requirement already satisfied: nltk in c:\\python\\python38\\lib\\site-packages (from cleantext) (3.8.1)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\python\\python38\\lib\\site-packages (from nltk->cleantext) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from click->nltk->cleantext) (0.4.6)\n",
            "Requirement already satisfied: nltk in c:\\python\\python38\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python38\\lib\\site-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\python\\python38\\lib\\site-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: pyenchant in c:\\python\\python38\\lib\\site-packages (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\python\\python38\\lib\\site-packages (1.3.2)\n",
            "Requirement already satisfied: lightgbm in c:\\python\\python38\\lib\\site-packages (4.3.0)\n",
            "Requirement already satisfied: catboost in c:\\python\\python38\\lib\\site-packages (1.2.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.24.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\python38\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: graphviz in c:\\python\\python38\\lib\\site-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in c:\\python\\python38\\lib\\site-packages (from catboost) (3.7.5)\n",
            "Requirement already satisfied: pandas>=0.24 in c:\\python\\python38\\lib\\site-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: plotly in c:\\python\\python38\\lib\\site-packages (from catboost) (5.19.0)\n",
            "Requirement already satisfied: six in c:\\python\\python38\\lib\\site-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\python\\python38\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->catboost) (6.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\python\\python38\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\python\\python38\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.17.0)\n",
            "Requirement already satisfied: gensim in c:\\python\\python38\\lib\\site-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\python\\python38\\lib\\site-packages (from gensim) (1.24.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\python\\python38\\lib\\site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\python\\python38\\lib\\site-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: transformers in c:\\python\\python38\\lib\\site-packages (4.38.2)\n",
            "Requirement already satisfied: sentencepiece in c:\\python\\python38\\lib\\site-packages (0.2.0)\n",
            "Requirement already satisfied: sacremoses in c:\\python\\python38\\lib\\site-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in c:\\python\\python38\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\python\\python38\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\python\\python38\\lib\\site-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: ekphrasis in c:\\python\\python38\\lib\\site-packages (0.5.4)\n",
            "Requirement already satisfied: termcolor in c:\\python\\python38\\lib\\site-packages (from ekphrasis) (2.4.0)\n",
            "Requirement already satisfied: tqdm in c:\\python\\python38\\lib\\site-packages (from ekphrasis) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from ekphrasis) (0.4.6)\n",
            "Requirement already satisfied: ujson in c:\\python\\python38\\lib\\site-packages (from ekphrasis) (5.9.0)\n",
            "Requirement already satisfied: matplotlib in c:\\python\\python38\\lib\\site-packages (from ekphrasis) (3.7.5)\n",
            "Requirement already satisfied: nltk in c:\\python\\python38\\lib\\site-packages (from ekphrasis) (3.8.1)\n",
            "Requirement already satisfied: ftfy in c:\\python\\python38\\lib\\site-packages (from ekphrasis) (6.1.3)\n",
            "Requirement already satisfied: numpy in c:\\python\\python38\\lib\\site-packages (from ekphrasis) (1.24.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\python\\python38\\lib\\site-packages (from ftfy->ekphrasis) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->ekphrasis) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\python\\python38\\lib\\site-packages (from matplotlib->ekphrasis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->ekphrasis) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->ekphrasis) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->ekphrasis) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->ekphrasis) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib->ekphrasis) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\python\\python38\\lib\\site-packages (from matplotlib->ekphrasis) (2.9.0.post0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib->ekphrasis) (6.1.3)\n",
            "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from nltk->ekphrasis) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from nltk->ekphrasis) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python38\\lib\\site-packages (from nltk->ekphrasis) (2023.12.25)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\python\\python38\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->ekphrasis) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\python\\python38\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "#Install required packages and libraries\n",
        "\n",
        "!apt-get install libenchant-2-2\n",
        "!pip install emoji\n",
        "!pip install cleantext\n",
        "!pip install nltk\n",
        "!pip install pyenchant\n",
        "!pip install scikit-learn lightgbm catboost\n",
        "!pip install gensim\n",
        "!pip install transformers sentencepiece sacremoses\n",
        "!pip install ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "FJz6I9CXqXzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a5f7fd-46a8-486e-dc7b-4311de6895db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in c:\\python\\python38\\lib\\site-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python\\python38\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python\\python38\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python\\python38\\lib\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python\\python38\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python\\python38\\lib\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\python\\python38\\lib\\site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python\\python38\\lib\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python\\python38\\lib\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python\\python38\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\python\\python38\\lib\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\python\\python38\\lib\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\python\\python38\\lib\\site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python\\python38\\lib\\site-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python\\python38\\lib\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python\\python38\\lib\\site-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in c:\\python\\python38\\lib\\site-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in c:\\python\\python38\\lib\\site-packages (from spacy) (41.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python\\python38\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\python\\python38\\lib\\site-packages (from spacy) (1.24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in c:\\python\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\python\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python\\python38\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python\\python38\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\python\\python38\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\python\\python38\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python\\python38\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     --------------------------------------- 0.0/12.8 MB 660.6 kB/s eta 0:00:20\n",
            "     --------------------------------------- 0.0/12.8 MB 660.6 kB/s eta 0:00:20\n",
            "     --------------------------------------- 0.1/12.8 MB 409.6 kB/s eta 0:00:32\n",
            "     --------------------------------------- 0.1/12.8 MB 598.8 kB/s eta 0:00:22\n",
            "     --------------------------------------- 0.1/12.8 MB 516.7 kB/s eta 0:00:25\n",
            "      -------------------------------------- 0.3/12.8 MB 874.6 kB/s eta 0:00:15\n",
            "     - ------------------------------------- 0.3/12.8 MB 955.3 kB/s eta 0:00:14\n",
            "     - -------------------------------------- 0.5/12.8 MB 1.3 MB/s eta 0:00:10\n",
            "     -- ------------------------------------- 0.7/12.8 MB 1.5 MB/s eta 0:00:08\n",
            "     --- ------------------------------------ 1.1/12.8 MB 2.1 MB/s eta 0:00:06\n",
            "     ---- ----------------------------------- 1.5/12.8 MB 2.5 MB/s eta 0:00:05\n",
            "     ------- -------------------------------- 2.4/12.8 MB 3.8 MB/s eta 0:00:03\n",
            "     --------- ------------------------------ 3.2/12.8 MB 4.5 MB/s eta 0:00:03\n",
            "     ---------------- ----------------------- 5.2/12.8 MB 6.8 MB/s eta 0:00:02\n",
            "     --------------------- ------------------ 6.9/12.8 MB 8.3 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 9.3/12.8 MB 10.5 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 9.3/12.8 MB 10.5 MB/s eta 0:00:01\n",
            "     --------------------------------------  12.7/12.8 MB 26.2 MB/s eta 0:00:01\n",
            "     --------------------------------------- 12.8/12.8 MB 26.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\python\\python38\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (41.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\python\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in c:\\python\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\python\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python\\python38\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python\\python38\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\python\\python38\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\python\\python38\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python\\python38\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "ner_categories =['PERSON', 'PRDOUCT' , 'ORG', 'GPE']\n",
        "text = 'John drives to Sidny school every day with his windows phone made by microsoft'\n",
        "doc = nlp(text)\n",
        "print(doc)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text , ent.label, ent.label_)"
      ],
      "metadata": {
        "id": "8n2Bl0jfrA8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef328846-c59e-4d8d-e54b-988082f91301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John drives to Sidny school every day with his windows phone made by microsoft\n",
            "John 380 PERSON\n",
            "Sidny 384 GPE\n",
            "microsoft 383 ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUeHgN3BM3x0"
      },
      "source": [
        "## Import data\n",
        "\n",
        "Get the required data files from github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PGYc5OXNBFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b512692e-fbcf-4887-826c-ab45f1a701c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fatal: destination path 'suggestion-mining' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cse-teacher/suggestion-mining.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2FjFI8E7gDn"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWr3Gqpm9264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093551d2-bab0-4740-af90-e88f7cb26d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current start time 2024-04-10_16-11-43\n"
          ]
        }
      ],
      "source": [
        "# Read data from input files\n",
        "#Reset environment\n",
        "%reset -f\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "#Set default seed:\n",
        "random.seed(42)\n",
        "\n",
        "#Main Application\n",
        "folder     = \"./suggestion-mining/data/\"\n",
        "train_file = folder + \"V1.4_Training.csv\" #\"Train_Augmented_03.csv\" # V1.4_Training.csv\" #  \"Train_processed.csv\" /suggestion-mining/data/Train_Augmented_03.csv\n",
        "valid_file = folder + \"SubtaskA_Trial_Test_Labeled.csv\" #\"validation_processed.csv\"\n",
        "test_file  = folder + \"SubtaskA_EvaluationData_labeled.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "valid_df = pd.read_csv(valid_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "test_df  = pd.read_csv(test_file,\n",
        "                       encoding_errors='ignore', header=None,\n",
        "                       names=[\"id\", \"sentence\", \"label\"])\n",
        "\n",
        "all_df = pd.concat([train_df, valid_df, test_df], axis=0)\n",
        "\n",
        "\n",
        "#Get the labels:\n",
        "y_train_original = train_df['label'].values\n",
        "y_valid_original = valid_df['label'].values\n",
        "y_test_original  = test_df['label'].values\n",
        "y_all_original  = all_df['label'].values\n",
        "train_size = len(train_df['label'])\n",
        "valid_size = len(valid_df['label'])\n",
        "test_size  = len(test_df['label'])\n",
        "\n",
        "# to save results and models we need time-based file names\n",
        "current_datetime = datetime.now()\n",
        "results_df = pd.DataFrame({'labels': y_test_original})\n",
        "start_time_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "print(f\"current start time {start_time_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsaeHSk9PmO1"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npr8F9hTQzwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e514ccd-3c4b-41c3-8f9e-597fab18a5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced Text: PERSON should seriously look into getting rid of GPE for all these paying stuff\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mmr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import re\n",
        "import nltk\n",
        "import cleantext\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_nonalpha(text):\n",
        "    #text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
        "  text = re.sub(r'[^A-Za-z]+', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "def remove_nonalphanumeric(text):\n",
        "    #text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
        "  text = re.sub(r'\\W+', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "def remove_stopwords_list(tokens):\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  filtered_tokens = remove_stopwords_list(tokens)\n",
        "  return ' '.join(filtered_tokens)\n",
        "\n",
        "#-----------------------------------\n",
        "# Replace hyperlinks\n",
        "#\n",
        "def replace_hyperlinks(text):\n",
        "  text = re.sub(r'https?:\\/\\/\\S+', 'hyperlink', text)\n",
        "  return text\n",
        "\n",
        "def stem(text):\n",
        "  tokens = word_tokenize(text.strip())\n",
        "  tokens_stem =[stemmer.stem(s) for s in tokens]\n",
        "  return ' '.join(tokens_stem)\n",
        "\n",
        "#----------------------------------------\n",
        "# replace_named_entities:\n",
        "#    Replaces each word or phrase in the input text with its\n",
        "#    Named Entity Recognition (NER) tag label.\n",
        "#    Args:\n",
        "#    text (str): Input text\n",
        "#\n",
        "#    Returns:\n",
        "#    str: Text with named entities replaced by their NER tag labels\n",
        "#\n",
        "def replace_named_entities(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Tag the words with Part-of-Speech (POS) tags\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    # Perform Named Entity Recognition (NER)\n",
        "    named_entities = ne_chunk(tagged_words)\n",
        "\n",
        "    # Replace entities with their NER tag labels\n",
        "    replaced_text = []\n",
        "    for entity in named_entities:\n",
        "        if isinstance(entity, nltk.tree.Tree):\n",
        "            label = entity.label()\n",
        "            named_entity_text = \" \".join([word for word, tag in entity.leaves()])\n",
        "            #replaced_text.append(f'<{label}>{named_entity_text}</{label}>')\n",
        "            replaced_text.append(f'{label}')\n",
        "            #replaced_text.append('')\n",
        "        else:\n",
        "            replaced_text.append(entity[0])\n",
        "\n",
        "    return \" \".join(replaced_text)\n",
        "\n",
        "#Global callings:\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Example usage:\n",
        "text = \"Microsoft should seriously look into getting rid of Syamentc for all these paying stuff\"\n",
        "replaced_text = replace_named_entities(text)\n",
        "print(\"Replaced Text:\", replaced_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op_replace_hyperlinks      = True\n",
        "op_remove_nonalphanumeric  = True\n",
        "op_remove_nonalpha         = True\n",
        "op_remove_stopwords        = False\n",
        "op_replace_named_entities  = False\n",
        "op_stem                    = False\n",
        "\n",
        "if op_replace_hyperlinks == True:\n",
        "  #replace named entities with their tag names:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_hyperlinks)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_hyperlinks)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_hyperlinks)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_hyperlinks)\n",
        "\n",
        "if op_remove_nonalphanumeric == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_nonalphanumeric)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_nonalphanumeric)\n",
        "\n",
        "if op_remove_nonalpha  == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_nonalpha)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_nonalpha)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_nonalpha)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_nonalpha)\n",
        "\n",
        "\n",
        "if op_replace_named_entities == True:\n",
        "  train_df['sentence']  = train_df['sentence'].apply(replace_named_entities)\n",
        "  test_df['sentence']   = test_df['sentence'].apply(replace_named_entities)\n",
        "  valid_df['sentence']  = valid_df['sentence'].apply(replace_named_entities)\n",
        "  all_df['sentence']    = all_df['sentence'].apply(replace_named_entities)\n",
        "\n",
        "if op_remove_stopwords == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(remove_stopwords)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(remove_stopwords)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(remove_stopwords)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(remove_stopwords)\n",
        "\n",
        "if op_stem == True:\n",
        "  train_df['sentence'] = train_df['sentence'].apply(stem)\n",
        "  valid_df['sentence'] = valid_df['sentence'].apply(stem)\n",
        "  test_df['sentence']  = test_df['sentence'].apply(stem)\n",
        "  all_df['sentence']   = all_df['sentence'].apply(stem)\n",
        "\n",
        "#print some instances\n",
        "train_df['sentence'][195:200].tolist()\n"
      ],
      "metadata": {
        "id": "ceUQZN9Ltd8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e3d97a-9ffc-4618-e8ef-2f5e710d3ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' When creating an app that uses MediaStreamSource to stream audio in Windows Phone everything works just great ',\n",
              " ' When porting this app to Windows Phone the sound is flickering and its components are stack overflow ing ',\n",
              " ' Here is a discussion on MSDN forums hyperlink And here is a sample project source code hyperlink',\n",
              " ' we are publishing the same apps in Windows Phone Store and Windows App Store ',\n",
              " ' Now we want to bundle these Apps ']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqJSjOY3bZPl"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMghoc88kqxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f168fcf4-bf8e-43ed-900f-fc9fa4ba1154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features size: \n",
            "Bow: (9925, 21609),\n",
            "BOW: (9925, 21609)\n",
            "Doc2vec: (9925, 300)\n"
          ]
        }
      ],
      "source": [
        "#Extract BOW feature test\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "#----------------------\n",
        "# BOW Features\n",
        "bow_vectorizer = CountVectorizer(analyzer='word',\n",
        "                                 stop_words=None,\n",
        "                                 lowercase=True,\n",
        "                                 encoding='utf-8',\n",
        "                                 min_df = 3 ,\n",
        "                                 max_df = 0.975,\n",
        "                                 ngram_range =(1,5))\n",
        "\n",
        "bow_vectorizer.fit(all_df['sentence'])\n",
        "train_bow_features = bow_vectorizer.transform(train_df['sentence']).toarray()\n",
        "valid_bow_features = bow_vectorizer.transform(valid_df['sentence']).toarray()\n",
        "test_bow_features  = bow_vectorizer.transform(test_df['sentence']).toarray()\n",
        "all_bow_features   = bow_vectorizer.transform(all_df['sentence']).toarray()\n",
        "\n",
        "#----------------------\n",
        "# TF-IDF Features\n",
        "\n",
        "# Fit the vectorizer on the sentences to learn vocabulary and IDF weights\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=None,\n",
        "                                 lowercase=True,\n",
        "                                 encoding='utf-8',\n",
        "                                 min_df = 3 ,\n",
        "                                 max_df = 0.975, #\n",
        "                                 ngram_range =(1,5))\n",
        "\n",
        "tfidf_vectorizer.fit(all_df['sentence'])\n",
        "\n",
        "# Transform the sentences into tf-idf vectors\n",
        "train_tfidf_features = tfidf_vectorizer.transform(train_df['sentence']).toarray()\n",
        "test_tfidf_features  = tfidf_vectorizer.transform(test_df['sentence']).toarray()\n",
        "valid_tfidf_features = tfidf_vectorizer.transform(valid_df['sentence']).toarray()\n",
        "all_tfidf_features   = tfidf_vectorizer.transform(all_df['sentence']).toarray()\n",
        "\n",
        "#------------------------------------------------\n",
        "# word2vec features\n",
        "#\n",
        "docs = [wordpunct_tokenize(doc) for doc in all_df['sentence']]\n",
        "docs1 = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
        "model_d2v = Doc2Vec(docs1, vector_size=300, window=4, min_count=1, workers=4, epochs=100)\n",
        "\n",
        "#Get the features:\n",
        "vectors = [model_d2v.infer_vector(doc) for doc in(docs)]\n",
        "all_d2v_features = np.array(vectors)\n",
        "train_d2v_features = all_d2v_features[0:train_size,:]\n",
        "valid_d2v_features = all_d2v_features[train_size:train_size+valid_size,:]\n",
        "test_d2v_features  = all_d2v_features[train_size+valid_size:,:]\n",
        "\n",
        "#define global features, empty at first:\n",
        "X_train     = np.empty([])\n",
        "X_test      = np.empty([])\n",
        "X_valid     = np.empty([])\n",
        "X_all       = np.empty([])\n",
        "X_train_val = np.empty([])\n",
        "\n",
        "y_train = y_train_original\n",
        "y_valid = y_valid_original\n",
        "y_test  = y_test_original\n",
        "y_all   = y_all_original\n",
        "y_train_val = np.concatenate((y_train , y_valid), axis= 0 )\n",
        "print(f\"Features size: \\nBow: {all_bow_features.shape},\\nBOW: { all_tfidf_features.shape}\\nDoc2vec: {all_d2v_features.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7Yunwtnxl3c"
      },
      "outputs": [],
      "source": [
        "#===============================================\n",
        "# Utility functions\n",
        "#\n",
        "\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from nltk.tokenize import word_tokenize\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "#------------------------------------------------\n",
        "# apply the given option\n",
        "#\n",
        "def select_optional_features(feature_group,\n",
        "                 op_scale_features  = False,\n",
        "                 op_upsample_smote  = False,\n",
        "                 op_upsample_over   = False,\n",
        "                 op_transform_pca   = False,\n",
        "                 op_downsample_majority = False\n",
        "                 ):\n",
        "  global X_train, X_valid, X_test, X_all\n",
        "  global y_train, y_valid, y_test, y_all\n",
        "  global X_train_val , y_train_val\n",
        "  global results_df, start_time_str\n",
        "\n",
        "  description = feature_group\n",
        "  y_train = y_train_original\n",
        "  y_valid = y_valid_original\n",
        "  y_test  = y_test_original\n",
        "\n",
        "  if feature_group == 'tfidf' :\n",
        "    X_train = train_tfidf_features\n",
        "    X_test  = test_tfidf_features\n",
        "    X_valid = valid_tfidf_features\n",
        "    X_all   = all_tfidf_features\n",
        "  elif feature_group == 'bow':\n",
        "    X_train = train_bow_features\n",
        "    X_test  = test_bow_features\n",
        "    X_valid = valid_bow_features\n",
        "    X_all   = all_bow_features\n",
        "  elif feature_group == 'd2v':\n",
        "    X_train = train_d2v_features\n",
        "    X_test  = test_d2v_features\n",
        "    X_valid = valid_d2v_features\n",
        "    X_all   = all_d2v_features\n",
        "\n",
        "  if op_scale_features == True: # Scale numerical features\n",
        "     scaler  = StandardScaler().fit(X_all); description += ', Standard Scaler'\n",
        "     X_all   = scaler.transform(X_all)\n",
        "     X_train = scaler.transform(X_train)\n",
        "     X_test  = scaler.transform(X_test)\n",
        "     X_valid = scaler.transform(X_valid)\n",
        "\n",
        "  if op_upsample_smote == True: # SMOTE oversampling\n",
        "    smote = SMOTE(sampling_strategy=\"minority\") ; description += ', SMOTE Augmentation'\n",
        "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "  if op_upsample_over == True: # Random oversampling\n",
        "    oversampler = RandomOverSampler(random_state=42); description += ', oversampling Augmentation'\n",
        "    X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "  if op_transform_pca == True:  # Do PCA\n",
        "    #n_comps = min(500 , 0.1 * X_all.shape[1])\n",
        "    pca = PCA(n_components=0.95).fit(X_all);  description += ', PCA'\n",
        "    X_train = pca.transform(X_train) ; X_test = pca.transform(X_test)\n",
        "    X_valid = pca.transform(X_valid) ; X_all = pca.transform(X_all)\n",
        "\n",
        "  if op_downsample_majority == True:  # Down sample majority class\n",
        "      # Separate instances for class 1\n",
        "    class_1_instances = X_train[y_train == 1,:]\n",
        "    class_0_instances = X_train[y_train == 0,:]\n",
        "    number_of_samples = class_1_instances.shape[0]\n",
        "    indices = np.random.choice(class_0_instances.shape[0], number_of_samples, replace=False)\n",
        "    sampled_class_0_instances = class_0_instances[indices,:]\n",
        "\n",
        "    # Combine instances for class 1 and sampled instances from class 0\n",
        "    X_train = np.concatenate([class_1_instances, sampled_class_0_instances])\n",
        "    y_train = np.concatenate([np.ones(class_1_instances.shape[0]), np.zeros(sampled_class_0_instances.shape[0])])\n",
        "\n",
        "\n",
        "    # Train + Validation data\n",
        "  X_train_val = np.concatenate((X_train, X_valid) , axis=0)\n",
        "  y_train_val = np.concatenate((y_train, y_valid) , axis=0)\n",
        "  return description\n",
        "\n",
        "#----------------------------------\n",
        "# Print results per class\n",
        "#\n",
        "def print_per_class_results(y_actual, y_pred, description=''):\n",
        "  for label in (0,1):\n",
        "    v0 = accuracy_score(y_actual, y_pred)\n",
        "    v1 = precision_score(y_actual, y_pred, pos_label=label)\n",
        "    v2 = recall_score(y_actual, y_pred, pos_label=label)\n",
        "    v3 = f1_score(y_actual, y_pred, pos_label=label)\n",
        "    print(f\"{description},\\t class={label}\\tAccuracy={v0:.2f},\\t Precision={v1:.2f},\\tRecall={v2:.2f}\\tF1-score={v3:.2f}\")\n",
        "\n",
        "\n",
        "#----------------------------------\n",
        "# Print results per class\n",
        "#\n",
        "def print_results(y_actual, y_pred, description=''):\n",
        "\n",
        "  try:\n",
        "    v00 = accuracy_score(y_actual, y_pred)\n",
        "    v01 = precision_score(y_actual, y_pred, pos_label=0)\n",
        "    v02 = recall_score(y_actual, y_pred, pos_label=0)\n",
        "    v03 = f1_score(y_actual, y_pred, pos_label=0)\n",
        "\n",
        "    v11 = precision_score(y_actual, y_pred, pos_label=1)\n",
        "    v12 = recall_score(y_actual, y_pred, pos_label=1)\n",
        "    v13 = f1_score(y_actual, y_pred, pos_label=1)\n",
        "\n",
        "    smsg = f\"{description},\\tAccuracy={v00:.2f},\\tC0: Pr={v01:.2f}, Re={v02:.2f}, F1={v03:.2f},\\tC1: Pr={v11:.2f}, Re={v12:.2f}, F1={v13:.2f}\"\n",
        "    print(smsg)\n",
        "    with open(f\"results_{start_time_str}.txt\", \"a\") as myfile:\n",
        "      myfile.write(f\"{datetime.now()}\\t {smsg}\\n\")\n",
        "\n",
        "    results_df.insert(len(results_df.columns),description, y_pred)\n",
        "  except Exception as error:\n",
        "      print(f\"something went wrong {error}\")\n",
        "\n",
        "#cutoff probability to make a binary value\n",
        "def prob2label (y, threshold=0.5):\n",
        "  y[y <  threshold] = 0\n",
        "  y[y >= threshold] = 1\n",
        "  return y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ttest based keywords selection:\n",
        "# Import the library\n",
        "import scipy.stats as stats\n",
        "\n",
        "def ttest2(X,y):\n",
        "  X1 = X[y==1, :]; X2 =X[y!=1,:] ;\n",
        "  numcols = X.shape[1]\n",
        "  sval = np.zeros(numcols, float)\n",
        "  pval = np.zeros(numcols, float)\n",
        "  for k in range(0,numcols):\n",
        "    test_result = stats.ttest_ind(a=X1[:,k], b=X2[:,k], equal_var=True)\n",
        "    sval[k] = test_result.statistic\n",
        "    pval[k] = test_result.pvalue\n",
        "  return sval, pval\n",
        "\n",
        "# filter bow features\n",
        "cutoff = 0.1\n",
        "[A,B] = ttest2(train_bow_features , y_train)\n",
        "best_bow_features = np.argsort(B, axis=-1, kind=None, order=None)\n",
        "last_bow_feature = np.min(np.argwhere(B[best_bow_features] >= cutoff))\n",
        "best_bow_feature_names = bow_vectorizer.get_feature_names_out()[best_bow_features[0:last_bow_feature]]\n",
        "\n",
        "print(\"top bag of words features\")\n",
        "for temp1, temp2 in zip(A[best_bow_features[0:21]] , best_bow_feature_names[0:21]):\n",
        "  print(temp1, temp2)\n",
        "\n",
        "train_bow_features= train_bow_features[:,best_bow_features[0:last_bow_feature]]\n",
        "test_bow_features = test_bow_features[:,best_bow_features[0:last_bow_feature]]\n",
        "valid_bow_features = valid_bow_features[:,best_bow_features[0:last_bow_feature]]\n",
        "all_bow_features   = all_bow_features[:,best_bow_features[0:last_bow_feature]]\n",
        "\n",
        "# filter tfidf features\n",
        "[A,B] = ttest2(train_tfidf_features , y_train)\n",
        "best_tfidf_features = np.argsort(B, axis=-1, kind=None, order=None)\n",
        "last_tfidf_feature  = np.min(np.argwhere(B[best_tfidf_features] > cutoff))\n",
        "best_tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()[best_tfidf_features[0:last_tfidf_feature]]\n",
        "\n",
        "train_tfidf_features= train_tfidf_features[:,best_tfidf_features[0:last_tfidf_feature]]\n",
        "test_tfidf_features = test_tfidf_features[:,best_tfidf_features[0:last_tfidf_feature]]\n",
        "valid_tfidf_features = valid_tfidf_features[:,best_tfidf_features[0:last_tfidf_feature]]\n",
        "all_tfidf_features   = all_tfidf_features[:,best_tfidf_features[0:last_tfidf_feature]]\n",
        "\n",
        "print(\"top tfidf features\")\n",
        "for temp1, temp2 in zip(A[best_tfidf_features[0:21]] , best_tfidf_feature_names[0:21]):\n",
        "  print(temp1, temp2)"
      ],
      "metadata": {
        "id": "07ktlt3mJnCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de713ace-c58c-4f1a-8772-7372ae6564bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top bag of words features\n",
            "29.84823731094551 be\n",
            "26.83771370728371 please\n",
            "24.477182481135024 would\n",
            "24.277129097207578 should\n",
            "24.04109159453123 would be\n",
            "23.744243274558357 it would be\n",
            "23.028961778924728 it would\n",
            "19.647209708601522 add\n",
            "17.91422174105728 should be\n",
            "16.341188180608427 to\n",
            "15.684943328100735 be nice\n",
            "15.107936555960004 allow\n",
            "14.829530810372402 would be nice\n",
            "14.561731461492071 be great\n",
            "14.261383265436097 would like\n",
            "14.213066453660174 like\n",
            "13.949114539147656 could\n",
            "13.878409913159958 like to\n",
            "13.60842887725592 nice\n",
            "13.225095456619743 please add\n",
            "13.151128026483732 would be great\n",
            "top tfidf features\n",
            "26.860273305594365 be\n",
            "23.341397329294534 should\n",
            "22.972202109103893 it would be\n",
            "22.599648508804396 please\n",
            "21.663924479147987 it would\n",
            "20.85548177338402 would be\n",
            "20.704967010442438 would\n",
            "19.145279434238937 add\n",
            "17.074184741542684 should be\n",
            "16.188344780708565 to\n",
            "14.949655022078138 allow\n",
            "14.896112496339999 be nice\n",
            "14.126907397595398 would be nice\n",
            "13.851169888779989 would like\n",
            "-13.49532185795253 is\n",
            "13.475972787179044 nice\n",
            "13.445771372600184 like to\n",
            "13.255468517977672 be great\n",
            "12.770161228554903 be able\n",
            "12.550669922696272 it would be great\n",
            "12.537546075414795 please add\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# print output to the console\n",
        "print(os.getcwd())\n",
        "#os.chdir('c:/users/mmr')\n",
        "print(os.getcwd())\n"
      ],
      "metadata": {
        "id": "pFObKJipx1PS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b49154c-f6c3-4c43-886b-c4ec795ad60f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C:\\Users\\mmr\n",
            "C:\\Users\\mmr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Top 10 bow features{best_bow_feature_names[0:10]}\")\n",
        "print(f\"Top 10 tfidf features{best_tfidf_feature_names[0:10]}\")\n",
        "#results_df.to_csv(f\"labels_{start_time_str}.csv\")"
      ],
      "metadata": {
        "id": "wUGUdvdfh9Uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5791641-2f6a-4682-fed1-43e734d45235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 bow features['be' 'please' 'would' 'should' 'would be' 'it would be' 'it would' 'add'\n",
            " 'should be' 'to']\n",
            "Top 10 tfidf features['be' 'should' 'it would be' 'please' 'it would' 'would be' 'would' 'add'\n",
            " 'should be' 'to']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srQ0TELENoKf"
      },
      "source": [
        "# Experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzpAtHXe5qsC"
      },
      "source": [
        "## Utility Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8KPz9KChLrA"
      },
      "source": [
        "**Experimental Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7M08KHYgBc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f968ff60-251e-4102-e0be-c87b10698ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf\n"
          ]
        }
      ],
      "source": [
        " #select options here and run classifiers as you like:\n",
        "\n",
        " current_options = select_optional_features(feature_group = 'tfidf',\n",
        "                 op_scale_features  = False,\n",
        "                 op_upsample_smote  = False,\n",
        "                 op_upsample_over   = False,\n",
        "                 op_transform_pca   = False ,\n",
        "                 op_downsample_majority = False,\n",
        "\n",
        "                                            )\n",
        "\n",
        " print(current_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rule Based Methods##\n",
        "\n",
        "This section contains several rule based methods."
      ],
      "metadata": {
        "id": "MEBPxuRbgF3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_group = 'human'\n",
        "keyword_count = 20\n",
        "human_keywords = [\"should\", \"could\", \"might\", \"ought to\", \"would\", \"recommend\",\n",
        "                       \"suggest\", \"consider\", \"better\", \"allow\", \"would you mind\",\n",
        "                       \"could you please\", \"I suggest\", \"please\", \"if you want to\",\n",
        "                       \"be able to\", \"it would be\"]\n",
        "\n",
        "if keyword_group =='human':\n",
        "  suggestion_keywords = human_keywords\n",
        "\n",
        "elif keyword_group =='bow':\n",
        "  suggestion_keywords = best_bow_feature_names.tolist()\n",
        "\n",
        "elif keyword_group == 'tfidf':\n",
        "  suggestion_keywords = best_tfidf_feature_names.tolist()\n",
        "\n",
        "if keyword_count >0 :\n",
        "  suggestion_keywords = suggestion_keywords[0:min(keyword_count, len(suggestion_keywords))]\n",
        "\n",
        "def contains_suggestion(paragraph):\n",
        "    for keyword in suggestion_keywords:\n",
        "        if keyword in paragraph.lower():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def classify_paragraphs(paragraphs):\n",
        "    y_pred = []\n",
        "    for paragraph in paragraphs:\n",
        "        if contains_suggestion(paragraph):\n",
        "            y_pred.append(1)\n",
        "        else:\n",
        "            y_pred.append(0)\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "y_pred = classify_paragraphs(test_df['sentence'])\n",
        "print_results(y_test, y_pred, f\"keywords: {keyword_group}, count: {len(suggestion_keywords)}\" )"
      ],
      "metadata": {
        "id": "kq0dRjOaPM3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "609b036b-9a96-445e-ee16-61f80e6193ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keywords: human, count: 17,\tAccuracy=0.87,\tC0: Pr=0.97, Re=0.89, F1=0.93,\tC1: Pr=0.44, Re=0.76, F1=0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypMLOuIfkaoB"
      },
      "source": [
        "**Basic Methods**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create MinMaxScaler instance with feature_range=(0, 10)\n",
        "scaler = MinMaxScaler(feature_range=(10, 20))\n",
        "\n",
        "# Fit the scaler to your data\n",
        "scaler.fit(X_all)\n",
        "\n",
        "# Transform your data\n",
        "X1 = scaler.transform(X_train_val)\n",
        "X2 = scaler.transform(X_test)\n",
        "\n",
        "# Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X1, y_train_val)\n",
        "nb_predictions = nb_classifier.predict(X2)\n",
        "\n",
        "# Bayesian classifier\n",
        "bayesian_classifier = BayesianRidge()\n",
        "bayesian_classifier.fit(X1, y_train_val)\n",
        "bayesian_predictions = bayesian_classifier.predict(X2)\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "print_results(y_test , nb_predictions >=0.5, 'Naive Bayes, '+ current_options)\n",
        "print_results(y_test , bayesian_predictions>=0.5, 'BayesianRidge, '+ current_options)\n",
        "\n"
      ],
      "metadata": {
        "id": "EIeAyjlbUl1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8668bf4b-d4f8-4934-fe55-ea656ffb7c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes, tfidf,\tAccuracy=0.90,\tC0: Pr=0.90, Re=1.00, F1=0.95,\tC1: Pr=0.57, Re=0.05, F1=0.09\n",
            "BayesianRidge, tfidf,\tAccuracy=0.91,\tC0: Pr=0.95, Re=0.94, F1=0.95,\tC1: Pr=0.54, Re=0.57, F1=0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLzrQpgakgHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c6bd00-c9bb-4822-c720-fd7b78d8e9f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, tfidf,\tAccuracy=0.91,\tC0: Pr=0.92, Re=0.98, F1=0.95,\tC1: Pr=0.67, Re=0.28, F1=0.39\n",
            "something went wrong cannot insert basic K-Nearest Neighbors, tfidf, already exists\n",
            "basic Linear Discrimination Analysis, tfidf,\tAccuracy=0.80,\tC0: Pr=0.95, Re=0.83, F1=0.88,\tC1: Pr=0.29, Re=0.60, F1=0.39\n",
            "basic Logistic Regression, tfidf,\tAccuracy=0.92,\tC0: Pr=0.94, Re=0.98, F1=0.96,\tC1: Pr=0.67, Re=0.43, F1=0.52\n",
            "basic Support Vector Machine-L, tfidf,\tAccuracy=0.92,\tC0: Pr=0.95, Re=0.97, F1=0.96,\tC1: Pr=0.65, Re=0.55, F1=0.60\n",
            "basic Support Vector Machine-R, tfidf,\tAccuracy=0.87,\tC0: Pr=0.95, Re=0.91, F1=0.93,\tC1: Pr=0.41, Re=0.55, F1=0.47\n",
            "basic Support Vector Machine-S, tfidf,\tAccuracy=0.91,\tC0: Pr=0.96, Re=0.94, F1=0.95,\tC1: Pr=0.55, Re=0.62, F1=0.58\n",
            "basic Support Vector Machine-WL, tfidf,\tAccuracy=0.81,\tC0: Pr=0.98, Re=0.80, F1=0.88,\tC1: Pr=0.35, Re=0.89, F1=0.50\n",
            "basic Support Vector Machine-WR, tfidf,\tAccuracy=0.85,\tC0: Pr=0.97, Re=0.86, F1=0.91,\tC1: Pr=0.38, Re=0.76, F1=0.51\n",
            "basic Support Vector Machine-WS, tfidf,\tAccuracy=0.76,\tC0: Pr=0.99, Re=0.74, F1=0.85,\tC1: Pr=0.29, Re=0.91, F1=0.44\n",
            "basic Decision Tree classifier, tfidf,\tAccuracy=0.89,\tC0: Pr=0.95, Re=0.93, F1=0.94,\tC1: Pr=0.49, Re=0.57, F1=0.53\n"
          ]
        }
      ],
      "source": [
        "#Some Useful classifiers\n",
        "def test_basic_models(X1,y1,X2,y2, description):\n",
        "  classifiers = {\n",
        "      'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=21, metric=\"cosine\"),\n",
        "      'Linear Discrimination Analysis':LinearDiscriminantAnalysis(),\n",
        "      'Logistic Regression': sklearn.linear_model.LogisticRegression(random_state=42),\n",
        "      'Support Vector Machine-L': sklearn.svm.SVC(kernel='linear', random_state=42),\n",
        "      'Support Vector Machine-R': sklearn.svm.SVC(kernel='rbf', random_state=42),\n",
        "      'Support Vector Machine-S': sklearn.svm.SVC(kernel='sigmoid', random_state=42),\n",
        "      'Support Vector Machine-WL': sklearn.svm.SVC(kernel=\"linear\", class_weight={1: 10}, random_state=42),\n",
        "      'Support Vector Machine-WR': sklearn.svm.SVC(kernel=\"rbf\", class_weight={1: 10}, random_state=42),\n",
        "      'Support Vector Machine-WS': sklearn.svm.SVC(kernel=\"sigmoid\", class_weight={1: 10}, random_state=42),\n",
        "      'Decision Tree classifier': DecisionTreeClassifier(max_depth=15, random_state=42),\n",
        "  }\n",
        "\n",
        "  # Loop through each classifier and evaluate performance\n",
        "  for name, clf in classifiers.items():\n",
        "      clf.fit(X1, y1)\n",
        "      y_pred = clf.predict(X2)\n",
        "      print_results(y2 , y_pred, 'basic ' + name + ', '+ description)\n",
        "#---------------------\n",
        "test_basic_models (X_train_val , y_train_val , X_test , y_test, current_options)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdGv3RxdMA5r"
      },
      "source": [
        "**Ensemble Models**\n",
        "\n",
        "This experiment trains well-known ensemble methods on the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0xQuTCmMCAp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def test_ensemble_models(X1,y1,X2,y2, description):\n",
        "  # Initialize classifiers\n",
        "  classifiers = {\n",
        "      \"Random Forest\"     : RandomForestClassifier(n_estimators=101,class_weight={0:1,1:10}, n_jobs=-1, random_state=42),\n",
        "      \"AdaBoost\"          : AdaBoostClassifier(n_estimators=101, random_state=42),\n",
        "      \"Gradient Boosting\" : GradientBoostingClassifier(n_estimators=101, random_state=42),\n",
        "      \"Extra Trees\"       : ExtraTreesClassifier(n_estimators=101, random_state=42),\n",
        "      \"LightGBM\"          : LGBMClassifier(n_estimators=101, random_state=42),\n",
        "      \"CatBoost\"          : CatBoostClassifier(n_estimators=101, random_state=42)\n",
        "  }\n",
        "\n",
        "  # Loop through each classifier and evaluate performance\n",
        "  for name, clf in classifiers.items():\n",
        "      clf.fit(X1, y1)\n",
        "      y_pred = clf.predict(X2)\n",
        "      print_results(y2 , y_pred, 'Ensemble, ' + name + ', ' + description)\n",
        "\n",
        "\n",
        "# Train and evaluate ensemble models\n",
        "test_ensemble_models (X_train_val , y_train_val , X_test , y_test, current_options)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHxOrXSPovV0"
      },
      "source": [
        "**Neural Networks**\n",
        "\n",
        "This network is trained on the training and validation sets and\n",
        "tested on the testing set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP Neural Networks**"
      ],
      "metadata": {
        "id": "X1vSzn0vw47q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#simple mlp neural networks\n",
        "mlp_classifier1 = MLPClassifier(hidden_layer_sizes=(50,25,20,10,5),\n",
        "                           max_iter=100,activation = 'relu',\n",
        "                           solver='adam',random_state=100).fit(train_d2v_features[0:y_train_val.shape[0],:], y_train)\n",
        "\n",
        "y_pred = mlp_classifier1.predict(all_d2v_features)\n",
        "print(confusion_matrix(y_all,y_pred))\n",
        "print(classification_report(y_all,y_pred))\n",
        "print_results(y_all , y_pred , 'MLP_Basic1_alldata(d2v)))\n",
        "\n",
        "\n",
        "y_pred = mlp_classifier1.predict(test_d2v_features)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print_results(y_test , y_pred , 'MLP_Basic1(d2v)')\n",
        "\n",
        "\n",
        "mlp_classifier2 = MLPClassifier(hidden_layer_sizes=(50,25,20,10,5),\n",
        "                           max_iter=100,activation = 'relu',\n",
        "                           solver='adam',random_state=100).fit(X_train_val, y_train_val)\n",
        "\n",
        "y_pred = mlp_classifier2.predict(X_all)\n",
        "print(confusion_matrix(y_all,y_pred))\n",
        "print(classification_report(y_all,y_pred))\n",
        "print_results(y_all , y_pred , 'MLP_Basic_2_all_data')\n",
        "\n",
        "\n",
        "y_pred = mlp_classifier2.predict(X_test)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print_results(y_test , y_pred , 'MLP_Basic_2')\n",
        "\n",
        "#Third MLP classifier\n",
        "mlp_classifier3 = MLPClassifier(hidden_layer_sizes=(150,100,50),\n",
        "                           max_iter=100,activation = 'relu',\n",
        "                           solver='adam',random_state=100).fit(X_train_val, y_train_val)\n",
        "\n",
        "y_pred = mlp_classifier3.predict(X_test)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "mlp_classifier3.fit(X_train,y_train); plt.plot(mlp_classifier3.loss_curve_,label=\"train\")\n",
        "mlp_classifier3.fit(X_valid,y_valid); plt.plot(mlp_classifier3.loss_curve_,label=\"validation\")\n",
        "mlp_classifier3.fit(X_test,y_test); plt.plot(mlp_classifier3.loss_curve_,label=\"test\")\n",
        "\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Misclassification Rate/Loss\");\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('mlp-tfidf-training')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rH5DSOIj2w6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "2c7ce0ad-822c-49e5-d790-78c9021f1914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[663  83]\n",
            " [ 31  56]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.89      0.92       746\n",
            "           1       0.40      0.64      0.50        87\n",
            "\n",
            "    accuracy                           0.86       833\n",
            "   macro avg       0.68      0.77      0.71       833\n",
            "weighted avg       0.90      0.86      0.88       833\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#init cuda usage\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datetime import datetime\n",
        "if torch.cuda.is_available():\n",
        " dev = \"cuda:0\"\n",
        "else:\n",
        " dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print(device)\n"
      ],
      "metadata": {
        "id": "6J5T6Ahtv0zy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48fd9bd-b369-44ae-fc6d-348a2347f052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datetime import datetime\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # Input layer\n",
        "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "        self.layers.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
        "        self.layers.append(nn.Tanh())\n",
        "        self.layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(hidden_sizes)):\n",
        "            self.layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
        "            self.layers.append(nn.BatchNorm1d(hidden_sizes[i]))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            self.layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
        "        self.layers.append(nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "#build and train model\n",
        "input_size = X_train_val.shape[1]  # Adjust this based on your input features\n",
        "hidden_sizes = [500, 250, 100, 50]\n",
        "\n",
        "# Instantiate the model_mlp\n",
        "model_mlp = MLPModel(input_size, hidden_sizes)\n",
        "model_mlp.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()  #nn.BCEWithLogitsLoss() #nn.CrossEntropyLoss() #nn.MSELoss() #nn.KLDivLoss() # nn.BCELoss()\n",
        "optimizer = optim.Adam(model_mlp.parameters(), lr=0.001)\n",
        "\n",
        "data_X = torch.Tensor(X_train_val).to(device)\n",
        "data_y = torch.Tensor(y_train_val).view(-1, 1).to(device)\n",
        "\n",
        "# Create DataLoader for the dataset\n",
        "dataset = TensorDataset(data_X, data_y)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "top_n      = 10 # top n better models\n",
        "num_epochs = 250\n",
        "losses     = [10000]* top_n\n",
        "# Get the current date and time\n",
        "current_datetime_str = f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
        "\n",
        "model_mlp.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_mlp(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
        "    # Save better models:\n",
        "    cur_loss = total_loss\n",
        "    for k in range(len(losses)):\n",
        "        if cur_loss < losses[k]:\n",
        "            losses[k] = cur_loss\n",
        "            if k < top_n:\n",
        "                model_file_name = f\"mlp_model_best_{k:02d}_{current_datetime_str}.pth\"\n",
        "                torch.save(model_mlp.state_dict(), model_file_name)\n",
        "                #print(losses)\n",
        "                break\n",
        "\n",
        "\n",
        "#save the last model_mlp\n",
        "print(losses)\n",
        "model_file_name = f\"mlp_model_last_{current_datetime_str}.pth\"\n",
        "torch.save(model_mlp, model_file_name)\n",
        "\n",
        "#------------------------------------------\n",
        "# Lets get training accuracy\n",
        "# Set the model_mlp to evaluation mode and evaluate it on train data:\n",
        "model_mlp.eval()\n",
        "with torch.no_grad():\n",
        "  #Training accuracy\n",
        "  predictions = model_mlp(torch.Tensor(X_train_val).to(device))\n",
        "  predictions[predictions >= 0.5]  = 1\n",
        "  predictions[predictions <  0.5]  = 0\n",
        "  print_results(y_train_val , predictions.cpu(), 'MLP_torch_train, ' + current_options)\n",
        "\n",
        "  # Test accuracy\n",
        "  predictions = model_mlp(torch.Tensor(X_test).to(device))\n",
        "  predictions[predictions >= 0.5]  = 1\n",
        "  predictions[predictions <  0.5]  = 0\n",
        "  print_results(y_test , predictions.cpu(), 'MLP_torch_test, ' + current_options)\n",
        "\n",
        "#print the model\n",
        "print(model_mlp)"
      ],
      "metadata": {
        "id": "mjhGuKclSinl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN Network for Classification**"
      ],
      "metadata": {
        "id": "YuIuZSBd2y85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchvision.transforms import Lambda\n",
        "from torch.nn.functional import softmax , relu, tanh\n",
        "from torch.nn.functional import sigmoid\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dropout_rate=0.5):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(num_filters * len(filter_sizes), 64)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.embedding(inputs).permute(0, 2, 1)  # Permute to (batch_size, embedding_dim, sequence_length)\n",
        "        conv_outputs = [conv(x) for conv in self.conv_layers]\n",
        "        pooled_outputs = [torch.max(conv_output, dim=2)[0] for conv_output in conv_outputs]\n",
        "        concatenated = torch.cat(pooled_outputs, dim=1)\n",
        "        concatenated = self.dropout(concatenated)\n",
        "        output1      = self.fc1(concatenated)\n",
        "        output1      = tanh(output1)\n",
        "        output = self.fc(output1)\n",
        "        return softmax(output, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "#deter mine maximum sequence length\n",
        "max_seq_length = 64\n",
        "\n",
        "#Preprocess data\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "counter = Counter()\n",
        "for line in all_df['sentence']:\n",
        "    counter.update(tokenizer(line))\n",
        "vocab = build_vocab_from_iterator([tokenizer(line) for line in all_df['sentence']], specials=['<unk>', '<pad>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "label_encoder = LabelEncoder()\n",
        "x_data = []\n",
        "y_data = label_encoder.fit_transform(all_df['label'])\n",
        "for line in all_df['sentence']:\n",
        "    seq_line = torch.tensor([vocab[token] for token in tokenizer(line)])\n",
        "    if len(seq_line > max_seq_length ): #truncate longer sequences\n",
        "      seq_line = seq_line[0:max_seq_length]\n",
        "    x_data.append(seq_line)\n",
        "\n",
        "# Pad sequences and convert to tensors\n",
        "x_data = nn.utils.rnn.pad_sequence(x_data, batch_first=True).to(device)\n",
        "y_data = torch.tensor(y_data).to(device)\n",
        "\n",
        "# Split data into train/validation sets\n",
        "x_data_train = x_data[0:len(y_train_val),:] ; y_data_train = y_data[0:len(y_train_val)]\n",
        "x_data_val = x_data[len(y_train_val):,:] ; y_data_val = y_data[len(y_train_val):]\n",
        "\n",
        "# Example: Instantiate the TextCNN model\n",
        "embedding_dim = 100\n",
        "num_filters = 32\n",
        "filter_sizes = [3, 5, 7]\n",
        "num_classes = 2\n",
        "dropout_rate = 0.5\n",
        "\n",
        "model_cnn = TextCNN(vocab_size=len(vocab),\n",
        "                embedding_dim=embedding_dim,\n",
        "                num_filters=num_filters,\n",
        "                filter_sizes=filter_sizes,\n",
        "                num_classes=num_classes,\n",
        "                dropout_rate=dropout_rate).to(device)\n",
        "\n",
        "#Define loss function and optimizer\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = optim.Adam(model_cnn.parameters())\n",
        "\n",
        "# Example: Train the model\n",
        "model_cnn.to(device)\n",
        "batch_size = 64\n",
        "epochs = 250\n",
        "model_cnn.train()\n",
        "for epoch in range(epochs):\n",
        "    acc_loss = 0\n",
        "    for i in range(0, len(x_data_train), batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        batch_x, batch_y = x_data_train[i:i+batch_size,:], y_data_train[i:i+batch_size]\n",
        "        outputs = model_cnn(batch_x)\n",
        "        #print(outputs[:,1] , batch_y)\n",
        "        loss = criterion(batch_y.float(), outputs[:,1]) +  criterion(torch.add(-batch_y.float(),1), outputs[:,0])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        acc_loss += loss.item()\n",
        "\n",
        "    #print accumumlated epoch loss\n",
        "    print(f'Epoch {epoch+1}/{epochs}\\tLoss: {acc_loss:.4f}')\n",
        "\n",
        "# Example: Evaluate the model on validation data\n",
        "model_cnn.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model_cnn(x_data_val)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_data_val).sum().item()\n",
        "    accuracy = correct / len(y_data_val)\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "    print_results(y_data_val.cpu(),predicted.cpu() , f'Text CNN, {max_seq_length}')"
      ],
      "metadata": {
        "id": "YKeKBAFafCn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BiLSTM for Classification**"
      ],
      "metadata": {
        "id": "QD-jzbbi28Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Define BiLSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_dim, dropout_rate):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,dropout=dropout_rate, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds      = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        output      = self.fc(lstm_out[:,-1,:])\n",
        "        return output\n",
        "\n",
        "#Convert data to sequences of equal length\n",
        "\n",
        "#determine maximum sequence length\n",
        "max_seq_length = 64\n",
        "\n",
        "#Preprocess data\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator([tokenizer(line) for line in all_df['sentence']], specials=['<unk>', '<pad>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "label_encoder = LabelEncoder()\n",
        "x_data = []\n",
        "y_data = label_encoder.fit_transform(all_df['label'])\n",
        "for line in all_df['sentence']:\n",
        "    seq_line = torch.tensor([vocab[token] for token in tokenizer(line)])\n",
        "    if len(seq_line > max_seq_length ): #truncate longer sequences\n",
        "      seq_line = seq_line[0:max_seq_length]\n",
        "    x_data.append(seq_line)\n",
        "\n",
        "# Pad sequences and convert to tensors\n",
        "x_data = nn.utils.rnn.pad_sequence(x_data, batch_first=True).to(device)\n",
        "y_data = torch.tensor(y_data).to(device)\n",
        "\n",
        "# Split data into train/validation sets\n",
        "x_data_train = x_data[0:len(y_train_val),:]; y_data_train  = y_data[0:len(y_train_val)]\n",
        "x_data_val   = x_data[len(y_train_val):,:];  y_data_val    = y_data[len(y_train_val):]\n",
        "\n",
        "# Define model parameters\n",
        "input_dim = x_data.shape[1]\n",
        "embedding_dim = 100\n",
        "hidden_dim = 50\n",
        "output_dim = 2  # Assuming binary classification\n",
        "dropout = 0.5\n",
        "\n",
        "\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model_lstm = BiLSTM(vocab_size = len(vocab),\n",
        "               embedding_dim=embedding_dim,\n",
        "               hidden_dim=hidden_dim,\n",
        "               output_dim=2,\n",
        "               dropout_rate=dropout)\n",
        "\n",
        "model_lstm.to(device)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = optim.Adam(model_lstm.parameters())\n",
        "\n",
        "# Example: Train the model\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "model_lstm.train()\n",
        "for epoch in range(epochs):\n",
        "    acc_loss = 0\n",
        "    for i in range(0, len(x_data_train), batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        batch_x, batch_y = x_data_train[i:i+batch_size,:], y_data_train[i:i+batch_size]\n",
        "        outputs = model_lstm(batch_x)\n",
        "        loss = criterion(batch_y.float(), outputs[:,1]) +  criterion(torch.add(-batch_y.float(),1), outputs[:,0])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        acc_loss += loss.item()\n",
        "        #print(loss.item())\n",
        "\n",
        "    #print accumumlated epoch loss\n",
        "    print(f'Epoch {epoch+1}/{epochs}\\tLoss: {acc_loss:.4f}')\n",
        "\n",
        "# Example: Evaluate the model on validation data\n",
        "model_lstm.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model_lstm(x_data_val)\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_data_val).sum().item()\n",
        "    accuracy = correct / len(y_data_val)\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "    print_results(y_data_val.cpu(),predicted.cpu() , f'LSTM-2layers, {max_seq_length}' )\n",
        "    print_per_class_results(y_data_val.cpu(),predicted.cpu() )"
      ],
      "metadata": {
        "id": "-lQqDBXfqYP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70814c2a-2aef-4fed-97a5-abd1788c9511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8932\n",
            "LSTM-2layers, 64,\tAccuracy=0.89,\tC0: Pr=0.97, Re=0.91, F1=0.94,\tC1: Pr=0.49, Re=0.76, F1=0.60\n",
            "something went wrong cannot insert LSTM-2layers, 64, already exists\n",
            ",\t class=0\tAccuracy=0.89,\t Precision=0.97,\tRecall=0.91\tF1-score=0.94\n",
            ",\t class=1\tAccuracy=0.89,\t Precision=0.49,\tRecall=0.76\tF1-score=0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Visualization**"
      ],
      "metadata": {
        "id": "i0QKVhg8TcD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "0kz1vhKqaFpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "make_dot(model_lstm(x_data_train), params=dict(list(model_lstm.named_parameters()))).render(\"./graphs/graph_lstm_torchviz\", format=\"png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNhkdQ8KaCsk",
        "outputId": "740626d1-8bcf-4187-ab83-b2a13195380a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'graphs\\\\graph_lstm_torchviz.png'"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir ./graphs\n",
        "x_temp_mlp =torch.Tensor(X_train_val[0:10,:]).to(device)\n",
        "make_dot(model_mlp(x_temp_mlp), params=dict(list(model_lstm.named_parameters()))).render(\"./graphs/graph_model_architecture_mlp\", format=\"png\")\n",
        "make_dot(model_cnn(batch_x), params=dict(list(model_cnn.named_parameters()))).render(\"./graphs/graph_model_architecture_cnn\", format=\"png\")\n",
        "\n",
        "#create_graph(model_cnn , './graphs/graph_model_architecture_cnn' , 'png')\n",
        "#create_graph(model_lstm , './graphs/graph_model_architecture_lstm' , 'png')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLQX0BMTQekP",
        "outputId": "9d2bc861-bbed-4103-8fb5-7487d5590f3d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'graphs\\\\graph_model_architecture_cnn.png'"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_kFp0kPW6a6",
        "outputId": "bb6d21d6-1be1-4d40-b7f9-44c54f72205c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in c:\\python\\python38\\lib\\site-packages (0.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from graphviz import Digraph  # Install graphviz with pip install graphviz\n",
        "\n",
        "# Define a function to create a node for a layer\n",
        "def create_node(layer_name, layer_type):\n",
        "    return f'{layer_name} ({layer_type})'\n",
        "\n",
        "def create_graph(model, fname , fmt=\"png\"):\n",
        "  # Create a directed graph object\n",
        "  dot = Digraph()\n",
        "  # Iterate through your model layers and add nodes and edges\n",
        "  in_layer = None  # Keep track of the previous layer\n",
        "  for i, layer in enumerate(model.modules()):\n",
        "      if isinstance(layer, torch.nn.Module):\n",
        "          layer_name = f'Layer {i+1}'\n",
        "          layer_type = str(layer).split('(')[0]  # Extract layer type\n",
        "\n",
        "          dot.node(layer_name, create_node(layer_name, layer_type))\n",
        "\n",
        "          if in_layer is not None:\n",
        "              dot.edge(in_layer, layer_name)  # Add edge from previous layer to current\n",
        "\n",
        "          in_layer = layer_name\n",
        "\n",
        "  # Render the graph to a file\n",
        "  dot.render(filename=fname+\".\"+fmt, format=fmt)\n",
        "\n",
        "#plot networks\n",
        "import os\n",
        "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\n"
      ],
      "metadata": {
        "id": "JI23Qtwi-9Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./graphs\n",
        "create_graph(model_mlp , './graphs/graph_model_architecture_mlp' , 'png')\n",
        "create_graph(model_cnn , './graphs/graph_model_architecture_cnn' , 'png')\n",
        "create_graph(model_lstm , './graphs/graph_model_architecture_lstm' , 'png')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "purZg8beYlsd",
        "outputId": "2880aeeb-3df1-43d9-a165-c1f7944c5b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The syntax of the command is incorrect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JqGmZ8wcsHq",
        "outputId": "522a75b0-7e23-4435-957b-2b168109bd72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextCNN(\n",
            "  (embedding): Embedding(9447, 100)\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): Conv1d(100, 128, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(100, 128, kernel_size=(5,), stride=(1,))\n",
            "    (2): Conv1d(100, 128, kernel_size=(7,), stride=(1,))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=384, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mixture of Experts**"
      ],
      "metadata": {
        "id": "2pKLzhsFlDWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Instantiate base classifiers\n",
        "rf_classifier = RandomForestClassifier()\n",
        "gb_classifier = GradientBoostingClassifier()\n",
        "lr_classifier = LogisticRegression()\n",
        "mlp_classifier = MLPClassifier()\n",
        "\n",
        "# Train base classifiers on your imbalanced dataset\n",
        "rf_classifier.fit(X_train_val, y_train_val)\n",
        "gb_classifier.fit(X_train_val, y_train_val)\n",
        "lr_classifier.fit(X_train_val, y_train_val)\n",
        "mlp_classifier.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Make predictions on the test set\n",
        "rf_preds = rf_classifier.predict(X_test)\n",
        "gb_preds = gb_classifier.predict(X_test)\n",
        "lr_preds = lr_classifier.predict(X_test)\n",
        "mlp_preds = mlp_classifier.predict(X_test)\n",
        "\n",
        "# Combine predictions using weighted voting\n",
        "ensemble_preds = (0.25 * rf_preds + 0.25 * gb_preds + 0.25 * lr_preds + 0.25 * mlp_preds)\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "y_pred[ensemble_preds >= 0.5] =1\n",
        "# Evaluate the ensemble\n",
        "print(classification_report(y_test, y_pred))\n",
        "print_results(y_test, y_pred, 'MoE, Eq. Weight, ' + current_options )\n",
        "\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_preds_rf  = rf_classifier.predict(X_train_val).reshape(-1,1)\n",
        "y_preds_gb  = gb_classifier.predict(X_train_val).reshape(-1,1)\n",
        "y_preds_lr  = lr_classifier.predict(X_train_val).reshape(-1,1)\n",
        "y_preds_mlp = mlp_classifier.predict(X_train_val).reshape(-1,1)\n",
        "y_pred_train_val = np.concatenate((y_preds_rf,y_preds_gb, y_preds_lr, y_preds_mlp ), axis=1)\n",
        "\n",
        "# Instantiate base classifiers\n",
        "rf_combiner = RandomForestClassifier()\n",
        "gb_combiner = GradientBoostingClassifier()\n",
        "lr_combiner = LogisticRegression()\n",
        "mlp_combiner = MLPClassifier()\n",
        "\n",
        "rf_combiner.fit(y_pred_train_val, y_train_val)\n",
        "gb_combiner.fit(y_pred_train_val, y_train_val)\n",
        "lr_combiner.fit(y_pred_train_val, y_train_val)\n",
        "mlp_combiner.fit(y_pred_train_val, y_train_val)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_test_rf  = rf_classifier.predict(X_test).reshape(-1,1)\n",
        "y_pred_test_gb  = gb_classifier.predict(X_test).reshape(-1,1)\n",
        "y_pred_test_lr  = lr_classifier.predict(X_test).reshape(-1,1)\n",
        "y_pred_test_mlp = mlp_classifier.predict(X_test).reshape(-1,1)\n",
        "y_pred_test = np.concatenate((y_pred_test_rf, y_pred_test_gb, y_pred_test_lr, y_pred_test_mlp ), axis=1)\n",
        "\n",
        "rfc_preds = rf_combiner.predict(y_pred_test)\n",
        "gbc_preds = gb_combiner.predict(y_pred_test)\n",
        "lrc_preds = lr_combiner.predict(y_pred_test)\n",
        "mlpc_preds = mlp_combiner.predict(y_pred_test)\n",
        "\n",
        "ensemblec_preds = (0.25 * rfc_preds + 0.25 * gbc_preds + 0.25 * lrc_preds + 0.25 * mlpc_preds)\n",
        "\n",
        "# Evaluate the ensemble\n",
        "print_results(y_test, prob2label(rfc_preds),  'MoE, rf-cmb, ' + current_options )\n",
        "print_results(y_test, prob2label(gbc_preds),  'MoE, gb-cmb, ' + current_options )\n",
        "print_results(y_test, prob2label(lrc_preds),  'MoE, lr-cmb, ' + current_options )\n",
        "print_results(y_test, prob2label(mlpc_preds), 'MoE, mlpc-cmb, ' + current_options )\n",
        "print_results(y_test, prob2label(ensemblec_preds), 'MoE, all-cmb, ' + current_options )\n",
        "\n"
      ],
      "metadata": {
        "id": "g-Jmzi2glCyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,prob2label( mlpc_preds)))"
      ],
      "metadata": {
        "id": "dWjP4VNjw07j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balanced Classification**\n",
        "\n",
        "This experiments trains an ensemble of random forests on the balanced subsets"
      ],
      "metadata": {
        "id": "vS--U_kAR4xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Separate instances for class 1\n",
        "class_1_instances = X_train_val[y_train_val == 1,:]\n",
        "class_0_instances = X_train_val[y_train_val == 0,:]\n",
        "num_classifiers = 101\n",
        "classifiers = []\n",
        "number_of_samples = class_1_instances.shape[0]\n",
        "# Build an ensemble of classifiers (Random Forests in this example)\n",
        "\n",
        "for ks in range(1,num_classifiers+1):\n",
        "  # Randomly sample 2000 instances from class 0\n",
        "  indices = np.random.choice(class_0_instances.shape[0], number_of_samples, replace=True)\n",
        "  sampled_class_0_instances = class_0_instances[indices,:]\n",
        "\n",
        "  # Combine instances for class 1 and sampled instances from class 0\n",
        "  balanced_X = np.concatenate([class_1_instances, sampled_class_0_instances])\n",
        "  balanced_y = np.concatenate([np.ones(class_1_instances.shape[0]), np.zeros(sampled_class_0_instances.shape[0])])\n",
        "\n",
        "  #classifier = RandomForestClassifier(n_estimators=101, random_state=100)\n",
        "  #classifier = sklearn.linear_model.LogisticRegression(random_state=42)\n",
        "  classifier = MLPClassifier(random_state=42)\n",
        "\n",
        "  classifier.fit(balanced_X, balanced_y)\n",
        "  classifiers.append(classifier)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  print_results(y_test , y_pred,  f'ensemble{ks} ' + current_options)\n",
        "\n",
        "# Make predictions on the test set using each classifier\n",
        "predictions = [classifier.predict(X_test) for classifier in classifiers]\n",
        "\n",
        "# Take a majority vote to get the final ensemble prediction\n",
        "ensemble_predictions = np.mean(predictions, axis=0) > 0.5\n",
        "\n",
        "# Evaluate the ensemble performance\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
        "print(f'Ensemble Accuracy: {ensemble_accuracy}')\n",
        "print_results(y_test , ensemble_predictions,  'ensemble_total ' + current_options)\n"
      ],
      "metadata": {
        "id": "R-gOUwRaR5VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9o02SgiTxpA"
      },
      "source": [
        "**Word2vec Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this set of experiments we build and test several well-known\n",
        "word2vec and doc2vec models"
      ],
      "metadata": {
        "id": "zbCmHPIRx1Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#basic functions\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "#Get embedding of a document as the mean of embeddings of its words\n",
        "def get_doc2vec(model ,doc ):\n",
        "  tokens = doc.lower().split()\n",
        "  vec = np.zeros(model.vector_size)\n",
        "  num_tokens = 0\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      vec += model.get_vector(token)\n",
        "      num_tokens += 1\n",
        "    except:\n",
        "      token = 'unk' #print(f\"{token} not found in vocab\")\n",
        "\n",
        "  if num_tokens > 0:\n",
        "    vec /= num_tokens\n",
        "  return vec.reshape(1,-1)\n",
        "\n",
        "#Generate document vectors for all of the sentences:\n",
        "def get_corpus_embeddings(model , documents):\n",
        "  X = get_doc2vec(model, documents[0] )\n",
        "  for i in range(1,len(documents)):\n",
        "    X = np.append(X, get_doc2vec(model, documents[i]).reshape(1,-1), axis=0)\n",
        "\n",
        "  return X\n",
        ""
      ],
      "metadata": {
        "id": "iNIdUBK9qcqJ"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#gensim doc2vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(all_df['sentence'].tolist())]\n",
        "model = Doc2Vec(documents, vector_size=300, window=4, min_count=1, workers=100, dbow_words =1, epochs=100)\n",
        "\n",
        "#Persist a model to disk:\n",
        "fname = get_tmpfile(\"gensim_doc2vec_model\")\n",
        "model.save(fname)\n",
        "model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n",
        "\n",
        "#Generate document vectors for all of the sentences:\n",
        "X_d2v = get_corpus_embeddings (model.wv, all_df['sentence'].tolist())\n",
        "\n",
        "test_basic_models(X_d2v[:y_train_val.shape[0],:], y_train_val,\n",
        "                  X_d2v[y_train_val.shape[0]:,:], y_test, 'gensim_doc2vec, ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tSJ88rcqlFH",
        "outputId": "24f66561-74a2-4801-9e9e-a19075cdc7cc"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, gensim_doc2vec, ,\tAccuracy=0.88,\tC0: Pr=0.95, Re=0.92, F1=0.93,\tC1: Pr=0.45, Re=0.56, F1=0.50\n",
            "basic Linear Discrimination Analysis, gensim_doc2vec, ,\tAccuracy=0.89,\tC0: Pr=0.94, Re=0.94, F1=0.94,\tC1: Pr=0.47, Re=0.45, F1=0.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Logistic Regression, gensim_doc2vec, ,\tAccuracy=0.89,\tC0: Pr=0.93, Re=0.94, F1=0.94,\tC1: Pr=0.47, Re=0.44, F1=0.46\n",
            "basic Support Vector Machine-L, gensim_doc2vec, ,\tAccuracy=0.90,\tC0: Pr=0.94, Re=0.94, F1=0.94,\tC1: Pr=0.50, Re=0.49, F1=0.50\n",
            "basic Support Vector Machine-R, gensim_doc2vec, ,\tAccuracy=0.91,\tC0: Pr=0.95, Re=0.96, F1=0.95,\tC1: Pr=0.60, Re=0.54, F1=0.57\n",
            "basic Support Vector Machine-S, gensim_doc2vec, ,\tAccuracy=0.76,\tC0: Pr=0.91, Re=0.81, F1=0.86,\tC1: Pr=0.15, Re=0.30, F1=0.20\n",
            "basic Support Vector Machine-WL, gensim_doc2vec, ,\tAccuracy=0.63,\tC0: Pr=0.98, Re=0.60, F1=0.75,\tC1: Pr=0.21, Re=0.90, F1=0.34\n",
            "basic Support Vector Machine-WR, gensim_doc2vec, ,\tAccuracy=0.74,\tC0: Pr=0.99, Re=0.71, F1=0.83,\tC1: Pr=0.28, Re=0.94, F1=0.43\n",
            "basic Support Vector Machine-WS, gensim_doc2vec, ,\tAccuracy=0.50,\tC0: Pr=0.95, Re=0.47, F1=0.63,\tC1: Pr=0.15, Re=0.80, F1=0.25\n",
            "basic Decision Tree classifier, gensim_doc2vec, ,\tAccuracy=0.78,\tC0: Pr=0.93, Re=0.82, F1=0.87,\tC1: Pr=0.24, Re=0.49, F1=0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gensim FastText\n",
        "from gensim.models import FastText\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "documents = all_df['sentence'].tolist()\n",
        "docs=[]\n",
        "for doc in documents:\n",
        "  #print(doc.split())\n",
        "  docs += [doc.split()]\n",
        "\n",
        "model_fasttext = FastText(vector_size=40, window=3, min_count=1, sentences=docs, epochs=100)\n",
        "\n",
        "fname = get_tmpfile(\"suggestion_fasttext.model\")\n",
        "model_fasttext.save(fname)\n",
        "model_fasttext = FastText.load(fname)\n",
        "\n",
        "#Generate document vectors for all of the sentences:\n",
        "X_ft = get_corpus_embeddings (model_fasttext.wv, all_df['sentence'].tolist())\n",
        "\n",
        "test_basic_models(X_ft[:y_train_val.shape[0],:], y_train_val,\n",
        "                  X_ft[y_train_val.shape[0]:,:], y_test, 'gensim-fasttext-words')\n"
      ],
      "metadata": {
        "id": "RzVwSiFk2Qtj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3854f3-d6a9-493b-d881-5db3ae132750"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, gensim-fasttext-words,\tAccuracy=0.87,\tC0: Pr=0.94, Re=0.91, F1=0.92,\tC1: Pr=0.39, Re=0.49, F1=0.43\n",
            "basic Linear Discrimination Analysis, gensim-fasttext-words,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.94, F1=0.93,\tC1: Pr=0.39, Re=0.33, F1=0.36\n",
            "basic Logistic Regression, gensim-fasttext-words,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.94, F1=0.93,\tC1: Pr=0.40, Re=0.33, F1=0.36\n",
            "basic Support Vector Machine-L, gensim-fasttext-words,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.95, F1=0.94,\tC1: Pr=0.41, Re=0.30, F1=0.35\n",
            "basic Support Vector Machine-R, gensim-fasttext-words,\tAccuracy=0.90,\tC0: Pr=0.94, Re=0.96, F1=0.95,\tC1: Pr=0.54, Re=0.45, F1=0.49\n",
            "basic Support Vector Machine-S, gensim-fasttext-words,\tAccuracy=0.77,\tC0: Pr=0.92, Re=0.81, F1=0.86,\tC1: Pr=0.20, Re=0.39, F1=0.26\n",
            "basic Support Vector Machine-WL, gensim-fasttext-words,\tAccuracy=0.48,\tC0: Pr=0.98, Re=0.42, F1=0.59,\tC1: Pr=0.16, Re=0.94, F1=0.27\n",
            "basic Support Vector Machine-WR, gensim-fasttext-words,\tAccuracy=0.62,\tC0: Pr=0.99, Re=0.58, F1=0.73,\tC1: Pr=0.21, Re=0.93, F1=0.34\n",
            "basic Support Vector Machine-WS, gensim-fasttext-words,\tAccuracy=0.45,\tC0: Pr=0.95, Re=0.41, F1=0.58,\tC1: Pr=0.14, Re=0.82, F1=0.24\n",
            "basic Decision Tree classifier, gensim-fasttext-words,\tAccuracy=0.79,\tC0: Pr=0.93, Re=0.83, F1=0.88,\tC1: Pr=0.25, Re=0.49, F1=0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test pretrained word2vec models:\n",
        "import gensim.downloader\n",
        "#models =['fasttext-wiki-news-subwords-300',\n",
        "models  =['conceptnet-numberbatch-17-06-300',\n",
        "         'word2vec-ruscorpora-300',\n",
        "         'word2vec-google-news-300',\n",
        "         'glove-wiki-gigaword-50',\n",
        "         'glove-wiki-gigaword-100',\n",
        "         'glove-wiki-gigaword-200',\n",
        "         'glove-wiki-gigaword-300',\n",
        "         'glove-twitter-25',\n",
        "         'glove-twitter-50',\n",
        "         'glove-twitter-100',\n",
        "         'glove-twitter-200',\n",
        "         '__testing_word2vec-matrix-synopsis',\n",
        "         ]\n",
        "\n",
        "#Play with pretrained word2vec embeddings\n",
        "for model_name in models:\n",
        "  print (model_name)\n",
        "  model_fname = model_name + \".model\"\n",
        "  model_pretrained = gensim.downloader.load(model_name)\n",
        "\n",
        "  #Generate document vectors for all of the sentences:\n",
        "  X_gl = get_corpus_embeddings (model_pretrained, all_df['sentence'].tolist())\n",
        "  try:\n",
        "    test_basic_models(X_gl[:y_train_val.shape[0],:], y_train_val,\n",
        "                    X_gl[y_train_val.shape[0]:,:], y_test, model_name)\n",
        "  except Exception as E:\n",
        "    print (f'Cannot eval word2vec {model_name}. Description {E}')\n",
        "\n"
      ],
      "metadata": {
        "id": "96kq9mftvxvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65a72e5-cc6c-47f4-c3f2-e3a66ec1d5be"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conceptnet-numberbatch-17-06-300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, conceptnet-numberbatch-17-06-300,\tAccuracy=0.90,\tC0: Pr=0.90, Re=1.00, F1=0.94,\tC1: Pr=0.00, Re=0.00, F1=0.00\n",
            "something went wrong cannot insert basic K-Nearest Neighbors, conceptnet-numberbatch-17-06-300, already exists\n",
            "Cannot eval word2vec conceptnet-numberbatch-17-06-300. Description Internal work array size computation failed: -10\n",
            "word2vec-ruscorpora-300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, word2vec-ruscorpora-300,\tAccuracy=0.90,\tC0: Pr=0.90, Re=1.00, F1=0.94,\tC1: Pr=0.00, Re=0.00, F1=0.00\n",
            "Cannot eval word2vec word2vec-ruscorpora-300. Description Internal work array size computation failed: -10\n",
            "word2vec-google-news-300\n",
            "basic K-Nearest Neighbors, word2vec-google-news-300,\tAccuracy=0.85,\tC0: Pr=0.93, Re=0.90, F1=0.92,\tC1: Pr=0.32, Re=0.38, F1=0.35\n",
            "basic Linear Discrimination Analysis, word2vec-google-news-300,\tAccuracy=0.87,\tC0: Pr=0.93, Re=0.93, F1=0.93,\tC1: Pr=0.39, Re=0.37, F1=0.38\n",
            "basic Logistic Regression, word2vec-google-news-300,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.95, F1=0.93,\tC1: Pr=0.40, Re=0.31, F1=0.35\n",
            "basic Support Vector Machine-L, word2vec-google-news-300,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.95, F1=0.93,\tC1: Pr=0.41, Re=0.32, F1=0.36\n",
            "basic Support Vector Machine-R, word2vec-google-news-300,\tAccuracy=0.89,\tC0: Pr=0.94, Re=0.94, F1=0.94,\tC1: Pr=0.49, Re=0.53, F1=0.51\n",
            "basic Support Vector Machine-S, word2vec-google-news-300,\tAccuracy=0.77,\tC0: Pr=0.91, Re=0.82, F1=0.87,\tC1: Pr=0.17, Re=0.32, F1=0.23\n",
            "basic Support Vector Machine-WL, word2vec-google-news-300,\tAccuracy=0.54,\tC0: Pr=0.99, Re=0.50, F1=0.66,\tC1: Pr=0.18, Re=0.94, F1=0.30\n",
            "basic Support Vector Machine-WR, word2vec-google-news-300,\tAccuracy=0.71,\tC0: Pr=0.98, Re=0.69, F1=0.81,\tC1: Pr=0.25, Re=0.91, F1=0.40\n",
            "basic Support Vector Machine-WS, word2vec-google-news-300,\tAccuracy=0.48,\tC0: Pr=0.96, Re=0.43, F1=0.60,\tC1: Pr=0.15, Re=0.86, F1=0.26\n",
            "basic Decision Tree classifier, word2vec-google-news-300,\tAccuracy=0.74,\tC0: Pr=0.93, Re=0.77, F1=0.84,\tC1: Pr=0.20, Re=0.51, F1=0.29\n",
            "glove-wiki-gigaword-50\n",
            "basic K-Nearest Neighbors, glove-wiki-gigaword-50,\tAccuracy=0.84,\tC0: Pr=0.92, Re=0.91, F1=0.91,\tC1: Pr=0.27, Re=0.29, F1=0.28\n",
            "basic Linear Discrimination Analysis, glove-wiki-gigaword-50,\tAccuracy=0.86,\tC0: Pr=0.91, Re=0.94, F1=0.92,\tC1: Pr=0.24, Re=0.17, F1=0.20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Logistic Regression, glove-wiki-gigaword-50,\tAccuracy=0.86,\tC0: Pr=0.91, Re=0.94, F1=0.92,\tC1: Pr=0.26, Re=0.18, F1=0.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-L, glove-wiki-gigaword-50,\tAccuracy=0.90,\tC0: Pr=0.90, Re=1.00, F1=0.94,\tC1: Pr=0.00, Re=0.00, F1=0.00\n",
            "basic Support Vector Machine-R, glove-wiki-gigaword-50,\tAccuracy=0.89,\tC0: Pr=0.90, Re=1.00, F1=0.94,\tC1: Pr=0.00, Re=0.00, F1=0.00\n",
            "basic Support Vector Machine-S, glove-wiki-gigaword-50,\tAccuracy=0.72,\tC0: Pr=0.90, Re=0.78, F1=0.83,\tC1: Pr=0.12, Re=0.25, F1=0.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WL, glove-wiki-gigaword-50,\tAccuracy=0.10,\tC0: Pr=0.00, Re=0.00, F1=0.00,\tC1: Pr=0.10, Re=1.00, F1=0.19\n",
            "basic Support Vector Machine-WR, glove-wiki-gigaword-50,\tAccuracy=0.22,\tC0: Pr=1.00, Re=0.13, F1=0.23,\tC1: Pr=0.12, Re=1.00, F1=0.21\n",
            "basic Support Vector Machine-WS, glove-wiki-gigaword-50,\tAccuracy=0.38,\tC0: Pr=0.95, Re=0.32, F1=0.48,\tC1: Pr=0.13, Re=0.86, F1=0.22\n",
            "basic Decision Tree classifier, glove-wiki-gigaword-50,\tAccuracy=0.72,\tC0: Pr=0.93, Re=0.74, F1=0.82,\tC1: Pr=0.18, Re=0.49, F1=0.27\n",
            "glove-wiki-gigaword-100\n",
            "basic K-Nearest Neighbors, glove-wiki-gigaword-100,\tAccuracy=0.85,\tC0: Pr=0.92, Re=0.91, F1=0.91,\tC1: Pr=0.29, Re=0.32, F1=0.31\n",
            "basic Linear Discrimination Analysis, glove-wiki-gigaword-100,\tAccuracy=0.88,\tC0: Pr=0.91, Re=0.95, F1=0.93,\tC1: Pr=0.35, Re=0.23, F1=0.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Logistic Regression, glove-wiki-gigaword-100,\tAccuracy=0.87,\tC0: Pr=0.91, Re=0.94, F1=0.93,\tC1: Pr=0.33, Re=0.24, F1=0.28\n",
            "basic Support Vector Machine-L, glove-wiki-gigaword-100,\tAccuracy=0.89,\tC0: Pr=0.91, Re=0.97, F1=0.94,\tC1: Pr=0.39, Re=0.17, F1=0.24\n",
            "basic Support Vector Machine-R, glove-wiki-gigaword-100,\tAccuracy=0.89,\tC0: Pr=0.91, Re=0.98, F1=0.94,\tC1: Pr=0.48, Re=0.17, F1=0.25\n",
            "basic Support Vector Machine-S, glove-wiki-gigaword-100,\tAccuracy=0.73,\tC0: Pr=0.90, Re=0.79, F1=0.84,\tC1: Pr=0.13, Re=0.28, F1=0.18\n",
            "basic Support Vector Machine-WL, glove-wiki-gigaword-100,\tAccuracy=0.36,\tC0: Pr=1.00, Re=0.29, F1=0.45,\tC1: Pr=0.14, Re=0.99, F1=0.24\n",
            "basic Support Vector Machine-WR, glove-wiki-gigaword-100,\tAccuracy=0.40,\tC0: Pr=1.00, Re=0.33, F1=0.49,\tC1: Pr=0.15, Re=0.99, F1=0.26\n",
            "basic Support Vector Machine-WS, glove-wiki-gigaword-100,\tAccuracy=0.39,\tC0: Pr=0.94, Re=0.34, F1=0.50,\tC1: Pr=0.13, Re=0.83, F1=0.22\n",
            "basic Decision Tree classifier, glove-wiki-gigaword-100,\tAccuracy=0.75,\tC0: Pr=0.94, Re=0.77, F1=0.85,\tC1: Pr=0.23, Re=0.60, F1=0.34\n",
            "glove-wiki-gigaword-200\n",
            "basic K-Nearest Neighbors, glove-wiki-gigaword-200,\tAccuracy=0.82,\tC0: Pr=0.92, Re=0.88, F1=0.90,\tC1: Pr=0.23, Re=0.30, F1=0.26\n",
            "basic Linear Discrimination Analysis, glove-wiki-gigaword-200,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.95, F1=0.93,\tC1: Pr=0.40, Re=0.31, F1=0.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Logistic Regression, glove-wiki-gigaword-200,\tAccuracy=0.87,\tC0: Pr=0.92, Re=0.94, F1=0.93,\tC1: Pr=0.34, Re=0.26, F1=0.30\n",
            "basic Support Vector Machine-L, glove-wiki-gigaword-200,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.95, F1=0.93,\tC1: Pr=0.39, Re=0.26, F1=0.32\n",
            "basic Support Vector Machine-R, glove-wiki-gigaword-200,\tAccuracy=0.89,\tC0: Pr=0.91, Re=0.97, F1=0.94,\tC1: Pr=0.42, Re=0.22, F1=0.29\n",
            "basic Support Vector Machine-S, glove-wiki-gigaword-200,\tAccuracy=0.75,\tC0: Pr=0.91, Re=0.80, F1=0.85,\tC1: Pr=0.15, Re=0.31, F1=0.21\n",
            "basic Support Vector Machine-WL, glove-wiki-gigaword-200,\tAccuracy=0.43,\tC0: Pr=0.99, Re=0.37, F1=0.54,\tC1: Pr=0.15, Re=0.98, F1=0.27\n",
            "basic Support Vector Machine-WR, glove-wiki-gigaword-200,\tAccuracy=0.48,\tC0: Pr=1.00, Re=0.42, F1=0.59,\tC1: Pr=0.17, Re=0.99, F1=0.28\n",
            "basic Support Vector Machine-WS, glove-wiki-gigaword-200,\tAccuracy=0.43,\tC0: Pr=0.96, Re=0.38, F1=0.54,\tC1: Pr=0.14, Re=0.85, F1=0.24\n",
            "basic Decision Tree classifier, glove-wiki-gigaword-200,\tAccuracy=0.75,\tC0: Pr=0.93, Re=0.78, F1=0.85,\tC1: Pr=0.21, Re=0.52, F1=0.30\n",
            "glove-wiki-gigaword-300\n",
            "basic K-Nearest Neighbors, glove-wiki-gigaword-300,\tAccuracy=0.83,\tC0: Pr=0.92, Re=0.90, F1=0.91,\tC1: Pr=0.26, Re=0.31, F1=0.28\n",
            "basic Linear Discrimination Analysis, glove-wiki-gigaword-300,\tAccuracy=0.87,\tC0: Pr=0.93, Re=0.92, F1=0.93,\tC1: Pr=0.37, Re=0.38, F1=0.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Logistic Regression, glove-wiki-gigaword-300,\tAccuracy=0.87,\tC0: Pr=0.92, Re=0.94, F1=0.93,\tC1: Pr=0.37, Re=0.32, F1=0.34\n",
            "basic Support Vector Machine-L, glove-wiki-gigaword-300,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.94, F1=0.93,\tC1: Pr=0.38, Re=0.31, F1=0.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-R, glove-wiki-gigaword-300,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.95, F1=0.93,\tC1: Pr=0.39, Re=0.28, F1=0.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-S, glove-wiki-gigaword-300,\tAccuracy=0.76,\tC0: Pr=0.91, Re=0.81, F1=0.86,\tC1: Pr=0.17, Re=0.33, F1=0.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WL, glove-wiki-gigaword-300,\tAccuracy=0.51,\tC0: Pr=0.99, Re=0.46, F1=0.63,\tC1: Pr=0.17, Re=0.95, F1=0.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WR, glove-wiki-gigaword-300,\tAccuracy=0.57,\tC0: Pr=0.99, Re=0.53, F1=0.69,\tC1: Pr=0.19, Re=0.97, F1=0.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WS, glove-wiki-gigaword-300,\tAccuracy=0.43,\tC0: Pr=0.94, Re=0.38, F1=0.55,\tC1: Pr=0.13, Re=0.80, F1=0.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Decision Tree classifier, glove-wiki-gigaword-300,\tAccuracy=0.74,\tC0: Pr=0.93, Re=0.76, F1=0.84,\tC1: Pr=0.21, Re=0.54, F1=0.30\n",
            "glove-twitter-25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, glove-twitter-25,\tAccuracy=0.85,\tC0: Pr=0.92, Re=0.91, F1=0.92,\tC1: Pr=0.31, Re=0.33, F1=0.32\n",
            "basic Linear Discrimination Analysis, glove-twitter-25,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.95, F1=0.93,\tC1: Pr=0.37, Re=0.26, F1=0.31\n",
            "basic Logistic Regression, glove-twitter-25,\tAccuracy=0.87,\tC0: Pr=0.92, Re=0.94, F1=0.93,\tC1: Pr=0.35, Re=0.28, F1=0.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n",
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n",
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-L, glove-twitter-25,\tAccuracy=0.90,\tC0: Pr=0.90, Re=1.00, F1=0.94,\tC1: Pr=0.00, Re=0.00, F1=0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-R, glove-twitter-25,\tAccuracy=0.90,\tC0: Pr=0.90, Re=1.00, F1=0.94,\tC1: Pr=0.00, Re=0.00, F1=0.00\n",
            "basic Support Vector Machine-S, glove-twitter-25,\tAccuracy=0.73,\tC0: Pr=0.90, Re=0.79, F1=0.84,\tC1: Pr=0.13, Re=0.26, F1=0.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n",
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WL, glove-twitter-25,\tAccuracy=0.10,\tC0: Pr=0.00, Re=0.00, F1=0.00,\tC1: Pr=0.10, Re=1.00, F1=0.19\n",
            "basic Support Vector Machine-WR, glove-twitter-25,\tAccuracy=0.20,\tC0: Pr=1.00, Re=0.11, F1=0.20,\tC1: Pr=0.12, Re=1.00, F1=0.21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WS, glove-twitter-25,\tAccuracy=0.34,\tC0: Pr=0.94, Re=0.28, F1=0.44,\tC1: Pr=0.12, Re=0.84, F1=0.21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Decision Tree classifier, glove-twitter-25,\tAccuracy=0.77,\tC0: Pr=0.93, Re=0.80, F1=0.86,\tC1: Pr=0.22, Re=0.47, F1=0.30\n",
            "glove-twitter-50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, glove-twitter-50,\tAccuracy=0.85,\tC0: Pr=0.93, Re=0.91, F1=0.92,\tC1: Pr=0.33, Re=0.39, F1=0.36\n",
            "basic Linear Discrimination Analysis, glove-twitter-50,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.95, F1=0.93,\tC1: Pr=0.37, Re=0.26, F1=0.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n",
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Logistic Regression, glove-twitter-50,\tAccuracy=0.88,\tC0: Pr=0.92, Re=0.95, F1=0.93,\tC1: Pr=0.39, Re=0.28, F1=0.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-L, glove-twitter-50,\tAccuracy=0.90,\tC0: Pr=0.90, Re=1.00, F1=0.94,\tC1: Pr=0.50, Re=0.01, F1=0.02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-R, glove-twitter-50,\tAccuracy=0.90,\tC0: Pr=0.90, Re=1.00, F1=0.94,\tC1: Pr=0.50, Re=0.03, F1=0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-S, glove-twitter-50,\tAccuracy=0.74,\tC0: Pr=0.90, Re=0.80, F1=0.85,\tC1: Pr=0.13, Re=0.25, F1=0.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WL, glove-twitter-50,\tAccuracy=0.20,\tC0: Pr=1.00, Re=0.10, F1=0.19,\tC1: Pr=0.12, Re=1.00, F1=0.21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WR, glove-twitter-50,\tAccuracy=0.29,\tC0: Pr=1.00, Re=0.21, F1=0.35,\tC1: Pr=0.13, Re=1.00, F1=0.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WS, glove-twitter-50,\tAccuracy=0.37,\tC0: Pr=0.95, Re=0.31, F1=0.47,\tC1: Pr=0.13, Re=0.86, F1=0.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Decision Tree classifier, glove-twitter-50,\tAccuracy=0.74,\tC0: Pr=0.93, Re=0.77, F1=0.84,\tC1: Pr=0.21, Re=0.51, F1=0.29\n",
            "glove-twitter-100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, glove-twitter-100,\tAccuracy=0.85,\tC0: Pr=0.92, Re=0.91, F1=0.92,\tC1: Pr=0.31, Re=0.34, F1=0.32\n",
            "basic Linear Discrimination Analysis, glove-twitter-100,\tAccuracy=0.89,\tC0: Pr=0.93, Re=0.95, F1=0.94,\tC1: Pr=0.46, Re=0.36, F1=0.40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n",
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Logistic Regression, glove-twitter-100,\tAccuracy=0.89,\tC0: Pr=0.93, Re=0.95, F1=0.94,\tC1: Pr=0.45, Re=0.37, F1=0.41\n",
            "basic Support Vector Machine-L, glove-twitter-100,\tAccuracy=0.89,\tC0: Pr=0.92, Re=0.97, F1=0.94,\tC1: Pr=0.46, Re=0.24, F1=0.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-R, glove-twitter-100,\tAccuracy=0.90,\tC0: Pr=0.92, Re=0.97, F1=0.94,\tC1: Pr=0.52, Re=0.25, F1=0.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-S, glove-twitter-100,\tAccuracy=0.75,\tC0: Pr=0.90, Re=0.80, F1=0.85,\tC1: Pr=0.14, Re=0.28, F1=0.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WL, glove-twitter-100,\tAccuracy=0.41,\tC0: Pr=0.99, Re=0.35, F1=0.52,\tC1: Pr=0.15, Re=0.97, F1=0.26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WR, glove-twitter-100,\tAccuracy=0.44,\tC0: Pr=1.00, Re=0.38, F1=0.55,\tC1: Pr=0.16, Re=0.99, F1=0.27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WS, glove-twitter-100,\tAccuracy=0.37,\tC0: Pr=0.95, Re=0.32, F1=0.47,\tC1: Pr=0.13, Re=0.86, F1=0.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Decision Tree classifier, glove-twitter-100,\tAccuracy=0.76,\tC0: Pr=0.93, Re=0.79, F1=0.85,\tC1: Pr=0.22, Re=0.51, F1=0.31\n",
            "glove-twitter-200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, glove-twitter-200,\tAccuracy=0.86,\tC0: Pr=0.93, Re=0.91, F1=0.92,\tC1: Pr=0.36, Re=0.44, F1=0.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Linear Discrimination Analysis, glove-twitter-200,\tAccuracy=0.88,\tC0: Pr=0.93, Re=0.95, F1=0.94,\tC1: Pr=0.42, Re=0.34, F1=0.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n",
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Logistic Regression, glove-twitter-200,\tAccuracy=0.88,\tC0: Pr=0.93, Re=0.94, F1=0.94,\tC1: Pr=0.43, Re=0.37, F1=0.40\n",
            "basic Support Vector Machine-L, glove-twitter-200,\tAccuracy=0.88,\tC0: Pr=0.93, Re=0.94, F1=0.93,\tC1: Pr=0.41, Re=0.36, F1=0.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-R, glove-twitter-200,\tAccuracy=0.90,\tC0: Pr=0.92, Re=0.97, F1=0.95,\tC1: Pr=0.54, Re=0.31, F1=0.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-S, glove-twitter-200,\tAccuracy=0.75,\tC0: Pr=0.90, Re=0.80, F1=0.85,\tC1: Pr=0.14, Re=0.28, F1=0.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WL, glove-twitter-200,\tAccuracy=0.52,\tC0: Pr=0.99, Re=0.47, F1=0.64,\tC1: Pr=0.17, Re=0.94, F1=0.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WR, glove-twitter-200,\tAccuracy=0.52,\tC0: Pr=0.99, Re=0.47, F1=0.64,\tC1: Pr=0.18, Re=0.97, F1=0.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Support Vector Machine-WS, glove-twitter-200,\tAccuracy=0.40,\tC0: Pr=0.96, Re=0.35, F1=0.51,\tC1: Pr=0.13, Re=0.87, F1=0.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic Decision Tree classifier, glove-twitter-200,\tAccuracy=0.76,\tC0: Pr=0.93, Re=0.79, F1=0.85,\tC1: Pr=0.21, Re=0.48, F1=0.29\n",
            "__testing_word2vec-matrix-synopsis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic K-Nearest Neighbors, __testing_word2vec-matrix-synopsis,\tAccuracy=0.90,\tC0: Pr=0.90, Re=1.00, F1=0.94,\tC1: Pr=0.00, Re=0.00, F1=0.00\n",
            "Cannot eval word2vec __testing_word2vec-matrix-synopsis. Description Internal work array size computation failed: -10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\mmr\\AppData\\Local\\Temp\\ipykernel_18236\\3213305879.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  results_df.insert(len(results_df.columns),description, y_pred)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Longest Common Subsequence**"
      ],
      "metadata": {
        "id": "3fF6EUfhxT42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def longest_common_subsequence(str1, str2):\n",
        "    words1 = str1.split()\n",
        "    words2 = str2.split()\n",
        "\n",
        "    m = len(words1)\n",
        "    n = len(words2)\n",
        "\n",
        "    # Initializing the dp table with zeros\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Building the dp table\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if words1[i - 1] == words2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
        "\n",
        "    # Backtracking to find the longest common subsequence\n",
        "    lcs_length = dp[m][n]\n",
        "    lcs = []\n",
        "    i = m\n",
        "    j = n\n",
        "    while i > 0 and j > 0:\n",
        "        if words1[i - 1] == words2[j - 1]:\n",
        "            lcs.append ( words1[i - 1])\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "            lcs_length -= 1\n",
        "        elif dp[i - 1][j] > dp[i][j - 1]:\n",
        "            i -= 1\n",
        "        else:\n",
        "            j -= 1\n",
        "\n",
        "    lcs.reverse()\n",
        "    return lcs , (len(lcs))/ (0.0001+m) # 0.0001 is denom is there to prevent div by 0\n",
        "# Example usage:\n",
        "str1        = \"roses are red. violets are blue\"\n",
        "str2        = \"the garden is full of roses and violets that are blue \"\n",
        "lcs12 , r12 = longest_common_subsequence(str1, str2)\n",
        "print(f\"Longest Common Subsequence:{r12}: {lcs12} \")\n"
      ],
      "metadata": {
        "id": "cKfUYz7WxTPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords_list(tokens):\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "def remove_stopwords_str(str):\n",
        "  str = re.sub(r'[^A-Za-z0-9]+', ' ', str)\n",
        "  str = re.sub(r'\\W+', ' ', str)\n",
        "  str = re.sub(r'\\s+', ' ', str)\n",
        "\n",
        "  tokens = str.split()\n",
        "  filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
        "  return ' '.join(filtered_tokens)\n",
        "\n",
        "#--------------------\n",
        "#\n",
        "\n",
        "do_lcs = True # switch this flag if you want to run lcs\n",
        "\n",
        "if do_lcs:\n",
        "  train = train_df['sentence'].tolist(); train  = [remove_stopwords_str(s) for s in train ]\n",
        "  test  = test_df['sentence'].tolist();  test   = [remove_stopwords_str(s) for s in test ]\n",
        "  valid = valid_df['sentence'].tolist(); valid  = [remove_stopwords_str(s) for s in valid ]\n",
        "\n",
        "  X = train + valid ;   y = y_train_val\n",
        "  num_train = len(X);   num_test  = len(test)\n",
        "\n",
        "  with open('train_test_lcs03.txt', 'w') as f:\n",
        "    for m in range(num_train):\n",
        "      if y[m] == 1:\n",
        "        for n in range(num_test):\n",
        "          temp, r = longest_common_subsequence (X[m], test[n])\n",
        "          if r >= 0.01:\n",
        "            print(f\"{m}\\t{n}\\t{y[m]}\\t{y_test[n]}\\t{r:0.02f}\\t{len(temp)}\\t{temp}\",file=f)\n"
      ],
      "metadata": {
        "id": "LzLh7vJ0xZpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def wordnet_path_similarity(word1, word2):\n",
        "    synsets1 = wn.synsets(word1)\n",
        "    synsets2 = wn.synsets(word2)\n",
        "    if not synsets1 or not synsets2:\n",
        "        return 0\n",
        "    else:\n",
        "        max_similarity = 0\n",
        "        for synset1 in synsets1:\n",
        "            for synset2 in synsets2:\n",
        "                path_similarity = synset1.path_similarity(synset2)\n",
        "                if path_similarity is not None and path_similarity > max_similarity:\n",
        "                    max_similarity = path_similarity\n",
        "        return max_similarity\n",
        "\n",
        "def dtw_distance(s1, s2, similarity_function=wordnet_path_similarity):\n",
        "    len_s1, len_s2 = len(s1), len(s2)\n",
        "    dtw_matrix = np.zeros((len_s1 + 1, len_s2 + 1))\n",
        "\n",
        "    # Initialize the DTW matrix with infinity\n",
        "    for i in range(len_s1 + 1):\n",
        "        for j in range(len_s2 + 1):\n",
        "            dtw_matrix[i, j] = float('inf')\n",
        "\n",
        "    dtw_matrix[0, 0] = 0\n",
        "\n",
        "    # Calculate DTW matrix\n",
        "    for i in range(1, len_s1 + 1):\n",
        "        for j in range(1, len_s2 + 1):\n",
        "            cost = 1 - similarity_function(s1[i - 1], s2[j - 1])  # Using WordNet similarity as cost\n",
        "            dtw_matrix[i, j] = cost + min(dtw_matrix[i - 1, j], dtw_matrix[i, j - 1], dtw_matrix[i - 1, j - 1])\n",
        "\n",
        "    return dtw_matrix[len_s1, len_s2]\n",
        "\n",
        "def dtw_distance_str(s1, s2):\n",
        "  return dtw_distance(s1.split() , s2.split())\n",
        "\n",
        "# Example lists of strings\n",
        "list1 = ['cat', 'dog', 'fish']\n",
        "list2 = ['cat', 'fish', 'bird']\n",
        "\n",
        "# Calculate DTW similarity\n",
        "dtw_distance_score = dtw_distance(list1, list2)\n",
        "\n",
        "print(\"DTW Similarity:\", dtw_distance_score)\n"
      ],
      "metadata": {
        "id": "aAJ17NQjFayF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------\n",
        "#\n",
        "\n",
        "do_dtw = True # switch this flag if you want to run lcs\n",
        "\n",
        "if do_dtw:\n",
        "  train = train_df['sentence'].tolist(); train  = [remove_stopwords_str(s) for s in train ]\n",
        "  test  = test_df['sentence'].tolist();  test   = [remove_stopwords_str(s) for s in test ]\n",
        "  valid = valid_df['sentence'].tolist(); valid  = [remove_stopwords_str(s) for s in valid ]\n",
        "\n",
        "  X = train + valid ;   y = y_train_val\n",
        "  num_train = len(X);   num_test  = len(test)\n",
        "\n",
        "  with open('train_test_dtw02.txt', 'w') as f2:\n",
        "    for m in range(num_train):\n",
        "      if y[m] == 1:\n",
        "        s1 = X[m].split() ; n1 = len(s1)\n",
        "        for n in range(num_test):\n",
        "          s2 = test[n].split() ; n2 = len(s2)\n",
        "          r = 1- (dtw_distance (s1, s2) /(n1+n2 + 0.00000001))\n",
        "          #print(f\"{m}\\t{n}\\t{y[m]}\\t{y_test[n]}\\t{r:0.02f}\")\n",
        "          print(f\"{m}\\t{n}\\t{y[m]}\\t{n1}\\t{n2}\\t{y_test[n]}\\t{r:0.02f}\",file=f2)\n"
      ],
      "metadata": {
        "id": "67boEFXX1xqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}